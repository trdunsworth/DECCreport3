---
title: "Weekly Report: Week 37 (08 Sep through 14 Sep)"
author: "Tony Dunsworth, PhD"
date: "2025-09-18"
format: 
  docx:
    toc: true
    toc-depth: 3
    number-sections: true
    fig-width: 6
    fig-height: 4
    fig-align: center
    code-fold: true
    code-summary: "Click to show/hide code"
    code-line-numbers: true
    highlight-style: "tango"
    fontsize: 12pt
    margin-left: 1in
    margin-right: 1in
    margin-top: 1in
    margin-bottom: 1in
    documentclass: article
    linestretch: 1.5
    keep-md: true
    md_extensions: +autolink_bare_uris
    prefer-html: true
---

```{r libraries}
#| label: setup
#| echo: false
#| message: false
#| warning: false
library(tidyverse)
library(tidymodels)
library(devtools)
library(remotes)
library(ggpubr)
library(ggrepel)
library(ggraph)
library(gt)
library(gtExtras)
library(GGally)
library(rstatix)
library(car)
library(janitor)
library(Hmisc)
library(psych)
library(corrr)
library(ggcorrplot)
library(ggthemes)
library(ggridges)
library(multcomp)
library(emmeans)
library(RVAideMemoire)
library(FactoMineR)
library(DescTools)
library(nlme)
library(funModeling)
library(inspectdf)
library(dlookr)
library(viridis)
library(merTools)
library(factoextra)
library(nortest)
library(MASS)
library(randtests)
library(summarytools)
library(report)
library(knitr)
library(kableExtra)
library(modelbased)
library(parameters)
library(performance)
library(insight)
library(lubridate)
library(broom)
library(GPfit)
library(survival)
```

## Introduction

This is the weekly report for week 37 covering the period from September 08 through September 14, 2025. The report will include analyses of the data to emphasize different information that is contained within the data and may be pertinent to both operations and management.

```{r data-load}
#| echo: false
#| output: false
df <- read_csv("data\\week37.csv")
 
# Use lubridate and across() to efficiently parse all date-time columns
df <- df |>
  mutate(across(c(Response_Date,
                   Incident_Start_Time,
                   TimeCallViewed,
                   Incident_Queue_Time,
                   Incident_Dispatch_Time,
                   Incident_Phone_Stop,
                   TimeFirstUnitDispatchAcknowledged,
                   Incident_Enroute_Time,
                   Incident_Arrival_Time,
                   TimeFirstCallCleared,
                   Incident_First_Close_Time,
                   Final_Closed_Time,
                   First_Reopen_Time), ymd_hms))

df$WeekNo <- as.factor(df$WeekNo)
df$Day <- as.factor(df$Day)
df$Hour <- as.factor(df$Hour)

# Convert DOW to an ordered factor to respect the sequence of days
df$DOW <- factor(
    df$DOW,
    levels = c("SUN", "MON", "TUE", "WED", "THU", "FRI", "SAT"),
    ordered = TRUE
)

# Convert Priority_Number to an ordered factor as well
df$Priority_Number <- ordered(df$Priority_Number)

# Convert numeric variables from 'doubles' to integers
df[c('Time_To_Queue', 'Time_To_Dispatch', 'Phone_Time', 'Processing_Time', 'Rollout_Time', 'Transit_Time', 'Total_Call_Time')] <- sapply(df[c('Time_To_Queue', 'Time_To_Dispatch', 'Phone_Time', 'Processing_Time', 'Rollout_Time', 'Transit_Time', 'Total_Call_Time')], as.numeric)
```

For this week, there were a total of `r nrow(df)` calls for service. The column list is below:

```{r example-data}
#| echo: false
#| tbl-cap: "A sample of the first 10 rows of incident data."

colnames(df)
```

## Data Cleaning

In order to have a good dataset for analysis, some data cleaning was performed. The first step is to check for missing values in the dataset.

```{r missing-values}
#| echo: false
#| fig-cap: "Prevalence of missing values. Only columns with missing data are shown."

# ...existing code...
# Count columns with any missing data
missing_cols_count <- sum(colSums(is.na(df)) > 0)

# Get named vector of missing counts per column
missing_counts <- colSums(is.na(df))

# Optionally, create a tibble for easy use
missing_summary <- tibble(
  column = names(df),
  missing = missing_counts
)

# Find the column with the largest number of missing values
max_missing_col <- names(missing_counts)[which.max(missing_counts)]
max_missing_count <- max(missing_counts)

# ...existing code...

inspect_na(df) |>
  dplyr::filter(cnt > 0) |> # Explicitly use dplyr's filter
  show_plot(
    col_palette = 4 # Mako palette from viridis
  ) +
  ggthemes::theme_fivethirtyeight(base_size = 10) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8), # Rotate x-axis labels
    axis.text.y = element_text(size = 9) # Fine-tune y-axis label size
  )
```

From this plot, we can see that there are only `r missing_cols_count` values with missing data. Of those, the column with the largest number of missing values is `r max_missing_col`. That is something that we would like to see because that means that most of our calls are closed once and left that way. Later, we will look deeper into those calls to see if there are any patterns to those calls. The number of missing values in Incident_Arrival_Time may be something we wish to focus on in future because it shows that we have calls to which we never arrived. We will want to correlate those with their disposition to see if they were cancelled. Where there are calls that were not cancelled but we did not arrive, we will want to look into those further to see what happened. Additionally, `r round(missing_counts["Incident_Arrival_Time"] / nrow(df) * 100, 1)`% of calls did not have a recorded time that the call stopped. We will have to determine if they were cancelled or how many of those were mutual aid calls where we did not receive a phone call.

## Exploratory Analysis

One of the first analyses is to break down different factor elements to see what we have in the dataset. Starting with the day of the week, the barchart below shows the number of calls for service by day of the week.

```{r day-of-week}
#| echo: false
#| fig-cap: "Number of calls for service by day of the week."
# ggplot2
dow_counts <- df |>
  count(DOW, sort = TRUE)

max_dow_info <- dow_counts |> filter(n == max(n))
busiest_day <- max_dow_info |> slice(1) |> pull(DOW)
busiest_day_count <- max_dow_info |> slice(1) |> pull(n)

min_dow_info <- dow_counts |> filter(n == min(n))
slowest_day <- min_dow_info |> slice(1) |> pull(DOW)
slowest_day_count <- min_dow_info |> slice(1) |> pull(n)

barDOW <- df |> ggplot(aes(x=DOW, fill=DOW)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="E") +
  labs(title="Number of Calls for Service by Day of the Week",
       x="Day of the Week",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) + 
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barDOW
```

From this chart, we can see that `r busiest_day` was the busiest day of the week with `r busiest_day_count` service calls, and the slowest day was `r slowest_day` with `r slowest_day_count` service calls.

Over the last three weeks of this report structure, we've seen different days of the week for the busiest and slowest days. A larger sample size will be needed to determine if there are any underlying patterns.

```{r hour-of-day}
#| echo: false
#| fig-cap: "Number of calls for service by hour of the day." 
# ggplot2
hour_counts <- df |>
  count(Hour, sort = TRUE)

max_hour_info <- hour_counts |> filter(n == max(n))
busiest_hour <- max_hour_info |> slice(1) |> pull(Hour)
busiest_hour_count <- max_hour_info |> slice(1) |> pull(n)

min_hour_info <- hour_counts |> filter(n == min(n))
slowest_hour <- min_hour_info |> slice(1) |> pull(Hour)
slowest_hour_count <- min_hour_info |> slice(1) |> pull(n)

barHour <- df |> ggplot(aes(x=Hour, fill=Hour)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="E") +
  labs(title="Number of Calls for Service by Hour of the Day",
       x="Hour of the Day",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) + 
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=8),
        axis.text.y = element_text(size=10))

barHour
```

This week, the busiest hour of the day was `r busiest_hour`00 hours, with `r busiest_hour_count` calls for service. `r slowest_hour`00 hours was the slowest hour of the day with `r slowest_hour_count` calls. Additionally, the pattern shows consistent traffic from late rush hour through the day into the early evening before seeing the volumes start to decline. This appears to confirm assumptions about the busiest parts of the day.

```{r priority-level}
#| echo: false
#| fig-cap: "Number of calls for service by priority level."
# ggplot2
pn_counts <- df |>
  count(Priority_Number, sort = TRUE)

max_pn_info <- pn_counts |> filter(n == max(n))
busiest_pn <- max_pn_info |> slice(1) |> pull(Priority_Number)
busiest_pn_count <- max_pn_info |> slice(1) |> pull(n)

barPriority <- df |> ggplot(aes(x=Priority_Number, fill=Priority_Number)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="E") +
  labs(title="Number of Calls for Service by Priority Level",
       x="Priority Level",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) + 
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barPriority
```

The majority of calls received were Priority `r busiest_pn` calls. `r busiest_pn` calls are `r round(busiest_pn_count / nrow(df) * 100, 1)` percent of the total number of calls, while Priority 1 calls are `r round((sum(df$Priority_Number == "1") / nrow(df)) * 100, 1)` percent of the total number of calls.

This appears to be consistent through the new reports.

```{r discipline}
#| echo: false
#| fig-cap: "Number of calls for service by discipline."
# ggplot2
agency_counts <- df |>
  count(Agency, sort = TRUE)

max_agency_info <- agency_counts |> filter(n == max(n))
busiest_agency <- max_agency_info |> slice(1) |> pull(Agency)
busiest_agency_count <- max_agency_info |> slice(1) |> pull(n)

barDiscipline <- df |> ggplot(aes(x=Agency, fill=Agency)) +
  geom_bar() + 
  scale_fill_viridis(discrete=TRUE, option="E") +
  labs(title="Number of Calls for Service by Discipline",
       x="Discipline",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) + 
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barDiscipline
```

As expected, the majority of calls are for `r busiest_agency`. They represent `r round((sum(df$Agency == "POLICE") / nrow(df)) * 100, 1)`
percent of the total number of calls. This is fairly consistent with previous analyses. We can also examine the way in which we are receiving the calls by looking at the Call_Reception column. That chart is below.

```{r call-reception}
#| echo: false
#| fig-cap: "Number of calls for service by call reception."
# ggplot2
cr_counts <- df |>
  count(Call_Reception, sort = TRUE)

max_cr_info <- cr_counts |> filter(n == max(n))
busiest_cr <- max_cr_info |> slice(1) |> pull(Call_Reception)
busiest_cr_count <- max_cr_info |> slice(1) |> pull(n)

barReception <- df |> ggplot(aes(x=Call_Reception, fill=Call_Reception)) +
  geom_bar() + 
  scale_fill_viridis(discrete=TRUE, option="E") +
  labs(title="Number of Calls for Service by Call Reception",
       x="Call Reception",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) + 
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barReception
```

Most of the calls arrived by `r busiest_cr` with the next largest method coming in as E-911 calls. There were `r sum(df$Call_Reception == "NOT CAPTURED")` calls where we did not indicated how the service call was received. Since this is only `r round((sum(df$Call_Reception == "NOT CAPTURED") / nrow(df)) * 100, 1)` percent of the total number of calls, this may be something to watch over time.

The following is a chart of the top 10 call types. The data is limited to ensure visual clarity and legibility of the information.

```{r call-type}
#| echo: false
#| fig-cap: "Number of calls for service by call type."
# ggplot2
ct_counts <- df |>
  count(Problem, sort = TRUE)

max_ct_info <- ct_counts |> filter(n == max(n))
busiest_ct <- max_ct_info |> slice(1) |> pull(Problem)
busiest_ct_count <- max_ct_info |> slice(1) |> pull(n)

# Find the most common Problem for each Agency
agency_top_problems <- df |>
  filter(!is.na(Agency), !is.na(Problem)) |>
  count(Agency, Problem, sort = TRUE) |>
  group_by(Agency) |>
  slice_head(n = 1) |>
  ungroup() |>
  select(Agency, Problem, n)

# Create named vectors for easy access in inline code (with error handling)
if(nrow(agency_top_problems) > 0) {
  agency_top_problem_names <- setNames(agency_top_problems$Problem, agency_top_problems$Agency)
  agency_top_problem_counts <- setNames(agency_top_problems$n, agency_top_problems$Agency)
} else {
  agency_top_problem_names <- c()
  agency_top_problem_counts <- c()
}

problem_counts <- df |>
  count(Problem, sort = TRUE) |>
  slice_head(n = 10)

barProblem <- problem_counts |>
  ggplot(aes(x=reorder(Problem, -n), y=n, fill=Problem)) +
  geom_bar(stat="identity") +
  scale_fill_viridis(discrete=TRUE, option="E") +
  labs(title="Number of Calls for Service by Call Type",
       x="Call Type",
       y="Number of Calls") +
  geom_text(
    aes(label = n),
    vjust = -0.5, 
    size = 3) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barProblem
```

This week, the most common problem nature was `r busiest_ct`. For AFD, the most common was `r agency_top_problem_names["FIRE"]`. The overall trend is that Disorderly Conduct and Trouble Breathing appear in the top 5 weekly over the three weeks of this report. These trends should be brought to the attention of our partners.

We can also look at the number of calls taken by telecommunicators. Again, like the problem types, we will limit the chart to the top 10 telecommunicators to ensure visual clarity and legibility of the information.

```{r telecommunicator}
#| echo: false
#| fig-cap: "Number of calls for service by telecommunicator."
# ggplot2
ct_counts <- df |> 
  count(Call_Taker, sort = TRUE) |>
  slice_head(n = 10)

barCallTaker <- ct_counts |>
  ggplot(aes(x=reorder(Call_Taker, -n), y=n, fill=Call_Taker)) +
  geom_bar(stat="identity") +
  scale_fill_viridis(discrete=TRUE, option="E") +
  labs(title="Number of Calls for Service by Call Taker",
       x="Call Taker",
       y="Number of Calls") +
  geom_text(
    aes(label = n),
    vjust = -0.5,
    size = 3) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barCallTaker
```

It is interesting to note that the top "call taker" is CAD2CAD again this week and with a large margin between that volume and the busiest calltaker.

### Call Distribution: Hour by Day of Week

The following visualization shows the distribution of calls throughout the day (by hour) for each day of the week. This helps identify patterns in call volume across different days and times.

```{r hour-dow-analysis}
#| label: hour-dow-analysis
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Call Volume by Hour and Day of Week"

# Create summary data
hourly_dow_summary <- df |>
  group_by(DOW, Hour) |>
  summarise(call_count = n(), .groups = 'drop') |>
  mutate(Hour_numeric = as.numeric(as.character(Hour)))

# Create a heatmap showing call patterns
hour_dow_plot <- ggplot(hourly_dow_summary, aes(x = Hour_numeric, y = DOW, fill = call_count)) +
  geom_tile(color = "white", linewidth = 0.1) +
  scale_x_continuous(name = "Hour of Day", 
                     breaks = seq(0, 23, 2),
                     labels = sprintf("%02d:00", seq(0, 23, 2))) +
  scale_y_discrete(name = "Day of Week", limits = rev) +
  scale_fill_gradient2(name = "Calls", 
                       low = "lightblue", 
                       mid = "yellow",
                       high = "red",
                       midpoint = median(hourly_dow_summary$call_count)) +
  labs(title = "Call Volume Heatmap by Hour and Day of Week",
       subtitle = "Darker colors indicate higher call volumes") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 12),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
    axis.text.y = element_text(size = 10),
    axis.title = element_text(size = 12),
    legend.position = "right",
    panel.grid = element_blank()
  )

hour_dow_plot
```

```{r alternative-ridge-plot}
#| label: alternative-ridge-plot  
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Ridge Plot Alternative - Calls per Hour by Day of Week"

# Try to create actual ridge plot if ggridges is available
tryCatch({
  if (requireNamespace("ggridges", quietly = TRUE)) {
    library(ggridges)
    
    # Create ridge plot using the raw data for a density estimate
    ridge_plot <- ggplot(df, aes(x = as.numeric(as.character(Hour)), y = DOW, fill = DOW)) +
      ggridges::geom_density_ridges(
        alpha = 0.7, 
        scale = 1.2, # Adjust scale for better separation
        rel_min_height = 0.01 # Removes trailing tails
      ) +
      scale_x_continuous(name = "Hour of Day",
                         breaks = seq(0, 23, 4),
                         labels = sprintf("%02d:00", seq(0, 23, 4))) +
      scale_y_discrete(name = "Day of Week", limits = rev) +
      scale_fill_brewer(name = "Day", palette = "Set3") +
      labs(title = "Ridge Plot: Call Volume Distribution by Hour and Day of Week",
           subtitle = "Each ridge shows the hourly distribution for one day") +
      theme_minimal() +
      theme(
        plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, size = 12),
        axis.text = element_text(size = 10),
        axis.title = element_text(size = 12),
        legend.position = "none"
      )
    
    print(ridge_plot)
  } else {
    cat("ggridges package not available. Using heatmap above instead.\n")
  }
}, error = function(e) {
  cat("Could not create ridge plot. Error:", e$message, "\n")
  cat("The heatmap above provides similar insights.\n")
})
```

```{r ridge-plot-summary-stats}
#| label: ridge-plot-summary-stats
#| echo: false
#| message: false
#| warning: false

# Summary statistics for calls by hour and DOW
hourly_summary <- df |>
  group_by(DOW) |>
  summarise(
    total_calls = n(),
    peak_hour = names(sort(table(Hour), decreasing = TRUE))[1],
    avg_calls_per_hour = round(n() / 24, 1),
    .groups = 'drop'
  ) |>
  arrange(desc(total_calls))

# Display summary table
hourly_summary |>
  gt() |>
  tab_header(
    title = "Call Volume Summary by Day of Week",
    subtitle = "Peak hours and average calls per hour"
  ) |>
  cols_label(
    DOW = "Day of Week",
    total_calls = "Total Calls",
    peak_hour = "Peak Hour",
    avg_calls_per_hour = "Avg Calls/Hour"
  ) |>
  fmt_number(
    columns = c(total_calls),
    decimals = 0
  ) |>
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels()
  ) |>
  tab_options(
    table.font.size = 12,
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12
  )
```

These visualizations show that the bulk of our calls are concentrated between 1000 hours and 1400 hours for the week. Additionally, we had a large concentration of service calls during the 0800 and 0900 hours on Wednesday. We should review that time period to see if there were any significant events that impacted our call volumes. 

### Summary statsitcs and analyses

In this section, we will analyse the continuous variables that represent the elapsed time for various segments of the call process. The variables of interest include: Time_To_Queue, Time_To_Dispatch, Phone_Time, Processing_Time, Rollout_Time, Transit_Time, and Total_Call_Time. They are defined as follows:

* Time_To_Queue
: The time from the start of the call to the time it is released to queue for dispatch.

* Time_To_Dispatch
: The time from the time the call is released for dispatch to the time the first unit is assigned.

* Phone_Time
: The time from the start of the call to the time the phone call ended.

* Processing_Time
: The time from the start of the call until the first unit is assigned.

* Rollout_Time
: The time from the assignment of the first unit to the first unit marking en route to the call.

* Transit_Time
: The time from the first unit marking en route to the call to the first unit arriving on scene.

* Total_Call_Time
: The total time from the start of the call to the time the call was closed. If the call is re-opened, then this clock stops with the first closure.

```{r custom-summary}
#| label: custom-summary
#| echo: false
#| message: false
#| warning: false

# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time", 
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")
  
  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df)]
  
  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {
    
    summary_table <- df %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]
                
                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)
                    
                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)
                    
                    if (var_val == 0) return(NA_real_)
                    
                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })
                
                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display
    
    # Use gt for better formatting compatibility with Word output
    summary_table %>%
      gt() %>%
      tab_header(
        title = "Weekly Elapsed Time Summary Table",
        subtitle = "Statistical summary of call processing times"
      ) %>%
      cols_label(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean", 
        Median = "Median",
        Std_Dev = "Std Dev",
        Skewness = "Skew",
        Kurtosis = "Kurt"
      ) %>%
      fmt_number(
        columns = c(Minimum, Mean, Median, Std_Dev, Skewness, Kurtosis),
        decimals = 2,
        use_seps = TRUE
      ) %>%
      tab_style(
        style = cell_text(weight = "bold"),
        locations = cells_column_labels()
      ) %>%
      tab_options(
        table.font.size = 12,
        heading.title.font.size = 14,
        heading.subtitle.font.size = 12
      )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})
```

The values from this table describe operations for the week being analyzed. In this case, the median time for a call to be placed in queue is 45 seconds. This is still in line with what has been seen in the last two weeks. The median time in queue was 15 seconds. These are comparable numbers with the prior weeks.  

```{r ttq-plots}
#| label: ttq-plots
#| echo: false
#| message: false
#| warning: false

# Helper to format seconds as mm:ss or h:mm:ss
sec_label <- function(x) {
  s <- round(x)
  h <- s %/% 3600
  m <- (s %% 3600) %/% 60
  sec <- s %% 60
  ifelse(h > 0, sprintf("%d:%02d:%02d", h, m, sec), sprintf("%d:%02d", m, sec))
}

# Prepare TTQ values in seconds and basic stats
ttq <- df |>
  transmute(ttq_sec = as.numeric(Time_To_Queue)) |>
  filter(!is.na(ttq_sec), ttq_sec >= 0)

n_ttq <- nrow(ttq)
ttq_med <- median(ttq$ttq_sec, na.rm = TRUE)
ttq_p90 <- as.numeric(quantile(ttq$ttq_sec, 0.90, na.rm = TRUE))
ttq_p99 <- as.numeric(quantile(ttq$ttq_sec, 0.99, na.rm = TRUE))
n_clipped <- sum(ttq$ttq_sec > ttq_p99, na.rm = TRUE)

# Adaptive bin width (Freedman–Diaconis) with nicening
fd <- 2 * IQR(ttq$ttq_sec, na.rm = TRUE) / (n_ttq^(1/3))
binw <- fd
if (!is.finite(binw) || binw <= 0) binw <- 5
binw <- dplyr::case_when(
  binw < 1 ~ 1,
  binw < 2 ~ 2,
  binw < 5 ~ 5,
  binw < 10 ~ 10,
  binw < 15 ~ 15,
  TRUE ~ round(binw, -1)
)

# Clip extreme tail for readability (ensure at least 20s to show reference lines)
x_max <- max(20, ttq_p99)

# Single plot: histogram (normalized to density) with density overlay and markers
ttq_hist_dens <- ggplot(ttq, aes(x = ttq_sec)) +
  # Histogram scaled to density for alignment with density curve
  geom_histogram(aes(y = after_stat(density)),
                 binwidth = binw, boundary = 0, closed = "left",
                 fill = "#1c5789", color = "white", alpha = 0.6) +
  # Density overlay
  geom_density(color = "#1c5789", fill = "#1c5789", alpha = 0.2, adjust = 1.1, linewidth = 1) +
  # Median and P90 reference lines
  geom_vline(xintercept = ttq_med, linetype = "dashed", color = "#d62728", linewidth = 0.8) +
  geom_vline(xintercept = ttq_p90, linetype = "dotted", color = "#ff7f0e", linewidth = 0.8) +
  # NENA/NFPA reference lines at 0:15 and 0:20
  geom_vline(xintercept = 15, linetype = "longdash", color = "#2ca02c", linewidth = 0.7) +
  geom_vline(xintercept = 20, linetype = "longdash", color = "#9467bd", linewidth = 0.7) +
  # Labels for markers (placed above the density peak area)
  annotate("label", x = ttq_med, y = Inf, vjust = 1.2,
           label = paste0("Median: ", round(ttq_med), "s"), size = 3, fill = "white") +
  annotate("label", x = ttq_p90, y = Inf, vjust = 1.2,
           label = paste0("P90: ", round(ttq_p90), "s"), size = 3, fill = "white") +
  annotate("label", x = 15, y = Inf, vjust = 1.2,
           label = "NENA 0:15", size = 3, fill = "white") +
  annotate("label", x = 20, y = Inf, vjust = 1.2,
           label = "NFPA 0:20", size = 3, fill = "white") +
  scale_x_continuous(name = "Time to Queue (mm:ss)",
                     limits = c(0, x_max),
                     breaks = scales::pretty_breaks(8),
                     labels = sec_label) +
  scale_y_continuous(name = "Density") +
  labs(title = "Time to Queue — Histogram with Density",
    subtitle = paste0("Histogram normalized to density; clipped at 99th percentile (", scales::comma(n_clipped), " removed).\n",
          "Median (dashed red), 90th percentile (dotted orange). Reference lines at 0:15 (NENA) and 0:20 (NFPA 1225)."),
       caption = "Data: CAD") +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold"),
    axis.title.x = element_text(size = 12),
    axis.title.y = element_text(size = 12)
  )

ttq_hist_dens

```

```{r elapsed-time-plots}
#| label: elapsed-time-plots
#| echo: false
#| message: false
#| warning: false

# Reusable plot helper: histogram normalized to density with density overlay and markers
plot_time_hist_dens <- function(data, var, title_text) {
  vals <- as.numeric(data[[var]])
  vals <- vals[is.finite(vals) & vals >= 0]
  if (length(vals) < 2) return(NULL)

  # Stats
  n <- length(vals)
  med <- median(vals)
  p90 <- as.numeric(quantile(vals, 0.90))
  p99 <- as.numeric(quantile(vals, 0.99))
  n_clipped <- sum(vals > p99)

  # Freedman–Diaconis bin width with nicening
  fd <- 2 * IQR(vals) / (n^(1/3))
  binw <- fd
  if (!is.finite(binw) || binw <= 0) binw <- 5
  binw <- dplyr::case_when(
    binw < 1 ~ 1,
    binw < 2 ~ 2,
    binw < 5 ~ 5,
    binw < 10 ~ 10,
    binw < 15 ~ 15,
    TRUE ~ round(binw, -1)
  )

  x_max <- max(20, p99)

  # mm:ss (or h:mm:ss) labels
  sec_label <- function(x) {
    s <- round(x)
    h <- s %/% 3600
    m <- (s %% 3600) %/% 60
    sec <- s %% 60
    ifelse(h > 0, sprintf("%d:%02d:%02d", h, m, sec), sprintf("%d:%02d", m, sec))
  }

  ggplot(data.frame(x = vals), aes(x = x)) +
    geom_histogram(aes(y = after_stat(density)),
                   binwidth = binw, boundary = 0, closed = "left",
                   fill = "#1c5789", color = "white", alpha = 0.6) +
    geom_density(color = "#1c5789", fill = "#1c5789", alpha = 0.2, adjust = 1.1, linewidth = 1) +
    geom_vline(xintercept = med, linetype = "dashed", color = "#d62728", linewidth = 0.8) +
    geom_vline(xintercept = p90, linetype = "dotted", color = "#ff7f0e", linewidth = 0.8) +
    geom_vline(xintercept = 15, linetype = "longdash", color = "#2ca02c", linewidth = 0.7) +
    geom_vline(xintercept = 20, linetype = "longdash", color = "#9467bd", linewidth = 0.7) +
  annotate("label", x = med, y = Inf, vjust = 1.2,
       label = "Median", size = 2.5, fill = "white") +
  annotate("label", x = p90, y = Inf, vjust = 1.2,
       label = "P90", size = 2.5, fill = "white") +
  annotate("label", x = 15, y = Inf, vjust = 1.2,
       label = "NENA", size = 2.5, fill = "white") +
  annotate("label", x = 20, y = Inf, vjust = 1.2,
       label = "NFPA", size = 2.5, fill = "white") +
    scale_x_continuous(name = "Time (mm:ss)",
                       limits = c(0, x_max),
                       breaks = scales::pretty_breaks(8),
                       labels = sec_label) +
    scale_y_continuous(name = "Density") +
    labs(title = title_text,
      subtitle = NULL,
      caption = "Data: CAD") +
    theme_minimal(base_size = 12) +
    theme(
      plot.title = element_text(face = "bold"),
      axis.title.x = element_text(size = 12),
      axis.title.y = element_text(size = 12)
    )
}

# Build and print plots for other elapsed-time metrics
p_dispatch   <- plot_time_hist_dens(df, "Time_To_Dispatch",  "Time to Dispatch — Histogram with Density")
p_phone      <- plot_time_hist_dens(df, "Phone_Time",       "Phone Time — Histogram with Density")
p_processing <- plot_time_hist_dens(df, "Processing_Time",  "Processing Time — Histogram with Density")
p_rollout    <- plot_time_hist_dens(df, "Rollout_Time",     "Rollout Time — Histogram with Density")
p_transit    <- plot_time_hist_dens(df, "Transit_Time",     "Transit Time — Histogram with Density")
p_total      <- plot_time_hist_dens(df, "Total_Call_Time",  "Total Call Time — Histogram with Density")

if (!is.null(p_dispatch))   print(p_dispatch)
if (!is.null(p_phone))      print(p_phone)
if (!is.null(p_processing)) print(p_processing)
if (!is.null(p_rollout))    print(p_rollout)
if (!is.null(p_transit))    print(p_transit)
if (!is.null(p_total))      print(p_total)
```

```{r elapsed-time-grid}
#| label: elapsed-time-grid
#| echo: false
#| message: false
#| warning: false

# Short-title versions for grid
p_dispatch_grid   <- plot_time_hist_dens(df, "Time_To_Dispatch",  "Time to Dispatch")
p_phone_grid      <- plot_time_hist_dens(df, "Phone_Time",       "Phone Time")
p_processing_grid <- plot_time_hist_dens(df, "Processing_Time",  "Processing Time")
p_rollout_grid    <- plot_time_hist_dens(df, "Rollout_Time",     "Rollout Time")
p_transit_grid    <- plot_time_hist_dens(df, "Transit_Time",     "Transit Time")
p_total_grid      <- plot_time_hist_dens(df, "Total_Call_Time",  "Total Call Time")

# Arrange elapsed-time plots in a 2x3 grid (skip NULL plots)
plots_list <- list(
  p_dispatch_grid,  # Time To Dispatch
  p_phone_grid,     # Phone Time
  p_processing_grid,# Processing Time
  p_rollout_grid,   # Rollout Time
  p_transit_grid,   # Transit Time
  p_total_grid      # Total Call Time
)
plots_list <- Filter(Negate(is.null), plots_list)

if (length(plots_list) > 0) {
  grid <- ggpubr::ggarrange(plotlist = plots_list, ncol = 3, nrow = 2, align = "hv")
  print(grid)
}
```

**Plot Key:**

| Line Type/Color      | Meaning                |
|---------------------|------------------------|
| **Dashed Red**    | Median                 |
| **Dotted Orange** | 90th Percentile (P90)  |
| **Longdash Green** | NENA 0:15 Standard     |
| **Longdash Purple** | NFPA 0:20 Standard     |

These show that the processing times for DECC are well within the NENA and NFPA guidelines. This is good operational data to show how well we are performing with repsect ot those guidelines. Over time, we can track these metrics to ensure that we continue to meet or exceed those standards.

## Discipline Analyses

As discussed earlier, we can create additional subsets from this data to look at specific areas of interest. We will create several new datasets from this weekly set for futher analysis. The first will be a dataset that combines APD Priority 1 calls with AFD Priority 1 and 2 calls and evaluates those as emergency calls. We will also create specific datasets for law, fire, and EMS for specific analyses of the disciplines. We will also create datasets that identify calls that exceed certain parameters that have been defined from other reports. Finally, because we have been evaluating Cardiac Arrest calls for some time, we'll create and analyze that dataset.

```{r new-datasets}
#| label: new-datasets
#| echo: false
#| message: false
#| warning: false

df_hp <- df |> 
  filter((Agency == "POLICE" & Priority_Number < 2) | (Agency %in% c("FIRE", "EMS") & Priority_Number < 3))

df_law <- df |> filter(Agency == "POLICE")
df_fire <- df |> filter(Agency == "FIRE")
df_ems <- df |> filter(Agency == "EMS")
df_ttq_delay <- df_hp |> filter(Time_To_Queue > 60)
df_ttd_delay <- df_hp |> filter(Time_To_Dispatch > 60)
df_ca <- df |> filter(Problem == "CARDIAC ARREST ALS 2- SUPV")

mental_health <- c("MUTUAL PSYCHOLOGICAL EMERGENCY", "PSYCHIATRIC EMERGENCY ALS 1", "PSYCHIATRIC EMERGENCY VIOLENT", "WELFARE CHECK", "JUMPER FROM WWB", "MENTAL HEALTH CASE", "SUICIDE DELAY", "SUICIDE IN PROG NO INJ", "SUICIDE IN PROG INJ/PILLS", "SUICIDE IN PROG TRAUMA")

# Mental health related calls subset
# - Filters rows where `Problem` is one of the values in `mental_health`
# - Uses `%in%` and handles potential NA values safely with `is.na()` check
# - Creates a new dataset `df_mh` for downstream analysis
df_mh <- df |> dplyr::filter(!is.na(Problem) & Problem %in% mental_health)
```

By defining these datasets, we can now add to our analyses. For example, we can reuse the same information from above to drill down into APD and AFD calls. Starting with APD calls for service, we can examine everything as we did above.

### APD Analyses

```{r apd-day-of-week}
#| echo: false
#| fig-cap: "Number of calls for service by day of the week."
# ggplot2
barDOW_APD <- df_law |> ggplot(aes(x=DOW, fill=DOW)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Calls for Service for APD by Day of the Week",
       x="Day of the Week",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) + 
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barDOW_APD
```

This week, Tuesdays were the busiest day of the week for APD service calls. Saturday, being the lightest day of the week overall, was the lightest day for the APD as well. 

```{r apd-hour-of-day}
#| echo: false
#| fig-cap: "Number of calls for service by hour of the day."
# ggplot2
barHour_APD <- df_law |> ggplot(aes(x=Hour, fill=Hour)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Calls for Service for APD by Hour of the Day",
       x="Hour of the Day",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=8),
        axis.text.y = element_text(size=10))

barHour_APD
```

The busiest time of the week for APD calls is from 1400 to 1700 hours. 0900 was also very busy in relation to surrounding hours for the APD. This is showing that the late afternoon/evening rush hour time period may need additional support in the future.

```{r apd-call-reception}
#| echo: false
#| fig-cap: "Number of calls for service by call reception."
# ggplot2
barReception_APD <- df_law |> ggplot(aes(x=Call_Reception, fill=Call_Reception)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Calls for Service for by Call Reception",
       x="Call Reception",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barReception_APD
```

As can be seen, the majority of calls came through telephone. This comports to the call reception results for the week overall.

```{r apd-call-type}
#| echo: false
#| fig-cap: "Number of calls for service by call type."
# ggplot2
problem_counts_APD <- df_law |>
  count(Problem, sort = TRUE) |>
  slice_head(n = 10)

barProblem_APD <- problem_counts_APD |>
  ggplot(aes(x=reorder(Problem, -n), y=n, fill=Problem)) +
  geom_bar(stat="identity") +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Calls for Service by Call Type",
       x="Call Type",
       y="Number of Calls") +
  geom_text(
    aes(label = n),
    vjust = -0.5,
    size=3) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barProblem_APD
```

The largest call type was for Disorderly Conduct, which was also the largest call type for the week overall. This could be something to monitor over time to see how the trend changes over time.

```{r apd-priority-level}
#| echo: false
#| fig-cap: "Number of calls for service by priority level."
# ggplot2
barPriority_APD <- df_law |> ggplot(aes(x=Priority_Number, fill=Priority_Number)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Calls for Service by Priority Level",
       x="Priority Level",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barPriority_APD
```

As expected, the largest number of calls werew Priority 2 calls which represent `r round((sum(df_law$Priority_Number == "2") / nrow(df_law)) * 100, 1)` percent of all APD calls. Again, this comports with the overall weekly trends.

```{r apd-custom-summary}
#| label: apd-custom-summary
#| echo: false
#| message: false
#| warning: false

# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time",
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_law)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- df_law %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

    # Use gt for better formatting compatibility with Word output
    summary_table %>%
      gt() %>%
      tab_header(
        title = "Weekly Elapsed Time Summary Table",
        subtitle = "Statistical summary of call processing times"
      ) %>%
      cols_label(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median",
        Std_Dev = "Std Dev",
        Skewness = "Skew",
        Kurtosis = "Kurt"
      ) %>%
      fmt_number(
        columns = c(Minimum, Mean, Median, Std_Dev, Skewness, Kurtosis),
        decimals = 2,
        use_seps = TRUE
      ) %>%
      tab_style(
        style = cell_text(weight = "bold"),
        locations = cells_column_labels()
      ) %>%
      tab_options(
        table.font.size = 12,
        heading.title.font.size = 14,
        heading.subtitle.font.size = 12
      )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})
```

This table shows that overall, we have a median time on the phones of about 2.5 minutes and it takes about double that for a call to start and be dispatched. Some of that difference is going to be due to having to hold Priority 4 and above calls until there is a unit available. Since the P4 calls are `r round((sum(df_law$Priority_Number == "4") / nrow(df_law)) * 100, 1)` percent of APD calls, this could have a measureable impact on service times for DECC staff.

### AFD FIRE Analyses

Because AFD calls for service can be split into two distinct disciplines, fire-related calls and medical-related calls, DECC has, historically, separated the two disciplines for analytical purposes. So this section will look at fire-related calls for service for the week.

```{r afd-day-of-week}
#| echo: false
#| fig-cap: "Number of calls for service by day of the week."
# ggplot2
barDOW_AFD <- df_fire |> ggplot(aes(x=DOW, fill=DOW)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="F") +
  labs(title="Number of Calls for Service for AFD by Day of the Week",
       x="Day of the Week",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barDOW_AFD
```

This week, the busiest day for fire-related calls was Wednesday with Thursday and Friday both showing similar numbers. Monday was the lightest day for fire-related calls.

```{r afd-hour-of-day}
#| echo: false
#| fig-cap: "Number of calls for service by hour of the day."
# ggplot2
barHour_AFD <- df_fire |> ggplot(aes(x=Hour, fill=Hour)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="F") +
  labs(title="Number of Calls for Service for AFD by Hour of the Day",
       x="Hour of the Day",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=8),
        axis.text.y = element_text(size=10))

barHour_AFD
```

Fire-related calls are much more spread out through the day as can be seen in the graph above. However, the 1100 and 1800 hours stand out as the hour where the most calls were received for the week. There hasn't been an overall trend identified in the three weeks of this report. However, we will continue to observe the patterns to see if any trends emerge in fire related calls.

```{r afd-call-reception}
#| echo: false
#| fig-cap: "Number of calls for service by call reception."
# ggplot2
barReception_AFD <- df_fire |> ggplot(aes(x=Call_Reception, fill=Call_Reception)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="F") +
  labs(title="Number of Calls for Service for AFD by Call Reception",
       x="Call Reception",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barReception_AFD
```

Like APD calls, most fire-related calls came in via Phone. However the numbers for Mutual Aid and E-911 were larger percentages of the overall volume. In this case, Phone, not necessarily E-911 represented `r round((sum(df_fire$Call_Reception == "Phone") / nrow(df_fire)) * 100, 1)` percent of all fire-related service calls received.

```{r afd-call-type}
#| echo: false
#| fig-cap: "Number of calls for service by call type."
# ggplot2
problem_counts_AFD <- df_fire |>
  count(Problem, sort = TRUE) |>
  slice_head(n = 10)

barProblem_AFD <- problem_counts_AFD |>
  ggplot(aes(x=reorder(Problem, -n), y=n, fill=Problem)) +
  geom_bar(stat="identity") +
  scale_fill_viridis(discrete=TRUE, option="F") +
  labs(title="Number of Calls for Service for AFD by Call Type",
       x="Call Type",
       y="Number of Calls") +
  geom_text(
    aes(label = n),
    vjust = -0.5,
    size=3) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barProblem_AFD
```

The greatest number of fire-related service calls were for Fire Alarms. That is an interesting observation and should be watched through the future.

```{r afd-priority-level}
#| echo: false
#| fig-cap: "Number of calls for service by priority level."
# ggplot2
barPriority_AFD <- df_fire |> ggplot(aes(x=Priority_Number, fill=Priority_Number)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="F") +
  labs(title="Number of Calls for Service by Priority Level",
       x="Priority Level",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barPriority_AFD
```

This week there were an equal number of priority 1 and 3 fire-related service calls for AFD.

```{r afd-custom-summary}
#| label: afd-custom-summary
#| echo: false
#| message: false
#| warning: false

# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time",
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_fire)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- df_fire %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

    # Use gt for better formatting compatibility with Word output
    summary_table %>%
      gt() %>%
      tab_header(
        title = "Weekly Elapsed Time Summary Table",
        subtitle = "Statistical summary of call processing times"
      ) %>%
      cols_label(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median",
        Std_Dev = "Std Dev",
        Skewness = "Skew",
        Kurtosis = "Kurt"
      ) %>%
      fmt_number(
        columns = c(Minimum, Mean, Median, Std_Dev, Skewness, Kurtosis),
        decimals = 2,
        use_seps = TRUE
      ) %>%
      tab_style(
        style = cell_text(weight = "bold"),
        locations = cells_column_labels()
      ) %>%
      tab_options(
        table.font.size = 12,
        heading.title.font.size = 14,
        heading.subtitle.font.size = 12
      )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})
```

Overall, DECC operations appear to be very efficient at getting fire-related service calls out to the field. The median processing time was only 32 seconds. This shows that we can easily be in compliance with all necessary NENA and NFPA guidelines. The median time on the phone was just over 2 minutes. The mean time was just over 3 minutes which is still amazing.

### AFD EMS Analyses

Because AFD calls for service can be split into two distinct disciplines, fire-related calls and medical-related calls, DECC has, historically, separated the two disciplines for analytical purposes. So this section will look at medical-related calls for service for the week.

```{r ems-day-of-week}
#| echo: false
#| fig-cap: "Number of calls for service by day of the week."
# ggplot2
barDOW_EMS <- df_ems |> ggplot(aes(x=DOW, fill=DOW)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="D") +
  labs(title="Number of Calls for Service for EMS by Day of the Week",
       x="Day of the Week",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barDOW_EMS
```

 This week, there is a spike in medical calls on Wednesday. This appears to correlate to the information that we saw earlier in the report. Due to the difference between Wednesday and the remainder of the week, like the calls by hour heatmap above, we should look at this time period to determine if there were any significant events that happened which can account for the increase in volume.

```{r ems-hour-of-day}
#| echo: false
#| fig-cap: "Number of calls for service by hour of the day." 
# ggplot2
barHour_EMS <- df_ems |> ggplot(aes(x=Hour, fill=Hour)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="D") +
  labs(title="Number of Calls for Service for EMS by Hour of the Day",
       x="Hour of the Day",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=8),
        axis.text.y = element_text(size=10))

barHour_EMS
```

 The spike of calls at 0800 also corresponds to the increase that we've seen around Wednesday morning. Remvoing that from the equation, the remainder of the week followed the expected curve where there are fewer calls in the early morning and the remainder of the day sees the increase and fall that would be normally anticipated.

```{r ems-call-reception}
#| echo: false
#| fig-cap: "Number of calls for service by call reception."
# ggplot2
barReception_EMS <- df_ems |> ggplot(aes(x=Call_Reception, fill=Call_Reception)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="D") +
  labs(title="Number of Calls for Service for EMS by Call Reception",
       x="Call Reception",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barReception_EMS
```

As expected, the vast majority of medical calls arrive via 911 trunk lines. However, `r round((sum(df_ems$Call_Reception == "NOT CAPTURED") / nrow(df_ems)) * 100, 1)` percent of medical calls arrived without a method by which we recevied the call. We should track this further to see if this is a one-off or if there is some issue that needs to be addressed.

```{r ems-call-type}
#| echo: false
#| fig-cap: "Number of calls for service by call type."
# ggplot2
problem_counts_EMS <- df_ems |>
  count(Problem, sort = TRUE) |>
  slice_head(n = 10)

barProblem_EMS <- problem_counts_EMS |>
  ggplot(aes(x=reorder(Problem, -n), y=n, fill=Problem)) +
  geom_bar(stat="identity") +
  scale_fill_viridis(discrete=TRUE, option="E") +
  labs(title="Number of Calls for Service for EMS by Call Type",
       x="Call Type",
       y="Number of Calls") +
  geom_text(
    aes(label = n),
    vjust = -0.5, 
    size = 3) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barProblem_EMS
```

Breathing issues and BLS Emergency calls were the two most prevalent call types for the week. The other call type that has a good percentage of service calls is Mutual ALS which spiked to 44 calls this week, third in the overall count.

```{r ems-priority-level}
#| echo: false
#| fig-cap: "Number of calls for service by priority level."
# ggplot2
barPriority_EMS <- df_ems |> ggplot(aes(x=Priority_Number, fill=Priority_Number)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="E") +
  labs(title="Number of Calls for Service by Priority Level",
       x="Priority Level",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barPriority_EMS
```

 The majority of medical service calls are P1, which is to be expected. P3 calls were the second most prevalent.

```{r ems-custom-summary}
#| label: ems-custom-summary
#| echo: false
#| message: false
#| warning: false

# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time", 
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_ems)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- df_ems %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

    # Use gt for better formatting compatibility with Word output
    summary_table %>%
      gt() %>%
      tab_header(
        title = "Weekly Elapsed Time Summary Table",
        subtitle = "Statistical summary of call processing times"
      ) %>%
      cols_label(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median",
        Std_Dev = "Std Dev",
        Skewness = "Skew",
        Kurtosis = "Kurtosis"
      ) %>%
      fmt_number(
        columns = c(Minimum, Mean, Median, Std_Dev, Skewness, Kurtosis),
        decimals = 2,
        use_seps = TRUE
      ) %>%
      tab_style(
        style = cell_text(weight = "bold"),
        locations = cells_column_labels()
      ) %>%
      tab_options(
        table.font.size = 12,
        heading.title.font.size = 14,
        heading.subtitle.font.size = 12
      )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})
```

The median time to process medical calls was `r round(median(as.numeric(df_ems$Processing_Time), na.rm = TRUE), 2)` seconds. Again, this puts us in good form when examening our operational efficiency. The median time on phones, `r round(median(as.numeric(df_ems$Phone_Time), na.rm = TRUE), 2)` seconds, is longer than the overall median. That is to be expected with these calls taking longer to triage.

## Additional Analyses

Earlier, for this analysis, we created some additional datasets that we can investigate in the course of our analysis. The first two are lists of calls where the elapsed time prior to release to queue or the time spent in dispatch is greater than 60 seconds for *emergency* calls. For the first, there are `r nrow(df_ttq_delay)` emergency service calls where the elapsed time from call start to the call entering the queue for dispatch was over 60 seconds. There are also `r nrow(df_ttd_delay)` emergency service calls where the elapsed time from entering queue to the first unit assigned was over 60 seconds.

### Possible Service Delays

We can look at the datasets and see if there are telecommunicators who may experience more challenging calls during the week. First will be a table of telecommunicators who worked emergency calls that took longer than 60 seconds to go from start to queue. The second will be a table of dispatchers who assigned an emergency call that waited in queue longer than 60 seconds.

```{r queue-too-long}
#| echo: false
#| message: false
#| warning: false

# Table: Call_Taker frequency in df_ttq_delay (descending)
library(dplyr)
library(knitr)

call_taker_counts <- df_ttq_delay %>%
  count(Call_Taker, sort = TRUE)

kable(call_taker_counts, col.names = c("Call Taker", "Count"), caption = "Frequency of Call Taker in Delayed TTQ Calls (Descending)")
```

From this, since there are a small number of telecommunicators who have more than one call in the table above, there may not be any need for amerlioration. This, however, could be something that is included in the report template in order to monitor. Should a telecommunicator appear multiple times in this table over a period of time, additional training or mentoring may be called for.

```{r ttd-delay-table}
#| echo: false
#| message: false
#| warning: false

# Table: Dispatcher frequency in df_ttd_delay (descending)
dispatcher_counts <- df_ttd_delay %>%
  count(Dispatcher, sort = TRUE)

kable(dispatcher_counts, col.names = c("Dispatcher", "Count"), caption = "Frequency of Dispatcher in Delayed TTD Calls (Descending)")
```

This list is fairly short and could simply be monitored in future should the need arise. 

## High-Priority and Critical Calls

In this section, we will focus on the calls that are deemed high-priority or critical. This includes APD Priority 1 calls and AFD Priority 1 and 2 calls. We have identified these calls in the `df_hp` dataset created earlier.

### High-Priority Call Types

```{r hp-call-types}
#| echo: false
#| fig-cap: "Top High-Priority Call Types"
# ggplot2
hp_call_types <- df_hp |>
  count(Problem, sort = TRUE) |>
  slice_head(n = 10)

ggplot(hp_call_types, aes(x = reorder(Problem, n), y = n, fill = Problem)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  scale_fill_viridis(discrete = TRUE, option = "E") +
  labs(title = "Top High-Priority Call Types",
       x = "Call Type",
       y = "Number of Calls") +
  theme_minimal()
```

Almost all of the problem types in this graph belong to AFD and are medical calls. Based on the information above, this is to be expected.

### High-Priority Response Times

```{r hp-response-times}
#| echo: false
#| fig-cap: "High-Priority Call Response Times"
#| warning: false
#| message: false
# ggplot2
hp_response_times <- df_hp |>
  transmute(
    Time_To_Queue = as.numeric(Time_To_Queue),
    Time_To_Dispatch = as.numeric(Time_To_Dispatch),
    Phone_Time = as.numeric(Phone_Time),
    Processing_Time = as.numeric(Processing_Time),
    Rollout_Time = as.numeric(Rollout_Time),
    Transit_Time = as.numeric(Transit_Time),
    Total_Call_Time = as.numeric(Total_Call_Time)
  ) |>
  pivot_longer(everything(), names_to = "Metric", values_to = "Time")
  
ggplot(hp_response_times |> filter(!is.na(Time) & Time >= 0), aes(x = Time)) +
  geom_histogram(binwidth = 5, fill = "#1c5789", color = "white", alpha = 0.7) +
  facet_wrap(~ Metric, scales = "free") +
  scale_x_continuous(labels = scales::comma) + # Improve readability of x-axis
  labs(title = "Distribution of Response Times for High-Priority Calls",
       x = "Time (seconds)",
       y = "Frequency") +
  theme_minimal()
```

These histograms show that, overall, our record for handling these calls is excellent.

## Cardiac Arrest Calls Analysis

Finally, we will look into the specific subset of calls that are related to cardiac arrests. These calls have been identified in the `df_ca` dataset.

```{r ca-call-volume}
#| echo: false
#| fig-cap: "Cardiac Arrest Call Volume by Day and Hour"
# ggplot2
barDOW_CA <- df_ca |> ggplot(aes(x=DOW, fill=DOW)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="D") +
  labs(title="Number of Calls for Service for Cardiac Arrest by Day of the Week",
       x="Day of the Week",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barDOW_CA
```

As we can see, with a very limited number of cardiac arrest calls for the week.

```{r ca-response-times}
#| echo: false
#| fig-cap: "Cardiac Arrest Call Response Times"
#| warning: false
#| message: false
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time", 
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_ca)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- df_ca %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

    # Use gt for better formatting compatibility with Word output
    summary_table %>%
      gt() %>%
      tab_header(
        title = "Weekly Elapsed Time Summary Table",
        subtitle = "Statistical summary of call processing times"
      ) %>%
      cols_label(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median",
        Std_Dev = "Std Dev",
        Skewness = "Skew",
        Kurtosis = "Kurtosis"
      ) %>%
      fmt_number(
        columns = c(Minimum, Mean, Median, Std_Dev, Skewness, Kurtosis),
        decimals = 2,
        use_seps = TRUE
      ) %>%
      tab_style(
        style = cell_text(weight = "bold"),
        locations = cells_column_labels()
      ) %>%
      tab_options(
        table.font.size = 12,
        heading.title.font.size = 14,
        heading.subtitle.font.size = 12
      )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})
```

However, we can see that the median time to process a cardiac arrest and get the units rolling is about `r round(median(as.numeric(df_ca$Processing_Time), na.rm = TRUE), 2)` seconds. The median time that we are on the phone is significantly longer, `r round(median(as.numeric(df_ca$Phone_Time), na.rm = TRUE), 2)` seconds. That is to be expected since the calltaker is likely giving T-CPR instructions while the units are en route.

### Mental Health Analyses

With the advent of Marcus' Law in Virginia, there has been an emphasis on how mental health calls are processed and serviced. The following analyses will focus on the mental health calls that have been defined as such after consultation with DCHS.

```{r mh-day-of-week}
#| echo: false
#| fig-cap: "Number of calls for service by day of the week."
# ggplot2
barDOW_MH <- df_mh |> ggplot(aes(x=DOW, fill=DOW)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Mental Health related calls by Day of the Week",
       x="Day of the Week",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) + 
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barDOW_MH
```

This week, the busies tday for mental health related calls was also Wednesday. Friday followed closely with a few mental health calls also. 

```{r mh-hour-of-day}
#| echo: false
#| fig-cap: "Number of calls for service by hour of the day."
# ggplot2
barHour_MH <- df_mh |> ggplot(aes(x=Hour, fill=Hour)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Calls for Service for APD by Hour of the Day",
       x="Hour of the Day",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=8),
        axis.text.y = element_text(size=10))

barHour_MH
```

Most of these calls arrived, for this past week, in the late afternoon and evenings. Again, should this data prove to be part of a trend, then we should adjust the availability of repsonders to address the community's needs.

```{r mh-call-reception}
#| echo: false
#| fig-cap: "Number of calls for service by call reception."
# ggplot2
barReception_MH <- df_mh |> ggplot(aes(x=Call_Reception, fill=Call_Reception)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Calls for Service for by Call Reception",
       x="Call Reception",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barReception_MH
```

As expected, most of the calls arrived by either phone, trunk line not defined, or from E-911 service calls. Further analysis could be understaken to determine if any of these are transfer calls from our local 988 provider partner.

```{r mh-call-type}
#| echo: false
#| fig-cap: "Number of calls for service by call type."
# ggplot2
problem_counts_MH <- df_mh |>
  count(Problem, sort = TRUE) |>
  slice_head(n = 10)

barProblem_MH <- problem_counts_MH |>
  ggplot(aes(x=reorder(Problem, -n), y=n, fill=Problem)) +
  geom_bar(stat="identity") +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Calls for Service by Call Type",
       x="Call Type",
       y="Number of Calls") +
  geom_text(
    aes(label = n),
    vjust = -0.5,
    size=3) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barProblem_MH
```

The most used call type was Mental Health Case which is expected. 

```{r mh-priority-level}
#| echo: false
#| fig-cap: "Number of calls for service by priority level."
# ggplot2
barPriority_MH <- df_mh |> ggplot(aes(x=Priority_Number, fill=Priority_Number)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Calls for Service by Priority Level",
       x="Priority Level",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barPriority_MH
```

Since Mental Health Case was the most used call type and is a P2 call, Priority 2 is the most used priority. The question, in the future, will be does these calls need to changed to a higher priority?

```{r mh-custom-summary}
#| label: mh-custom-summary
#| echo: false
#| message: false
#| warning: false

# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time",
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_mh)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- df_mh %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

    # Use gt for better formatting compatibility with Word output
    summary_table %>%
      gt() %>%
      tab_header(
        title = "Weekly Elapsed Time Summary Table",
        subtitle = "Statistical summary of call processing times"
      ) %>%
      cols_label(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median",
        Std_Dev = "Std Dev",
        Skewness = "Skew",
        Kurtosis = "Kurt"
      ) %>%
      fmt_number(
        columns = c(Minimum, Mean, Median, Std_Dev, Skewness, Kurtosis),
        decimals = 2,
        use_seps = TRUE
      ) %>%
      tab_style(
        style = cell_text(weight = "bold"),
        locations = cells_column_labels()
      ) %>%
      tab_options(
        table.font.size = 12,
        heading.title.font.size = 14,
        heading.subtitle.font.size = 12
      )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})
```

Processing times for these calls are longer, somewhere around `r round(median(as.numeric(df_mh$Processing_Time), na.rm = TRUE), 2)` seconds. There are several factors that can impact this. The time to make it dispatchable was longer, implying that with these types of calls, it take calltakers longer to get the information necessary in the initial triage to accurately locate and classify the call. Another possible issue, in reviewing the dispatch times is that these calls require specialized training and skill sets on the part of the field responders. If those responders are already assigned to other calls, this could create the delay as seen here. As these values change over time, we should be able to build better pictures and determine the delay points and create strategies to ameliorate them.

## Conclusion

This report has covered various aspects of the calls for service during week 35, 2025. We have analyzed the data for completeness and accuracy, explored it for insights into call patterns and trends, and focused on specific areas of interest such as high-priority calls and cardiac arrest incidents. The findings will assist in making informed decisions to improve service delivery and operational efficiency.
