---
title: "DECC Weekly Report"
author: "Tony Dunsworth, PhD"
date: "2025-12-01"
format:
    html:
        toc: true
        toc-depth: 3
        toc-location: left
execute:
  freeze: auto
  echo: false
---

## Week 48 from 23 November through 29 November 2025

## Table of Contents

### Main Sections

- **[Introduction](#introduction)**
- **[Data Cleaning](#data-cleaning)**
- **[Exploratory Analysis](#exploratory-analysis)**
  - [Call Distribution: Hour by Day of Week](#call-distribution-hour-by-day-of-week)
  - [Distribution of Service Calls by Shift](#distribution-of-service-calls-by-shift)
  - [CAD-centric Analyses](#cad-centric-analyses)
  - [Summary Statistics and Analyses](#summary-statsitcs-and-analyses)
- **[Discipline Analyses](#discipline-analyses)**
  - [APD Analyses](#apd-analyses)
  - [AFD FIRE Analyses](#afd-fire-analyses)
  - [AFD EMS Analyses](#afd-ems-analyses)
- **[Additional Analyses](#additional-analyses)**
  - [Possible Service Delays](#possible-service-delays)
- **[High-Priority and Critical Calls](#high-priority-and-critical-calls)**
  - [High-Priority Call Types](#high-priority-call-types)
  - [High-Priority Response Times](#high-priority-response-times)
- **[E-911 Service Call Analyses](#e-911-service-call-analyses)**
  - [E-911 Call Response Summary](#e-911-call-response-summary)
  - [E-911 Call Breakdowns](#e-911-call-breakdowns)
- **[Cardiac Arrest Calls Analysis](#cardiac-arrest-calls-analysis)**
- **[Mental Health Analyses](#mental-health-analyses)**
- **[Call Source Unrecorded](#call-source-unrecorded)**
- **[Hybrid Call Takers](#hybrid-call-takers)**
- **[Outlier Identification](#outlier-identification)**
- **[Comparisons](#comparisons)**
- **[Conclusion](#conclusion)**

---

```{r}
#| label: setup
#| echo: false
#| message: false
#| warning: false
#| cache: false

# Suppress dbus warnings that can cause preview issues
suppressWarnings({
  suppressPackageStartupMessages({
    library(tidyverse)
    library(tidymodels)
    library(devtools)
    library(remotes)
    library(ggpubr)
    library(ggrepel)
    library(ggraph)
    library(gt)
    library(gtExtras)
    library(GGally)
    library(rstatix)
    library(car)
    library(janitor)
    library(Hmisc)
    library(psych)
    library(corrr)
    library(ggcorrplot)
    library(ggthemes)
    library(ggridges)
    library(multcomp)
    library(emmeans)
    library(RVAideMemoire)
    library(FactoMineR)
    library(DescTools)
    library(nlme)
    library(funModeling)
    library(inspectdf)
    library(dlookr)
    library(viridis)
    library(merTools)
    library(factoextra)
    library(nortest)
    library(MASS)
    library(randtests)
    library(summarytools)
    library(report)
    library(knitr)
    library(kableExtra)
    library(patchwork)
    library(ggradar)
    library(modelbased)
    library(parameters)
    library(performance)
    library(insight)
    library(lubridate)
    library(broom)
    library(GPfit)
    library(survival)
    library(paletteer)
    library(flextable)
    library(officer)
    library(outliers)
    library(reticulate)
  })
})
```

```{r}
#| label: dataframes
#| include: false
#| cache: false

# ============================================================================
# AUTOMATED WEEK CALCULATION AND DATA LOADING
# ============================================================================
# This chunk automatically determines which weeks to load based on the report date
# Report covers: week PRIOR to the week containing the report date
# Comparison uses: week BEFORE the reporting week

# Report date (today's date or override here if needed)
REPORT_DATE <- Sys.Date()  # Change this to lock to a specific date if needed
# REPORT_DATE <- as.Date("2025-10-30")  # Example: uncomment to use specific date

# Calculate which week the REPORT_DATE falls into (ISO 8601: weeks start Sunday)
report_week_start <- floor_date(REPORT_DATE, "week", week_start = 7)  # 7 = Sunday
report_week_num <- isoweek(report_week_start)
report_year <- year(report_week_start)

# The week we're REPORTING ON is the week PRIOR to the report week
current_week_start <- report_week_start - weeks(1)
current_week_end <- current_week_start + days(6)
# Use the middle of the week (Wednesday) to get accurate ISO week number
current_week_num <- isoweek(current_week_start + days(3))
current_year <- year(current_week_start)

# The COMPARISON week is one week before the current week
last_week_start <- current_week_start - weeks(1)
last_week_end <- last_week_start + days(6)
# Use the middle of the week (Wednesday) to get accurate ISO week number
last_week_num <- isoweek(last_week_start + days(3))
last_year <- year(last_week_start)

# Store week numbers for use throughout the document
WEEK_NUMBER <- current_week_num
WEEK_START_DATE <- current_week_start
WEEK_END_DATE <- current_week_end

LAST_WEEK_NUMBER <- last_week_num
LAST_WEEK_START_DATE <- last_week_start
LAST_WEEK_END_DATE <- last_week_end

# Format dates for display
WEEK_START_FORMATTED <- format(WEEK_START_DATE, "%d %b")
WEEK_END_FORMATTED <- format(WEEK_END_DATE, "%d %b")

LAST_WEEK_START_FORMATTED <- format(LAST_WEEK_START_DATE, "%d %b")
LAST_WEEK_END_FORMATTED <- format(LAST_WEEK_END_DATE, "%d %b")

# ============================================================================
# DATA FILE LOADING HELPER
# ============================================================================
# Function to find and load the correct CSV file for a given week
load_week_data <- function(week_num, week_year) {
  # Determine which folder to look in
  if (week_year == year(REPORT_DATE)) {
    # Current year - look in data/current_year
    base_path <- "data/current_year"
  } else {
    # Prior year - look in data/prior_year
    base_path <- "data/prior_year"
  }
  
  # Try different naming conventions
  possible_files <- c(
    file.path(base_path, paste0("week", week_num, ".csv")),
    file.path(base_path, paste0("Week", week_num, "_", substr(week_year, 3, 4), ".csv")),
    file.path(base_path, paste0("Week", week_num, ".csv")),
    # Fallback: also check root data folder for backwards compatibility
    paste0("data/week", week_num, ".csv"),
    paste0("data/Week", week_num, "_", substr(week_year, 3, 4), ".csv"),
    paste0("data/Week", week_num, ".csv")
  )
  
  # Find the first file that exists
  data_file <- NULL
  for(file in possible_files) {
    if(file.exists(file)) {
      data_file <- file
      break
    }
  }
  
  if(is.null(data_file)) {
    stop("No data file found for week ", week_num, " (", week_year, "). Checked: ", 
         paste(possible_files, collapse = ", "))
  }
  
  cat("Loading week", week_num, "(", week_year, ") data from:", data_file, "\n")
  
  # Load the CSV file
  df <- tryCatch({
    if (requireNamespace("vroom", quietly = TRUE)) {
      as_tibble(vroom::vroom(
        data_file,
        col_types = vroom::cols(.default = "c"),
        na = c("", "NA", "NULL"),
        progress = FALSE
      ))
    } else {
      readr::read_csv(
        data_file,
        col_types = readr::cols(.default = readr::col_character()),
        na = c("", "NA", "NULL"),
        show_col_types = FALSE
      )
    }
  }, error = function(e) {
    readr::read_csv(
      data_file,
      col_types = readr::cols(.default = readr::col_character()),
      na = c("", "NA", "NULL"),
      show_col_types = FALSE
    )
  })
  
  # Validate data loaded
  if(nrow(df) == 0) {
    stop("Data file is empty or failed to load: ", data_file)
  }
  
  cat("  Successfully loaded", nrow(df), "rows\n")
  
  return(df)
}

# ============================================================================
# PHONE DATA FILE LOADING HELPER
# ============================================================================
# Function to find and load the correct phone CSV file for a given week
load_phone_data <- function(week_num, week_year) {
  # Determine which folder to look in
  if (week_year == year(REPORT_DATE)) {
    # Current year - look in data/current_year
    base_path <- "data/current_year"
  } else {
    # Prior year - look in data/prior_year
    base_path <- "data/prior_year"
  }
  
  # Try different naming conventions for phone data
  possible_files <- c(
    file.path(base_path, paste0("week_", week_num, "_phone.csv")),
    file.path(base_path, paste0("Week_", week_num, "_phone.csv")),
    # Fallback: also check root data folder for backwards compatibility
    paste0("data/week_", week_num, "_phone.csv"),
    paste0("data/Week_", week_num, "_phone.csv")
  )
  
  # Find the first file that exists
  data_file <- NULL
  for(file in possible_files) {
    if(file.exists(file)) {
      data_file <- file
      break
    }
  }
  
  if(is.null(data_file)) {
    warning("No phone data file found for week ", week_num, " (", week_year, "). Checked: ", 
         paste(possible_files, collapse = ", "))
    return(NULL)
  }
  
  cat("Loading week", week_num, "(", week_year, ") phone data from:", data_file, "\n")
  
  # Load the CSV file
  df <- tryCatch({
    if (requireNamespace("vroom", quietly = TRUE)) {
      as_tibble(vroom::vroom(
        data_file,
        col_types = vroom::cols(.default = "c"),
        na = c("", "NA", "NULL"),
        progress = FALSE
      ))
    } else {
      readr::read_csv(
        data_file,
        col_types = readr::cols(.default = readr::col_character()),
        na = c("", "NA", "NULL"),
        show_col_types = FALSE
      )
    }
  }, error = function(e) {
    readr::read_csv(
      data_file,
      col_types = readr::cols(.default = readr::col_character()),
      na = c("", "NA", "NULL"),
      show_col_types = FALSE
    )
  })
  
  # Validate data loaded
  if(nrow(df) == 0) {
    warning("Phone data file is empty or failed to load: ", data_file)
    return(NULL)
  }
  
  cat("  Successfully loaded", nrow(df), "rows\n")
  
  return(df)
}

# ============================================================================
# LOAD DATA FOR CURRENT WEEK AND LAST WEEK (CAD DATA)
# ============================================================================
cat("\n=== AUTOMATED WEEK CALCULATION ===\n")
cat("Report Date:", format(REPORT_DATE, "%Y-%m-%d"), "\n")
cat("Current Week (reporting on):", WEEK_NUMBER, "—", WEEK_START_FORMATTED, "through", WEEK_END_FORMATTED, current_year, "\n")
cat("Last Week (comparison):", LAST_WEEK_NUMBER, "—", LAST_WEEK_START_FORMATTED, "through", LAST_WEEK_END_FORMATTED, last_year, "\n\n")

# Load current week CAD data
current_week <- load_week_data(current_week_num, current_year)

# Load last week CAD data
last_week <- load_week_data(last_week_num, last_year)

cat("\n=== CAD DATA LOADING COMPLETE ===\n")

# ============================================================================
# LOAD PHONE DATA FOR CURRENT WEEK AND LAST WEEK
# ============================================================================
cat("\n=== LOADING PHONE DATA ===\n")

# Load current week phone data
current_week_phone <- load_phone_data(current_week_num, current_year)

# Load last week phone data
last_week_phone <- load_phone_data(last_week_num, last_year)

# Check if phone data loaded successfully
if (!is.null(current_week_phone)) {
  cat("Current week phone data loaded:", nrow(current_week_phone), "rows\n")
} else {
  cat("Current week phone data not available\n")
}

if (!is.null(last_week_phone)) {
  cat("Last week phone data loaded:", nrow(last_week_phone), "rows\n")
} else {
  cat("Last week phone data not available\n")
}

cat("\n=== PHONE DATA LOADING COMPLETE ===\n")

# ============================================================================
# LOAD LAST 4 WEEKS OF DATA (CAD AND PHONE)
# ============================================================================
cat("\n=== LOADING LAST 4 WEEKS DATA ===\n")

# Calculate the 4 weeks prior to the current week
last_4_weeks <- list()
last_4_weeks_phone <- list()

for (i in 1:4) {
  # Calculate week number and year for this offset
  week_offset_start <- current_week_start - weeks(i)
  week_offset_num <- isoweek(week_offset_start + days(3))
  week_offset_year <- year(week_offset_start)
  
  cat("Loading week", week_offset_num, "(", week_offset_year, ")\n")
  
  # Load CAD data
  tryCatch({
    last_4_weeks[[i]] <- load_week_data(week_offset_num, week_offset_year)
  }, error = function(e) {
    cat("  Warning: Could not load CAD data for week", week_offset_num, "\n")
    last_4_weeks[[i]] <- NULL
  })
  
  # Load phone data
  phone_data <- load_phone_data(week_offset_num, week_offset_year)
  if (!is.null(phone_data)) {
    last_4_weeks_phone[[i]] <- phone_data
  } else {
    cat("  Warning: Could not load phone data for week", week_offset_num, "\n")
    last_4_weeks_phone[[i]] <- NULL
  }
}

# Combine last 4 weeks CAD data (remove NULLs first)
last_4_weeks <- last_4_weeks[!sapply(last_4_weeks, is.null)]
if (length(last_4_weeks) > 0) {
  last_4_weeks_combined <- bind_rows(last_4_weeks)
  cat("\nLast 4 weeks CAD data combined:", nrow(last_4_weeks_combined), "total rows\n")
} else {
  cat("\nWarning: No last 4 weeks CAD data available\n")
  last_4_weeks_combined <- NULL
}

# Combine last 4 weeks phone data (remove NULLs first)
last_4_weeks_phone <- last_4_weeks_phone[!sapply(last_4_weeks_phone, is.null)]
if (length(last_4_weeks_phone) > 0) {
  last_4_weeks_phone_combined <- bind_rows(last_4_weeks_phone)
  cat("Last 4 weeks phone data combined:", nrow(last_4_weeks_phone_combined), "total rows\n")
} else {
  cat("Warning: No last 4 weeks phone data available\n")
  last_4_weeks_phone_combined <- NULL
}

cat("\n=== ALL DATA LOADING COMPLETE ===\n")

# Update title dynamically (since we can't use R in YAML directly)
DYNAMIC_TITLE <- paste0("Weekly Report: Week ", WEEK_NUMBER, " (", 
                        WEEK_START_FORMATTED, " through ", WEEK_END_FORMATTED, " ", current_year, ")")

```

```{r}
#| label: global-options
#| include: false
#| cache: false

# Global knitr options for faster renders and consistent behavior
knitr::opts_chunk$set(
  cache = TRUE,           # enable caching by default
  cache.lazy = FALSE,     # avoid lazy loading cache on Windows
  cache.path = paste0('cache/week-', WEEK_NUMBER, '/'),
  fig.retina = 1,         # smaller figures in HTML for speed
  dpi = 96,               # reasonable default DPI for speed
  dev = 'png',            # faster device for HTML
  message = FALSE,
  warning = FALSE
)

# Reduce noisy dplyr summarise messages
options(dplyr.summarise.inform = FALSE)

# Precompute a few shared palettes to avoid repeated paletteer calls
palette_cyan_base <- tryCatch(as.character(paletteer::paletteer_d("ggsci::cyan_material")),
                              error = function(e) c("#E0F2F1", "#B2DFDB", "#80CBC4", "#4DB6AC", "#26A69A", "#009688", "#00897B", "#00796B", "#00695C", "#004D40"))
palette_blue_base <- tryCatch(as.character(paletteer::paletteer_d("cartography::blue.pal")),
                              error = function(e) c("#08306B", "#08519C", "#2171B5", "#4292C6", "#6BAED6", "#9ECAE1", "#C6DBEF", "#DEEBF7"))
palette_red_base  <- tryCatch(as.character(paletteer::paletteer_d("cartography::red.pal")),
                              error = function(e) c("#67000D", "#A50F15", "#CB181D", "#EF3B2C", "#FB6A4A", "#FC9272", "#FCBBA1", "#FEE0D2"))

palette_cyan_24 <- grDevices::colorRampPalette(palette_cyan_base)(24)
palette_blue_24 <- grDevices::colorRampPalette(palette_blue_base)(24)
palette_red_24  <- grDevices::colorRampPalette(palette_red_base)(24)

# Small helper: safe quantiles/iqr with na.rm
q90 <- function(x) as.numeric(quantile(x, 0.9, na.rm = TRUE))
q99 <- function(x) as.numeric(quantile(x, 0.99, na.rm = TRUE))
iqr_safe <- function(x) IQR(x, na.rm = TRUE)
```

```{r}
#| label: flextable-defaults
#| echo: false
#| message: false
#| warning: false
#| cache: false

flextable::set_flextable_defaults(
  theme_fun = flextable::theme_booktabs,
  font.size = 9,
  padding = 3,
  borders = officer::fp_border_default()
)

to_ft <- function(tbl, caption = NULL, header_map = NULL, digits = 2) {
  df <- as.data.frame(tbl)
  ft <- flextable::flextable(df)
  if (!is.null(header_map)) ft <- flextable::set_header_labels(ft, values = header_map)
  if (nrow(df) > 0) ft <- flextable::bg(ft, i = seq(1, nrow(df), by = 2), bg = "#F5F5F5", part = "body")
  ft <- flextable::bold(ft, part = "header")
  ft <- flextable::fontsize(ft, part = "header", size = 9)
  num_cols <- names(df)[vapply(df, is.numeric, logical(1))]
  if (length(num_cols) > 0) {
    ft <- flextable::align(ft, j = num_cols, align = "right", part = "all")
    ft <- flextable::colformat_num(ft, j = num_cols, digits = digits, big.mark = ",")
  }
  if (!is.null(caption)) ft <- flextable::set_caption(ft, caption)
  flextable::autofit(ft)
}
```

## Introduction

This is the weekly report for week `r WEEK_NUMBER` covering the period from `r WEEK_START_FORMATTED` through `r WEEK_END_FORMATTED` 2025. The report will include analyses of the data to emphasize different information that is contained within the data and may be pertinent to both operations and management.

```{r}
#| label: data-processing
#| echo: false
#| output: false
#| cache: true

# Data is now loaded in the dataframes chunk as 'current_week' and 'last_week'
# This chunk processes the current week data for the main report

# Use current_week as the primary dataframe for this report
df <- current_week

cat("Processing current week data:", nrow(df), "rows\n")
cat("Call_Reception column check:", "Call_Reception" %in% names(df), "\n")

# Use lubridate and across() to efficiently parse all date-time columns
df <- df |>
  mutate(across(c(Response_Date,
                   Incident_Start_Time,
                   TimeCallViewed,
                   Incident_Queue_Time,
                   Incident_Dispatch_Time,
                   Incident_Phone_Stop,
                   TimeFirstUnitDispatchAcknowledged,
                   Incident_Enroute_Time,
                   Incident_Arrival_Time,
                   TimeFirstCallCleared,
                   Incident_First_Close_Time,
                   Final_Closed_Time,
                   First_Reopen_Time), ymd_hms))

df$WeekNo <- as.factor(df$WeekNo)
df$Day <- as.factor(df$Day)
df$Hour <- as.factor(sprintf("%02d", as.numeric(trimws(df$Hour))))

# Convert DOW to an ordered factor to respect the sequence of days
df$DOW <- factor(
    df$DOW,
    levels = c("SUN", "MON", "TUE", "WED", "THU", "FRI", "SAT"),
    ordered = TRUE
)

df$ShiftPart <- factor(
  df$ShiftPart,
  levels = c("EARLY", "MIDS", "LATE"),
  ordered = TRUE
)

# Convert Priority_Number to an ordered factor as well
df$Priority_Number <- ordered(df$Priority_Number)

# Convert numeric variables from 'doubles' to integers
df[c('Time_To_Queue', 'Time_To_Dispatch', 'Phone_Time', 'Processing_Time', 'Rollout_Time', 'Transit_Time', 'Total_Call_Time')] <- sapply(df[c('Time_To_Queue', 'Time_To_Dispatch', 'Phone_Time', 'Processing_Time', 'Rollout_Time', 'Transit_Time', 'Total_Call_Time')], as.numeric)

# Process last week data for comparison (same transformations)
df_last <- last_week |>
  mutate(across(c(Response_Date,
                   Incident_Start_Time,
                   TimeCallViewed,
                   Incident_Queue_Time,
                   Incident_Dispatch_Time,
                   Incident_Phone_Stop,
                   TimeFirstUnitDispatchAcknowledged,
                   Incident_Enroute_Time,
                   Incident_Arrival_Time,
                   TimeFirstCallCleared,
                   Incident_First_Close_Time,
                   Final_Closed_Time,
                   First_Reopen_Time), ymd_hms))

df_last$WeekNo <- as.factor(df_last$WeekNo)
df_last$Day <- as.factor(df_last$Day)
df_last$Hour <- as.factor(sprintf("%02d", as.numeric(trimws(df_last$Hour))))
df_last$DOW <- factor(df_last$DOW, levels = c("SUN", "MON", "TUE", "WED", "THU", "FRI", "SAT"), ordered = TRUE)
df_last$ShiftPart <- factor(df_last$ShiftPart, levels = c("EARLY", "MIDS", "LATE"), ordered = TRUE)
df_last$Priority_Number <- ordered(df_last$Priority_Number)
df_last[c('Time_To_Queue', 'Time_To_Dispatch', 'Phone_Time', 'Processing_Time', 'Rollout_Time', 'Transit_Time', 'Total_Call_Time')] <- sapply(df_last[c('Time_To_Queue', 'Time_To_Dispatch', 'Phone_Time', 'Processing_Time', 'Rollout_Time', 'Transit_Time', 'Total_Call_Time')], as.numeric)

cat("Processing last week data:", nrow(df_last), "rows\n")
cat("Both weeks ready for analysis and comparison\n")
```

For this week, there were a total of `r nrow(df)` calls for service. 

## Executive Summary

```{r}
#| label: kpi-calculations
#| echo: false
#| message: false
#| warning: false
#| cache: true

# Calculate KPIs for current and last week
# SLA Compliance Calculations

# LAW P1: ≤60s Time_To_Dispatch
law_p1_current <- df %>% filter(Agency == "POLICE", Priority_Number == "1", !is.na(Time_To_Dispatch))
law_p1_compliant_current <- law_p1_current %>% filter(Time_To_Dispatch <= 60)
law_p1_sla_current <- ifelse(nrow(law_p1_current) > 0, 
                              round((nrow(law_p1_compliant_current) / nrow(law_p1_current)) * 100, 1), 
                              NA)

law_p1_last <- df_last %>% filter(Agency == "POLICE", Priority_Number == "1", !is.na(Time_To_Dispatch))
law_p1_compliant_last <- law_p1_last %>% filter(Time_To_Dispatch <= 60)
law_p1_sla_last <- ifelse(nrow(law_p1_last) > 0, 
                           round((nrow(law_p1_compliant_last) / nrow(law_p1_last)) * 100, 1), 
                           NA)

# LAW P2: ≤120s Time_To_Dispatch
law_p2_current <- df %>% filter(Agency == "POLICE", Priority_Number == "2", !is.na(Time_To_Dispatch))
law_p2_compliant_current <- law_p2_current %>% filter(Time_To_Dispatch <= 120)
law_p2_sla_current <- ifelse(nrow(law_p2_current) > 0, 
                              round((nrow(law_p2_compliant_current) / nrow(law_p2_current)) * 100, 1), 
                              NA)

law_p2_last <- df_last %>% filter(Agency == "POLICE", Priority_Number == "2", !is.na(Time_To_Dispatch))
law_p2_compliant_last <- law_p2_last %>% filter(Time_To_Dispatch <= 120)
law_p2_sla_last <- ifelse(nrow(law_p2_last) > 0, 
                           round((nrow(law_p2_compliant_last) / nrow(law_p2_last)) * 100, 1), 
                           NA)

# FIRE/EMS: ≤64s and ≤106s Time_To_Queue
fire_ems_current <- df %>% filter(Agency %in% c("FIRE", "EMS"), !is.na(Time_To_Queue))
fire_ems_64_compliant_current <- fire_ems_current %>% filter(Time_To_Queue <= 64)
fire_ems_106_compliant_current <- fire_ems_current %>% filter(Time_To_Queue <= 106)
fire_ems_64_sla_current <- ifelse(nrow(fire_ems_current) > 0, 
                                   round((nrow(fire_ems_64_compliant_current) / nrow(fire_ems_current)) * 100, 1), 
                                   NA)
fire_ems_106_sla_current <- ifelse(nrow(fire_ems_current) > 0, 
                                    round((nrow(fire_ems_106_compliant_current) / nrow(fire_ems_current)) * 100, 1), 
                                    NA)

fire_ems_last <- df_last %>% filter(Agency %in% c("FIRE", "EMS"), !is.na(Time_To_Queue))
fire_ems_64_compliant_last <- fire_ems_last %>% filter(Time_To_Queue <= 64)
fire_ems_106_compliant_last <- fire_ems_last %>% filter(Time_To_Queue <= 106)
fire_ems_64_sla_last <- ifelse(nrow(fire_ems_last) > 0, 
                                round((nrow(fire_ems_64_compliant_last) / nrow(fire_ems_last)) * 100, 1), 
                                NA)
fire_ems_106_sla_last <- ifelse(nrow(fire_ems_last) > 0, 
                                 round((nrow(fire_ems_106_compliant_last) / nrow(fire_ems_last)) * 100, 1), 
                                 NA)

# Overall metrics
total_calls_current <- nrow(df)
total_calls_last <- nrow(df_last)
median_ttq_current <- median(df$Time_To_Queue, na.rm = TRUE)
median_ttq_last <- median(df_last$Time_To_Queue, na.rm = TRUE)
median_ttd_current <- median(df$Time_To_Dispatch, na.rm = TRUE)
median_ttd_last <- median(df_last$Time_To_Dispatch, na.rm = TRUE)
median_phone_current <- median(df$Phone_Time, na.rm = TRUE)
median_phone_last <- median(df_last$Phone_Time, na.rm = TRUE)

# Calculate changes
call_volume_change <- total_calls_current - total_calls_last
call_volume_pct <- round((call_volume_change / total_calls_last) * 100, 1)
ttq_change <- median_ttq_current - median_ttq_last
ttq_pct <- round((ttq_change / median_ttq_last) * 100, 1)
ttd_change <- median_ttd_current - median_ttd_last
ttd_pct <- round((ttd_change / median_ttd_last) * 100, 1)
phone_change <- median_phone_current - median_phone_last
phone_pct <- round((phone_change / median_phone_last) * 100, 1)

# Helper function for trend indicators
trend_indicator <- function(value, threshold_good = 0, threshold_warning = 5, lower_is_better = TRUE) {
  if (is.na(value)) return("—")
  if (lower_is_better) {
    if (value <= threshold_good) return("✓")
    if (value <= threshold_warning) return("⚠")
    return("✗")
  } else {
    if (value >= threshold_good) return("✓")
    if (value >= threshold_warning) return("⚠")
    return("✗")
  }
}

# SLA status indicators
law_p1_status <- ifelse(!is.na(law_p1_sla_current), 
                        ifelse(law_p1_sla_current >= 90, "✓", 
                               ifelse(law_p1_sla_current >= 85, "⚠", "✗")), 
                        "—")
law_p2_status <- ifelse(!is.na(law_p2_sla_current), 
                        ifelse(law_p2_sla_current >= 85, "✓", 
                               ifelse(law_p2_sla_current >= 80, "⚠", "✗")), 
                        "—")
fire_ems_64_status <- ifelse(!is.na(fire_ems_64_sla_current), 
                              ifelse(fire_ems_64_sla_current >= 90, "✓", 
                                     ifelse(fire_ems_64_sla_current >= 85, "⚠", "✗")), 
                              "—")
fire_ems_106_status <- ifelse(!is.na(fire_ems_106_sla_current), 
                               ifelse(fire_ems_106_sla_current >= 95, "✓", 
                                      ifelse(fire_ems_106_sla_current >= 90, "⚠", "✗")), 
                               "—")

# ---------------------------------------------------------------------------
# Phone (9-1-1) Answer Time Metrics Integration for Executive Summary
# Standards: 90% answered <=15s, 95% answered <=20s
# Compute current week, last week, and rolling 4-week average for foundation
# ---------------------------------------------------------------------------

compute_phone_week_metrics <- function(phone_df) {
  if (is.null(phone_df)) return(NULL)
  phone_df <- phone_df %>% mutate(DateTime = suppressWarnings(mdy_hm(DateTime)))
  num_cols <- setdiff(names(phone_df), c("DateTime","Hour"))
  phone_df[num_cols] <- lapply(phone_df[num_cols], function(x) suppressWarnings(as.numeric(x)))
  answered <- sum(phone_df$`911_T`, na.rm = TRUE) - sum(phone_df$`911_AB`, na.rm = TRUE)
  if (answered <= 0) return(NULL)
  within15 <- sum(phone_df$`911_10`, na.rm = TRUE) + sum(phone_df$`911_15`, na.rm = TRUE)
  within20 <- within15 + sum(phone_df$`911_20`, na.rm = TRUE)
  tibble(answered = answered,
         within15 = within15,
         within20 = within20,
         pct15 = (within15/answered)*100,
         pct20 = (within20/answered)*100)
}

phone_current_metrics <- compute_phone_week_metrics(current_week_phone)
phone_last_metrics    <- compute_phone_week_metrics(last_week_phone)

# Rolling 4-week (prior weeks only) for baseline
phone_rolling_prior4 <- NA_real_
phone_rolling_prior4_20 <- NA_real_
if (!is.null(last_4_weeks_phone_combined)) {
  prior4_metrics <- last_4_weeks_phone_combined %>% mutate(DateTime = suppressWarnings(mdy_hm(DateTime)))
  num_cols <- setdiff(names(prior4_metrics), c("DateTime","Hour"))
  prior4_metrics[num_cols] <- lapply(prior4_metrics[num_cols], function(x) suppressWarnings(as.numeric(x)))
  answered4 <- sum(prior4_metrics$`911_T`, na.rm = TRUE) - sum(prior4_metrics$`911_AB`, na.rm = TRUE)
  within15_4 <- sum(prior4_metrics$`911_10`, na.rm = TRUE) + sum(prior4_metrics$`911_15`, na.rm = TRUE)
  within20_4 <- within15_4 + sum(prior4_metrics$`911_20`, na.rm = TRUE)
  if (answered4 > 0) {
    phone_rolling_prior4 <- (within15_4/answered4)*100
    phone_rolling_prior4_20 <- (within20_4/answered4)*100
  }
}

# Status indicators for phone compliance
phone_status_15 <- ifelse(!is.null(phone_current_metrics),
                          ifelse(phone_current_metrics$pct15 >= 90, "✓",
                                 ifelse(phone_current_metrics$pct15 >= 85, "⚠", "✗")),"—")
phone_status_20 <- ifelse(!is.null(phone_current_metrics),
                          ifelse(phone_current_metrics$pct20 >= 95, "✓",
                                 ifelse(phone_current_metrics$pct20 >= 90, "⚠", "✗")),"—")
```

### Weekly KPI Summary

```{r}
#| label: kpi-summary-table
#| echo: false
#| message: false
#| warning: false

kpi_summary <- tibble(
  Metric = c(
    "Total Call Volume",
    "Median Time to Queue (s)",
    "Median Time to Dispatch (s)",
    "Median Phone Time (s)",
    "LAW P1 SLA (≤60s)",
    "LAW P2 SLA (≤120s)",
    "FIRE/EMS SLA (≤64s)",
    "FIRE/EMS SLA (≤106s)",
    "9-1-1 <=15s (≥90%)",
    "9-1-1 <=20s (≥95%)"
  ),
  Current_Week = c(
    format(total_calls_current, big.mark = ","),
    round(median_ttq_current, 1),
    round(median_ttd_current, 1),
    round(median_phone_current, 1),
    paste0(law_p1_sla_current, "%"),
    paste0(law_p2_sla_current, "%"),
    paste0(fire_ems_64_sla_current, "%"),
    paste0(fire_ems_106_sla_current, "%"),
    ifelse(!is.null(phone_current_metrics), paste0(round(phone_current_metrics$pct15,1), "%"), "—"),
    ifelse(!is.null(phone_current_metrics), paste0(round(phone_current_metrics$pct20,1), "%"), "—")
  ),
  Prior_Week = c(
    format(total_calls_last, big.mark = ","),
    round(median_ttq_last, 1),
    round(median_ttd_last, 1),
    round(median_phone_last, 1),
    paste0(law_p2_sla_last, "%"),
    paste0(law_p2_sla_last, "%"),
    paste0(fire_ems_64_sla_last, "%"),
    paste0(fire_ems_106_sla_last, "%"),
    ifelse(!is.null(phone_last_metrics), paste0(round(phone_last_metrics$pct15,1), "%"), "—"),
    ifelse(!is.null(phone_last_metrics), paste0(round(phone_last_metrics$pct20,1), "%"), "—")
  ),
  Change = c(
    paste0(ifelse(call_volume_change >= 0, "+", ""), call_volume_change, " (", 
           ifelse(call_volume_pct >= 0, "+", ""), call_volume_pct, "%)"),
    paste0(ifelse(ttq_change >= 0, "+", ""), round(ttq_change, 1), " (", 
           ifelse(ttq_pct >= 0, "+", ""), ttq_pct, "%)"),
    paste0(ifelse(ttd_change >= 0, "+", ""), round(ttd_change, 1), " (", 
           ifelse(ttd_pct >= 0, "+", ""), ttd_pct, "%)"),
    paste0(ifelse(phone_change >= 0, "+", ""), round(phone_change, 1), " (", 
           ifelse(phone_pct >= 0, "+", ""), phone_pct, "%)"),
    paste0(ifelse(!is.na(law_p1_sla_current) && !is.na(law_p1_sla_last), 
                  paste0(ifelse(law_p1_sla_current - law_p1_sla_last >= 0, "+", ""), 
                         round(law_p1_sla_current - law_p1_sla_last, 1), "%"), "—")),
    paste0(ifelse(!is.na(law_p2_sla_current) && !is.na(law_p2_sla_last), 
                  paste0(ifelse(law_p2_sla_current - law_p2_sla_last >= 0, "+", ""), 
                         round(law_p2_sla_current - law_p2_sla_last, 1), "%"), "—")),
    paste0(ifelse(!is.na(fire_ems_64_sla_current) && !is.na(fire_ems_64_sla_last), 
                  paste0(ifelse(fire_ems_64_sla_current - fire_ems_64_sla_last >= 0, "+", ""), 
                         round(fire_ems_64_sla_current - fire_ems_64_sla_last, 1), "%"), "—")),
    paste0(ifelse(!is.na(fire_ems_106_sla_current) && !is.na(fire_ems_106_sla_last), 
        paste0(ifelse(fire_ems_106_sla_current - fire_ems_106_sla_last >= 0, "+", ""), 
          round(fire_ems_106_sla_current - fire_ems_106_sla_last, 1), "%"), "—")),
    ifelse(!is.null(phone_current_metrics) && !is.null(phone_last_metrics), 
      paste0(ifelse(round(phone_current_metrics$pct15 - phone_last_metrics$pct15,1) >= 0, "+", ""),
        round(phone_current_metrics$pct15 - phone_last_metrics$pct15,1), " pts"), "—"),
    ifelse(!is.null(phone_current_metrics) && !is.null(phone_last_metrics), 
      paste0(ifelse(round(phone_current_metrics$pct20 - phone_last_metrics$pct20,1) >= 0, "+", ""),
        round(phone_current_metrics$pct20 - phone_last_metrics$pct20,1), " pts"), "—")
  ),
  Status = c(
    "—",
    "—",
    "—",
    "—",
    law_p1_status,
    law_p2_status,
    fire_ems_64_status,
    fire_ems_106_status,
    phone_status_15,
    phone_status_20
  )
)

to_ft(
  kpi_summary,
  caption = paste0("Weekly KPI Summary - Week ", WEEK_NUMBER, " vs Week ", LAST_WEEK_NUMBER),
  header_map = list(
    Metric = "Key Performance Indicator",
    Current_Week = "This Week",
    Prior_Week = "Last Week",
    Change = "Change (Δ%)",
    Status = "Status"
  ),
  digits = 1
)

# Store phone summary metrics for use in executive insights chunk
PHONE_PCT_15_CURRENT <- ifelse(!is.null(phone_current_metrics), round(phone_current_metrics$pct15,1), NA)
PHONE_PCT_20_CURRENT <- ifelse(!is.null(phone_current_metrics), round(phone_current_metrics$pct20,1), NA)
PHONE_PCT_15_LAST    <- ifelse(!is.null(phone_last_metrics), round(phone_last_metrics$pct15,1), NA)
PHONE_PCT_20_LAST    <- ifelse(!is.null(phone_last_metrics), round(phone_last_metrics$pct20,1), NA)
PHONE_PCT_15_ROLL4   <- ifelse(!is.na(phone_rolling_prior4), round(phone_rolling_prior4,1), NA)
PHONE_PCT_20_ROLL4   <- ifelse(!is.na(phone_rolling_prior4_20), round(phone_rolling_prior4_20,1), NA)
```

**Status Legend:** ✓ = Meets Target | ⚠ = Warning | ✗ = Below Target | — = Not Applicable

### Key Insights

```{r}
#| label: executive-insights
#| echo: false
#| results: asis

insights <- list()

# Call volume insight
if (abs(call_volume_pct) >= 5) {
  direction <- ifelse(call_volume_change > 0, "increased", "decreased")
  insights <- append(insights, 
    paste0("**Call Volume**: Total calls ", direction, " by ", abs(call_volume_change), 
           " (", abs(call_volume_pct), "%) compared to last week."))
}

# Response time insights
if (abs(ttd_pct) >= 10) {
  direction <- ifelse(ttd_change > 0, "increased", "decreased")
  insights <- append(insights,
    paste0("**Response Time**: Median time to dispatch ", direction, " by ", 
           abs(round(ttd_change, 1)), " seconds (", abs(ttd_pct), "%)."))
}

# SLA compliance insights
sla_issues <- c()
if (!is.na(law_p1_sla_current) && law_p1_sla_current < 90) {
  sla_issues <- append(sla_issues, paste0("LAW P1 at ", law_p1_sla_current, "%"))
}
if (!is.na(law_p2_sla_current) && law_p2_sla_current < 85) {
  sla_issues <- append(sla_issues, paste0("LAW P2 at ", law_p2_sla_current, "%"))
}
if (!is.na(fire_ems_64_sla_current) && fire_ems_64_sla_current < 90) {
  sla_issues <- append(sla_issues, paste0("FIRE/EMS 64s at ", fire_ems_64_sla_current, "%"))
}

if (length(sla_issues) > 0) {
  insights <- append(insights,
    paste0("**SLA Compliance**: ", length(sla_issues), " metric(s) below target: ", 
           paste(sla_issues, collapse = ", "), "."))
}

# Phone time insight
if (median_phone_current > 120 && phone_pct > 5) {
  insights <- append(insights,
    paste0("**Call Complexity**: Median phone time at ", round(median_phone_current), 
           " seconds, up ", abs(phone_pct), "%, suggesting more complex calls this week."))
}

# 9-1-1 phone standards integration (current, prior, rolling baseline)
if (!is.na(PHONE_PCT_15_CURRENT)) {
  status15 <- ifelse(PHONE_PCT_15_CURRENT >= 90, "met", ifelse(PHONE_PCT_15_CURRENT >= 85, "near", "below"))
  status20 <- ifelse(PHONE_PCT_20_CURRENT >= 95, "met", ifelse(PHONE_PCT_20_CURRENT >= 90, "near", "below"))
  insights <- append(insights,
    paste0("**9-1-1 Answer Standards**: 15s ", status15, " (", PHONE_PCT_15_CURRENT, "%), 20s ", status20, " (", PHONE_PCT_20_CURRENT, "%)."))
  if (!is.na(PHONE_PCT_15_LAST)) {
    delta15 <- round(PHONE_PCT_15_CURRENT - PHONE_PCT_15_LAST,1)
    delta20 <- round(PHONE_PCT_20_CURRENT - PHONE_PCT_20_LAST,1)
    dir15 <- ifelse(delta15>0,"improved", ifelse(delta15<0,"declined","held steady"))
    dir20 <- ifelse(delta20>0,"improved", ifelse(delta20<0,"declined","held steady"))
    insights <- append(insights,
      paste0("**Week-over-Week 9-1-1**: 15s ", dir15, " (", ifelse(delta15>0,"+",""), delta15, " pts), 20s ", dir20, " (", ifelse(delta20>0,"+",""), delta20, " pts)."))
  }
  if (!is.na(PHONE_PCT_15_ROLL4)) {
    diff_roll15 <- round(PHONE_PCT_15_CURRENT - PHONE_PCT_15_ROLL4,1)
    diff_roll20 <- round(PHONE_PCT_20_CURRENT - PHONE_PCT_20_ROLL4,1)
    insights <- append(insights,
      paste0("**Relative to Prior 4 Weeks**: 15s ", ifelse(diff_roll15>=0,"above","below"), " baseline by ", abs(diff_roll15), " pts; 20s ", ifelse(diff_roll20>=0,"above","below"), " baseline by ", abs(diff_roll20), " pts."))
  }
}

# Output insights
if (length(insights) > 0) {
  cat("\n")
  for (i in seq_along(insights)) {
    cat(paste0(i, ". ", insights[[i]], "\n\n"))
  }
} else {
  cat("\nOperational performance remains stable with no significant week-over-week changes.\n")
}
```

## Data Cleaning

In order to have a good dataset for analysis, some data cleaning was performed. The first step is to check for missing values in the dataset.

```{r}
#| label: missing-values-plot
#| echo: false
#| warning: false
#| fig-cap: "Prevalence of missing values. Only columns with missing data are shown."
#| cache: true

# ...existing code...
# Count columns with any missing data
missing_cols_count <- sum(colSums(is.na(df)) > 0)

# Get named vector of missing counts per column
missing_counts <- colSums(is.na(df))

# Optionally, create a tibble for easy use
missing_summary <- tibble(
  column = names(df),
  missing = missing_counts
)

# Find the column with the largest number of missing values
max_missing_col <- names(missing_counts)[which.max(missing_counts)]
max_missing_count <- max(missing_counts)

# Calculate percentage of calls without Incident_Arrival_Time
incident_arrival_missing_pct <- round(missing_counts["Incident_Arrival_Time"] / nrow(df) * 100, 1)

# Identify columns with missing data for inline reporting
incident_arrival_missing_count <- missing_counts["Incident_Arrival_Time"]

# Create the missing values plot
missing_plot <- inspect_na(df) |>
  dplyr::filter(cnt > 0) |>
  show_plot()

# Optional: adjust theme if the plot supports ggplot theme
if (inherits(missing_plot, "ggplot")) {
  missing_plot <- missing_plot +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
      axis.text.y = element_text(size = 9)
    )
}

# Display the plot
print(missing_plot)
```

From this plot, we can see that there are only `r missing_cols_count` columns with missing data. Of those, the column with the largest number of missing values is `r max_missing_col`. That is something that we would like to see because that means that most of our calls are closed once and left that way. Later, we will look deeper into those calls to see if there are any patterns to those calls. There were `r incident_arrival_missing_count` calls that did not have a recorded time that the call arrived, representing `r incident_arrival_missing_pct`% of calls for the week. We will have to determine if they were cancelled or how many of those were mutual aid calls where we did not receive a phone call.

## Exploratory Analysis

One of the first analyses is to break down different factor elements to see what we have in the dataset. Since the data source for this report is the CAD system, the counts of service calls may not correlate to phone volumes. We will have to, in future, find good methods to integrate the phone date into the reports.

### Phone Summary

```{r}
#| label: phone-data-summary
#| echo: false
#| message: false
#| warning: false

if (!is.null(current_week_phone)) {
  # Parse DateTime column
  current_week_phone <- current_week_phone %>%
    mutate(DateTime = mdy_hm(DateTime))
  
  # Convert numeric columns
  numeric_cols <- names(current_week_phone)[!names(current_week_phone) %in% c("DateTime", "Hour")]
  current_week_phone[numeric_cols] <- lapply(current_week_phone[numeric_cols], as.numeric)
  
  # Calculate key metrics
  total_911_calls <- sum(current_week_phone$`911_T`, na.rm = TRUE)
  total_admin_calls <- sum(current_week_phone$ADM_T, na.rm = TRUE)
  total_phone_calls <- sum(current_week_phone$Total, na.rm = TRUE)
  
  avg_911_pct_10 <- mean(current_week_phone$`911_PCT_10`, na.rm = TRUE) * 100
  avg_911_pct_15 <- mean(current_week_phone$`911_PCT_15`, na.rm = TRUE) * 100
  avg_911_pct_20 <- mean(current_week_phone$`911_PCT_20`, na.rm = TRUE) * 100
  
  avg_adm_pct_10 <- mean(current_week_phone$ADM_PCT_10, na.rm = TRUE) * 100
  avg_adm_pct_15 <- mean(current_week_phone$PCT_ADM_15, na.rm = TRUE) * 100
  avg_adm_pct_20 <- mean(current_week_phone$ADM_PCT_20, na.rm = TRUE) * 100
  
  avg_total_pct_10 <- mean(current_week_phone$TTL_PCT_10, na.rm = TRUE) * 100
  avg_total_pct_15 <- mean(current_week_phone$TTL_PCT_15, na.rm = TRUE) * 100
  avg_total_pct_20 <- mean(current_week_phone$TTL_PCT_20, na.rm = TRUE) * 100
  
  # Calculate abandoned call metrics
  total_911_abandoned <- sum(current_week_phone$`911_AB`, na.rm = TRUE)
  total_adm_abandoned <- sum(current_week_phone$ADM_AB, na.rm = TRUE)
  
  pct_911_abandoned <- (total_911_abandoned / total_911_calls) * 100
  pct_adm_abandoned <- (total_adm_abandoned / total_admin_calls) * 100
  
  phone_data_available <- TRUE
} else {
  phone_data_available <- FALSE
}
```

```{r}
#| label: phone-summary-output
#| echo: false
#| results: asis

if (phone_data_available) {
  cat("This week's phone system data provides insights into call answering performance across both 9-1-1 emergency lines and administrative lines.\n\n")
  
  cat("#### Call Volume Overview\n\n")
  cat(paste0("- **Total Phone Calls:** ", format(total_phone_calls, big.mark = ","), "\n"))
  cat(paste0("- **9-1-1 Emergency Calls:** ", format(total_911_calls, big.mark = ","), 
             " (", round((total_911_calls/total_phone_calls)*100, 1), "%)\n"))
  cat(paste0("- **Administrative Calls:** ", format(total_admin_calls, big.mark = ","), 
             " (", round((total_admin_calls/total_phone_calls)*100, 1), "%)\n\n"))
  
  cat("#### Answer Time Performance\n\n")
  cat("**9-1-1 Emergency Lines:**\n\n")
  cat(paste0("- Answered within 10 seconds: ", round(avg_911_pct_10, 1), "%\n"))
  cat(paste0("- Answered within 15 seconds: ", round(avg_911_pct_15, 1), "%\n"))
  cat(paste0("- Answered within 20 seconds: ", round(avg_911_pct_20, 1), "%\n\n"))
  
  cat("**Administrative Lines:**\n\n")
  cat(paste0("- Answered within 10 seconds: ", round(avg_adm_pct_10, 1), "%\n"))
  cat(paste0("- Answered within 15 seconds: ", round(avg_adm_pct_15, 1), "%\n"))
  cat(paste0("- Answered within 20 seconds: ", round(avg_adm_pct_20, 1), "%\n\n"))
  
  cat("**Overall Performance:**\n\n")
  cat(paste0("- All calls answered within 10 seconds: ", round(avg_total_pct_10, 1), "%\n"))
  cat(paste0("- All calls answered within 15 seconds: ", round(avg_total_pct_15, 1), "%\n"))
  cat(paste0("- All calls answered within 20 seconds: ", round(avg_total_pct_20, 1), "%\n\n"))
  
  cat("#### Abandoned Calls\n\n")
  cat(paste0("- **9-1-1 Abandoned:** ", format(total_911_abandoned, big.mark = ","), 
             " (", round(pct_911_abandoned, 2), "% of 9-1-1 calls)\n"))
  cat(paste0("- **Administrative Abandoned:** ", format(total_adm_abandoned, big.mark = ","), 
             " (", round(pct_adm_abandoned, 2), "% of admin calls)\n\n"))
  
  # Performance assessment
  if (avg_911_pct_10 >= 90) {
    cat("*9-1-1 performance is meeting the 10-second answer standard.*\n\n")
  } else if (avg_911_pct_10 >= 80) {
    cat("*9-1-1 performance is approaching the 10-second answer standard but requires attention.*\n\n")
  } else {
    cat("*9-1-1 performance is below the 10-second answer standard and requires immediate attention.*\n\n")
  }
  
} else {
  cat("*Phone data is not available for this reporting period.*\n\n")
}
```

```{r}
#| label: phone-standards-comparison
#| echo: false
#| message: false
#| warning: false

# Compare current vs last week 9-1-1 answer performance against standards
# Standards: 90% answered <=15s, 95% answered <=20s

compute_phone_metrics <- function(phone_df) {
  if (is.null(phone_df)) return(NULL)
  # Ensure types
  phone_df <- phone_df %>% mutate(DateTime = suppressWarnings(mdy_hm(DateTime)))
  numeric_cols <- setdiff(names(phone_df), c("DateTime", "Hour"))
  phone_df[numeric_cols] <- lapply(phone_df[numeric_cols], function(x) suppressWarnings(as.numeric(x)))
  # Derive answered call counts (exclude abandoned)
  answered_calls <- sum(phone_df$`911_T`, na.rm = TRUE) - sum(phone_df$`911_AB`, na.rm = TRUE)
  if (answered_calls <= 0) return(NULL)
  within_15 <- sum(phone_df$`911_10`, na.rm = TRUE) + sum(phone_df$`911_15`, na.rm = TRUE)
  within_20 <- within_15 + sum(phone_df$`911_20`, na.rm = TRUE)
  pct_15 <- (within_15 / answered_calls) * 100
  pct_20 <- (within_20 / answered_calls) * 100
  tibble(answered_calls = answered_calls,
         within_15 = within_15,
         within_20 = within_20,
         pct_15 = pct_15,
         pct_20 = pct_20)
}

current_metrics <- compute_phone_metrics(current_week_phone)
last_metrics    <- compute_phone_metrics(last_week_phone)

standards_tbl <- NULL
if (!is.null(current_metrics)) {
  standards_tbl <- tibble(
    Metric = c("9-1-1 answered <=15s", "9-1-1 answered <=20s"),
    `Current Week %` = c(round(current_metrics$pct_15, 1), round(current_metrics$pct_20, 1)),
    `Last Week %` = c(if (!is.null(last_metrics)) round(last_metrics$pct_15, 1) else NA_real_,
                      if (!is.null(last_metrics)) round(last_metrics$pct_20, 1) else NA_real_),
    Standard = c(">= 90%", ">= 95%"),
    Status = c(
      ifelse(current_metrics$pct_15 >= 90, "✓", ifelse(current_metrics$pct_15 >= 85, "⚠", "✗")),
      ifelse(current_metrics$pct_20 >= 95, "✓", ifelse(current_metrics$pct_20 >= 90, "⚠", "✗"))
    )
  )
}

standards_tbl_out <- if (!is.null(standards_tbl)) to_ft(standards_tbl, caption = "9-1-1 Answer Time Standards Comparison", digits = 1) else NULL

standards_tbl_out
```

```{r}
#| label: phone-heatmaps-dow
#| echo: false
#| message: false
#| warning: false
#| fig-width: 10
#| fig-height: 6

if (!is.null(current_week_phone) && nrow(current_week_phone) > 0) {
  # Prepare hourly + DOW summaries for 9-1-1 and Admin calls
  phone_df_hm <- current_week_phone %>%
    dplyr::mutate(
      Hour_numeric = suppressWarnings(as.integer(as.numeric(Hour))),
      dt_parsed = suppressWarnings(lubridate::ymd_hms(DateTime, quiet = TRUE)),
      dow_lab = lubridate::wday(dt_parsed, label = TRUE, abbr = TRUE, week_start = 7),
      DOW = factor(toupper(as.character(dow_lab)),
                   levels = c("SUN","MON","TUE","WED","THU","FRI","SAT"), ordered = TRUE)
    ) %>%
    dplyr::filter(!is.na(Hour_numeric), !is.na(DOW))

  hm_911 <- phone_df_hm %>%
    dplyr::group_by(DOW, Hour_numeric) %>%
    dplyr::summarise(call_count = sum(`911_T`, na.rm = TRUE), .groups = "drop")
  max_911 <- max(hm_911$call_count, na.rm = TRUE)
  hm_911  <- hm_911 %>% dplyr::mutate(label_color = ifelse(call_count >= 0.6 * max_911, "white", "black"))

  hm_adm <- phone_df_hm %>%
    dplyr::group_by(DOW, Hour_numeric) %>%
    dplyr::summarise(call_count = sum(ADM_T, na.rm = TRUE), .groups = "drop")
  max_adm <- max(hm_adm$call_count, na.rm = TRUE)
  hm_adm  <- hm_adm %>% dplyr::mutate(label_color = ifelse(call_count >= 0.6 * max_adm, "white", "black"))

  # 9-1-1 heatmap
  p_hm_911 <- ggplot(hm_911, aes(x = Hour_numeric, y = DOW, fill = call_count)) +
    geom_tile(color = "white") +
    geom_text(aes(label = scales::comma(call_count), color = label_color), size = 3.1) +
    scale_color_identity() +
    scale_x_continuous(breaks = seq(0, 23, by = 2)) +
    scale_fill_gradient(low = "#FDEDEC", high = "#8B0000", name = "Calls") +
    labs(title = "9-1-1 Calls by Hour and Day of Week (Current Week)",
         x = "Hour of Day", y = NULL) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 13))

  # Administrative heatmap
  p_hm_adm <- ggplot(hm_adm, aes(x = Hour_numeric, y = DOW, fill = call_count)) +
    geom_tile(color = "white") +
    geom_text(aes(label = scales::comma(call_count), color = label_color), size = 3.1) +
    scale_color_identity() +
    scale_x_continuous(breaks = seq(0, 23, by = 2)) +
    scale_fill_gradient(low = "#EAF2F8", high = "#00008B", name = "Calls") +
    labs(title = "Administrative Calls by Hour and Day of Week (Current Week)",
         x = "Hour of Day", y = NULL) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 13))

  print(p_hm_911)
  cat("\n\n")
  print(p_hm_adm)
} else {
  cat("*Phone data not available for hourly heatmaps.*\n\n")
}
```

```{r}
#| label: phone-heatmaps-analysis
#| echo: false
#| results: asis

if (!is.null(current_week_phone) && nrow(current_week_phone) > 0) {
  # Reuse prepared fields to compute quick insights
  phone_df_hm2 <- current_week_phone %>%
    dplyr::mutate(
      Hour_numeric = suppressWarnings(as.integer(as.numeric(Hour))),
      dt_parsed = suppressWarnings(lubridate::ymd_hms(DateTime, quiet = TRUE)),
      dow_lab = lubridate::wday(dt_parsed, label = TRUE, abbr = TRUE, week_start = 7),
      DOW = factor(toupper(as.character(dow_lab)),
                   levels = c("SUN","MON","TUE","WED","THU","FRI","SAT"), ordered = TRUE)
    ) %>%
    dplyr::filter(!is.na(Hour_numeric), !is.na(DOW))

  hm_911 <- phone_df_hm2 %>%
    dplyr::group_by(DOW, Hour_numeric) %>%
    dplyr::summarise(call_count = sum(`911_T`, na.rm = TRUE), .groups = "drop")
  hm_adm <- phone_df_hm2 %>%
    dplyr::group_by(DOW, Hour_numeric) %>%
    dplyr::summarise(call_count = sum(ADM_T, na.rm = TRUE), .groups = "drop")

  top_911 <- hm_911 %>% dplyr::filter(call_count == max(call_count, na.rm = TRUE)) %>% dplyr::slice(1)
  top_adm <- hm_adm %>% dplyr::filter(call_count == max(call_count, na.rm = TRUE)) %>% dplyr::slice(1)

  if (nrow(top_911) == 1) {
    cat(paste0("- 9-1-1 busiest cell: ", as.character(top_911$DOW),
               " at ", sprintf("%02d", top_911$Hour_numeric), ":00 with ",
               top_911$call_count, " calls.\n"))
  }
  if (nrow(top_adm) == 1) {
    cat(paste0("- Administrative busiest cell: ", as.character(top_adm$DOW),
               " at ", sprintf("%02d", top_adm$Hour_numeric), ":00 with ",
               top_adm$call_count, " calls.\n\n"))
  }
} else {
  cat("*No phone data available for hourly heatmap analysis.*\n\n")
}
```

```{r}
#| label: phone-standards-narrative
#| echo: false
#| results: asis

if (!is.null(current_metrics)) {
  cat("**9-1-1 Standards Adherence Narrative:**\n\n")
  # 15s standard narrative
  if (current_metrics$pct_15 >= 90) {
    cat(paste0("- The center met the 15-second standard (", round(current_metrics$pct_15,1), "%)."))
  } else if (current_metrics$pct_15 >= 85) {
    cat(paste0("- The center is marginally below the 15-second standard (", round(current_metrics$pct_15,1), "%), monitoring recommended."))
  } else {
    cat(paste0("- The center fell below the 15-second standard (", round(current_metrics$pct_15,1), "%), immediate corrective action advised."))
  }
  cat("\n")
  # 20s standard narrative
  if (current_metrics$pct_20 >= 95) {
    cat(paste0("- The 20-second standard was met (", round(current_metrics$pct_20,1), "%)."))
  } else if (current_metrics$pct_20 >= 90) {
    cat(paste0("- Slightly below the 20-second standard (", round(current_metrics$pct_20,1), "%), performance should be reviewed."))
  } else {
    cat(paste0("- Below the 20-second standard (", round(current_metrics$pct_20,1), "%), urgent focus required."))
  }
  cat("\n")
  # Week-over-week comparison
  if (!is.null(last_metrics)) {
    delta_15 <- round(current_metrics$pct_15 - last_metrics$pct_15,1)
    delta_20 <- round(current_metrics$pct_20 - last_metrics$pct_20,1)
    dir_15 <- ifelse(delta_15>0,"improved","declined")
    dir_20 <- ifelse(delta_20>0,"improved","declined")
    if (delta_15==0) dir_15 <- "held steady"
    if (delta_20==0) dir_20 <- "held steady"
    cat(paste0("- Week-over-week: 15s compliance ", dir_15, " (", ifelse(delta_15>=0,"+",""), delta_15, " pts); 20s compliance ", dir_20, " (", ifelse(delta_20>=0,"+",""), delta_20, " pts)."))
  } else {
    cat("- Prior week phone data unavailable; no week-over-week comparison.")
  }
  cat("\n\n")
} else {
  cat("*9-1-1 standards comparison not available (no phone data).*\n\n")
}
```

```{r}
#| label: phone-4week-trend-data
#| echo: false
#| message: false
#| warning: false
#| cache: true

# Build 4-week trend dataset for 9-1-1 volumes, percentages, and abandoned calls
# This includes the current week plus the last 4 weeks (5 total weeks)

build_phone_weekly_summary <- function(phone_df, week_num, week_start) {
  if (is.null(phone_df)) return(NULL)
  phone_df <- phone_df %>% mutate(DateTime = suppressWarnings(mdy_hm(DateTime)))
  num_cols <- setdiff(names(phone_df), c("DateTime", "Hour"))
  phone_df[num_cols] <- lapply(phone_df[num_cols], function(x) suppressWarnings(as.numeric(x)))
  
  total_911 <- sum(phone_df$`911_T`, na.rm = TRUE)
  abandoned_911 <- sum(phone_df$`911_AB`, na.rm = TRUE)
  answered_911 <- total_911 - abandoned_911
  
  if (answered_911 <= 0) return(NULL)
  
  within_10 <- sum(phone_df$`911_10`, na.rm = TRUE)
  within_15 <- within_10 + sum(phone_df$`911_15`, na.rm = TRUE)
  within_20 <- within_15 + sum(phone_df$`911_20`, na.rm = TRUE)
  
  tibble(
    week_num = week_num,
    week_start = week_start,
    total_911 = total_911,
    answered_911 = answered_911,
    abandoned_911 = abandoned_911,
    abandoned_pct = (abandoned_911 / total_911) * 100,
    within_10 = within_10,
    within_15 = within_15,
    within_20 = within_20,
    pct_10 = (within_10 / answered_911) * 100,
    pct_15 = (within_15 / answered_911) * 100,
    pct_20 = (within_20 / answered_911) * 100
  )
}

# Gather weekly summaries for current week + last 4 weeks
phone_weekly_trends <- list()

# Current week
if (!is.null(current_week_phone)) {
  phone_weekly_trends[[1]] <- build_phone_weekly_summary(current_week_phone, WEEK_NUMBER, WEEK_START_DATE)
}

# Last week
if (!is.null(last_week_phone)) {
  phone_weekly_trends[[2]] <- build_phone_weekly_summary(last_week_phone, LAST_WEEK_NUMBER, LAST_WEEK_START_DATE)
}

# Prior 4 weeks (individual)
if (length(last_4_weeks_phone) > 0) {
  for (i in seq_along(last_4_weeks_phone)) {
    if (!is.null(last_4_weeks_phone[[i]])) {
      week_offset_start <- current_week_start - weeks(i)
      week_offset_num <- isoweek(week_offset_start + days(3))
      phone_weekly_trends[[i + 2]] <- build_phone_weekly_summary(last_4_weeks_phone[[i]], week_offset_num, week_offset_start)
    }
  }
}

# Combine and sort by week
phone_weekly_trends <- phone_weekly_trends[!sapply(phone_weekly_trends, is.null)]
if (length(phone_weekly_trends) > 0) {
  phone_trends_df <- bind_rows(phone_weekly_trends) %>%
    arrange(week_start) %>%
    mutate(week_label = paste0("Wk ", week_num, "\n", format(week_start, "%m/%d")))
} else {
  phone_trends_df <- NULL
}
```

```{r}
#| label: phone-4week-volume-plot
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "9-1-1 Call Volumes and Answer Performance (Last 4 Weeks)"
#| fig-width: 10
#| fig-height: 6

if (!is.null(phone_trends_df) && nrow(phone_trends_df) > 0) {
  # Create dual-axis plot: volumes (bars) + percentages (lines)
  
  # Reshape for plotting
  volume_data <- phone_trends_df %>%
    dplyr::select(week_label, week_start, total_911, answered_911) %>%
    pivot_longer(cols = c(total_911, answered_911), names_to = "metric", values_to = "count") %>%
    mutate(metric_label = case_when(
      metric == "total_911" ~ "Total 9-1-1 Calls",
      metric == "answered_911" ~ "Answered Calls"
    ))
  
  pct_data <- phone_trends_df %>%
    dplyr::select(week_label, week_start, pct_10, pct_15, pct_20)
  
  # Volume bar plot
  p_volume <- ggplot(volume_data, aes(x = reorder(week_label, week_start), y = count, fill = metric_label)) +
    geom_col(position = "dodge", width = 0.7) +
    geom_text(aes(label = scales::comma(count)), 
              position = position_dodge(width = 0.7), 
              vjust = -0.3, size = 3) +
    scale_fill_manual(values = c("Total 9-1-1 Calls" = palette_cyan_base[8], 
                                   "Answered Calls" = palette_blue_base[7])) +
    labs(title = "9-1-1 Call Volumes Over Last 4 Weeks",
         x = "Week",
         y = "Call Count",
         fill = "Metric") +
    theme_minimal() +
    theme(legend.position = "bottom",
          plot.title = element_text(hjust = 0.5, face = "bold", size = 14),
          axis.text.x = element_text(size = 9))
  
  # Percentage line plot
  p_pct <- ggplot(phone_trends_df, aes(x = reorder(week_label, week_start))) +
    geom_line(aes(y = pct_15, color = "<=15s", group = 1), linewidth = 1.2) +
    geom_point(aes(y = pct_15, color = "<=15s"), size = 3) +
    geom_line(aes(y = pct_20, color = "<=20s", group = 1), linewidth = 1.2) +
    geom_point(aes(y = pct_20, color = "<=20s"), size = 3) +
    geom_hline(yintercept = 90, linetype = "dashed", color = "darkgreen", linewidth = 0.8) +
    geom_hline(yintercept = 95, linetype = "dashed", color = "darkblue", linewidth = 0.8) +
    geom_text(aes(y = pct_15, label = paste0(round(pct_15, 1), "%")), 
              vjust = -1, size = 3, color = "#006400") +
    geom_text(aes(y = pct_20, label = paste0(round(pct_20, 1), "%")), 
              vjust = 2, size = 3, color = "#00008B") +
    scale_color_manual(values = c("<=15s" = "#006400", "<=20s" = "#00008B"),
                       labels = c("<=15s (≥90% standard)", "<=20s (≥95% standard)")) +
    labs(title = "9-1-1 Answer Time Compliance Over Last 4 Weeks",
         x = "Week",
         y = "Percentage Answered (%)",
         color = "Answer Time") +
    scale_y_continuous(limits = c(max(0, min(phone_trends_df$pct_15, phone_trends_df$pct_20) - 5), 100),
                       breaks = seq(0, 100, 10)) +
    theme_minimal() +
    theme(legend.position = "bottom",
          plot.title = element_text(hjust = 0.5, face = "bold", size = 14),
          axis.text.x = element_text(size = 9))
  
  # Display both plots
  print(p_volume)
  cat("\n\n")
  print(p_pct)
  
} else {
  cat("*Insufficient phone data for 4-week trend visualization.*\n\n")
}
```

```{r}
#| label: phone-4week-abandoned-counts
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "9-1-1 Abandoned Calls (Counts, Last 4 Weeks)"
#| fig-width: 10
#| fig-height: 6

if (!is.null(phone_trends_df) && nrow(phone_trends_df) > 0) {
  p_abandoned_count <- ggplot(phone_trends_df, aes(x = reorder(week_label, week_start))) +
    geom_col(aes(y = abandoned_911), fill = "#8B0000", width = 0.6, alpha = 0.85) +
    geom_text(aes(y = abandoned_911, label = scales::comma(abandoned_911)),
              vjust = -0.5, size = 3.5, fontface = "bold", color = "#8B0000") +
    labs(title = "9-1-1 Abandoned Calls – Counts",
         x = "Week",
         y = "Abandoned Call Count") +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold", size = 13),
      axis.text.x = element_text(size = 9)
    )
  print(p_abandoned_count)
} else {
  cat("*Insufficient phone data for abandoned call count visualization.*\n\n")
}
```

```{r}
#| label: phone-4week-abandoned-percent
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "9-1-1 Abandoned Calls (% of 9-1-1, Last 4 Weeks)"
#| fig-width: 10
#| fig-height: 6

if (!is.null(phone_trends_df) && nrow(phone_trends_df) > 0) {
  p_abandoned_pct <- ggplot(phone_trends_df, aes(x = reorder(week_label, week_start), y = abandoned_pct)) +
    geom_line(group = 1, color = "#00008B", linewidth = 1.2) +
    geom_point(color = "#00008B", size = 3) +
    geom_text(aes(label = paste0(round(abandoned_pct, 1), "%")),
              vjust = -0.8, size = 3.2, color = "#00008B") +
    scale_y_continuous(labels = function(x) paste0(x, "%")) +
    labs(title = "9-1-1 Abandoned Calls – Percentage",
         x = "Week",
         y = "Abandoned Percentage") +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold", size = 13),
      axis.text.x = element_text(size = 9)
    )
  print(p_abandoned_pct)
} else {
  cat("*Insufficient phone data for abandoned call percentage visualization.*\n\n")
}
```

### Call Distribution: Hour by Day of Week

The following visualization shows the distribution of service calls throughout the day (by hour) for each day of the week. This helps identify patterns in service call volume across different days and times.

```{r}
#| label: dow-distribution
#| echo: false
#| fig-cap: "Number of calls for service by day of the week."
#| cache: true
# ggplot2
dow_counts <- df |>
  count(DOW, sort = TRUE)

max_dow_info <- dow_counts |> filter(n == max(n))
busiest_day_abbr <- max_dow_info |> slice(1) |> pull(DOW)
busiest_day_count <- max_dow_info |> slice(1) |> pull(n)

min_dow_info <- dow_counts |> filter(n == min(n))
slowest_day_abbr <- min_dow_info |> slice(1) |> pull(DOW)
slowest_day_count <- min_dow_info |> slice(1) |> pull(n)

# Create mapping from abbreviations to full day names
day_names <- c(
  "SUN" = "Sunday",
  "MON" = "Monday",
  "TUE" = "Tuesday",
  "WED" = "Wednesday",
  "THU" = "Thursday",
  "FRI" = "Friday",
  "SAT" = "Saturday"
)

# Convert abbreviations to full day names
busiest_day <- day_names[busiest_day_abbr]
slowest_day <- day_names[slowest_day_abbr]

barDOW <- df |> ggplot(aes(x=DOW, fill=DOW)) +
  geom_bar() +
  paletteer::scale_fill_paletteer_d("ggsci::cyan_material") +
  labs(title="Number of Calls for Service by Day of the Week",
       x="Day of the Week",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size = 3.5
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barDOW
```

From this chart, we can see that `r busiest_day` was the busiest day of the week with `r busiest_day_count` service calls created, and the slowest day was `r slowest_day` with `r slowest_day_count` service calls. There is some consistency throughout the week, with `r busiest_day_count - slowest_day_count` calls difference between the busiest and slowest days.

```{r}
#| label: hour-of-the-day
#| echo: false
#| fig-cap: "Number of calls for service by hour of the day."
#| cache: true
# ggplot2
hour_counts <- df |>
  count(Hour, sort = TRUE)

# Find the actual maximum count and corresponding hour(s)
max_count <- max(hour_counts$n)
max_hour_info <- hour_counts |> filter(n == max_count)

# Convert factor levels back to numeric for proper comparison, then back to formatted string
busiest_hour_numeric <- max_hour_info |>
  slice(1) |>
  pull(Hour) |>
  as.character() |>
  as.numeric()
busiest_hour <- sprintf("%02d", busiest_hour_numeric)
busiest_hour_count <- max_hour_info |> slice(1) |> pull(n)

min_hour_info <- hour_counts |> filter(n == min(n))
# Convert factor levels back to numeric for proper comparison, then back to formatted string
slowest_hour_numeric <- min_hour_info |>
  slice(1) |>
  pull(Hour) |>
  as.character() |>
  as.numeric()
slowest_hour <- sprintf("%02d", slowest_hour_numeric)
slowest_hour_count <- min_hour_info |> slice(1) |> pull(n)

barHour <- df |> ggplot(aes(x=Hour, fill=Hour)) +
  geom_bar() +
  # Use precomputed palette to avoid repeated paletteer calls
  scale_fill_manual(values = palette_cyan_24) +
  labs(title="Number of Calls for Service by Hour of the Day",
       x="Hour of the Day",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size = 3.5
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barHour
```

This week, the busiest hour of the day was `r busiest_hour`00 hours, with `r busiest_hour_count` calls for service. `r slowest_hour`00 hours was the slowest hour of the day with `r slowest_hour_count` calls. Additionally, the pattern shows consistent traffic from late rush hour through the day into the early evening before seeing the volumes start to decline. This appears to confirm assumptions about the busiest parts of the day.

```{r}
#| label: hour-dow-analysis
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Call Volume by Hour and Day of Week"
#| fig-width: 10
#| fig-height: 6
#| cache: true

# Create summary data
hourly_dow_summary <- df |>
  group_by(DOW, Hour) |>
  summarise(call_count = n(), .groups = 'drop') |>
  mutate(
    Hour_numeric = as.numeric(as.character(Hour)),
    # Choose a contrasting text color based on relative intensity
    text_color = ifelse(call_count >= median(call_count), "black", "black")
  )

# Create a heatmap showing call patterns
hour_dow_plot <- ggplot(hourly_dow_summary, aes(x = Hour_numeric, y = DOW, fill = call_count)) +
  geom_tile(color = "white", linewidth = 0.1) +
  # Overlay counts per cell
  geom_text(aes(label = call_count, color = text_color), size = 3) +
  scale_color_identity() +
  scale_x_continuous(name = "Hour of Day",
                     breaks = seq(0, 23, 2),
                     labels = sprintf("%02d:00", seq(0, 23, 2))) +
  # Reverse the factor levels so the plot reads in the natural top-to-bottom order
  scale_y_discrete(name = "Day of Week", limits = rev(levels(df$DOW))) +
  scale_fill_gradient2(name = "Calls",
                       low = "lightblue",
                       mid = "yellow",
                       high = "red",
                       midpoint = median(hourly_dow_summary$call_count)) +
  labs(title = "Call Volume Heatmap by Hour and Day of Week",
       subtitle = "Darker colors indicate higher call volumes") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 9, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 9),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
    axis.text.y = element_text(size = 9),
    axis.title = element_text(size = 9),
    legend.position = "right",
    legend.title = element_text(size = 9),
    legend.text = element_text(size = 9),
    panel.grid = element_blank()
  )

hour_dow_plot
```

The heat map shows one intense spike on Tuesday at 1000 hours. There is an additional spike on Wednesday at 1000 hours. Overall, since `r busiest_hour`00 hours was the busiest hour of the week, the heatmap shows that it was reasonably consistent throughout the week. 

### Distribution of Service Calls by Shift

These analyses show the distribution of service calls as created through both day and night shifts and specific shifts in general. Each shift was split into three four-hour blocks for future analyses in order to determine how fatigue may be expressed during the course of the shift.

```{r}
#| label: shift-distribution
#| echo: false
#| fig-cap: "Number of calls for service by shift."
#| cache: true
shift_counts <- df |>
  count(Shift, sort = TRUE)

dn_counts <- df |>
  count(Day_Night, sort = TRUE)

sp_counts <- df |>
  count(ShiftPart, sort = TRUE) # Note: ShiftPart is not used in the plots below

barShift <- df |>
  ggplot(aes(x=Shift, fill=ShiftPart)) +
  geom_bar() +
  paletteer::scale_fill_paletteer_d("ggsci::cyan_material") +
  labs(title="Call Volume per Shift",
    x = "Shift",
    y = "Number of Calls") +
  # Add segment counts centered within each segment (white text)
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    position = position_stack(vjust = 0.5),
    size = 3.5,
    color = "black",
    fontface = "bold"
  ) +
  # Add total counts at the top of each bar
  stat_count(
    aes(label = after_stat(count), fill = NULL),
    geom = "text",
  vjust = -0.5,
    size = 3.5,
    fontface = "bold",
    color = "black"
  ) +
  theme_minimal() +
  theme(legend.position="right",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.title = element_text(size = 9),
        legend.text = element_text(size = 9))

barShift

barDN <- df |>
  ggplot(aes(x=Day_Night, fill=ShiftPart)) +
  geom_bar() +
  paletteer::scale_fill_paletteer_d("ggsci::cyan_material") +
  labs(title="Call Volume by Day/Night",
       x="Day/Night",
       y="Number of Calls") +
  # Add segment counts centered within each segment (white text)
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    position = position_stack(vjust = 0.5),
    size = 3.5,
    color = "black",
    fontface = "bold"
  ) +
  # Add total counts at the top of each bar
  stat_count(
    aes(label = after_stat(count), fill = NULL),
    geom = "text",
  vjust = -0.5,
    size = 3.5,
    fontface = "bold",
    color = "black"
  ) +
  theme_minimal() +
  theme(legend.position="right",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.title = element_text(size = 9),
        legend.text = element_text(size = 9))

barDN
```

As expected, day shifts are busier than night shifts by a considerable margin. As expected, the call volumes throughout the shift for day shift are fairly consistent and on night shift, the longer the shifts go, fewer calls are received. These are both expected from prior work.

```{r}
#| label: shift-narrative-insights
#| echo: false
#| results: asis

# Calculate shift metrics for narrative
shift_comparison <- df %>%
  group_by(Day_Night) %>%
  summarise(
    total_calls = n(),
    avg_ttq = mean(Time_To_Queue, na.rm = TRUE),
    avg_ttd = mean(Time_To_Dispatch, na.rm = TRUE),
    .groups = 'drop'
  )

day_calls <- shift_comparison %>% filter(Day_Night == "DAY") %>% pull(total_calls)
night_calls <- shift_comparison %>% filter(Day_Night == "NIGHT") %>% pull(total_calls)
call_ratio <- round(day_calls / night_calls, 2)

day_ttd <- shift_comparison %>% filter(Day_Night == "DAY") %>% pull(avg_ttd)
night_ttd <- shift_comparison %>% filter(Day_Night == "NIGHT") %>% pull(avg_ttd)

cat("\n**Shift Performance Narrative:**\n\n")
cat(paste0("Day shift handled ", format(day_calls, big.mark = ","), " calls compared to ", 
           format(night_calls, big.mark = ","), " on night shift—a ", call_ratio, 
           ":1 ratio. "))

if (abs(day_ttd - night_ttd) > 10) {
  faster_shift <- if(day_ttd < night_ttd) "day" else "night"
  cat(paste0("Notably, ", faster_shift, " shift demonstrated faster average dispatch times (",
             round(min(day_ttd, night_ttd), 1), "s vs ", round(max(day_ttd, night_ttd), 1), 
             "s), suggesting ", if(faster_shift == "night") "lower volume periods allow for more thorough call processing" else "day shift staff efficiently manage higher call volumes", ". "))
}

# Analyze shift part patterns
shift_part_stats <- df %>%
  group_by(ShiftPart) %>%
  summarise(
    calls = n(),
    median_processing = median(Processing_Time, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  arrange(desc(calls))

busiest_part <- shift_part_stats %>% slice(1) %>% pull(ShiftPart)
cat(paste0("The ", busiest_part, " portion of shifts consistently showed highest call volumes, ",
           "which should inform staffing and break scheduling decisions.\n\n"))
```

### CAD-centric Analyses

The following bar charts break down the volume of service calls using various other CAD-centric breakdowns that may be useful for various supplemental analyses. The first of those below shows the distribution pattern of service calls by priority number. This is overall for the center in all disciplines. Specific breakdowns for each discipline will be reflected in later sections.

```{r}
#| label: priority-distribution
#| echo: false
#| fig-cap: "Number of calls for service by priority level."
#| cache: true
# ggplot2
# Ensure Priority_Number is parsed to numeric and ordered
df_priority <- df %>%
  mutate(
    Priority_Number_num = suppressWarnings(as.numeric(gsub("[^0-9]", "", as.character(Priority_Number)))),
    Priority_Number_fac = factor(
      Priority_Number_num,
      levels = sort(unique(na.omit(Priority_Number_num))),
      ordered = TRUE
    )
  )

pn_counts <- df_priority |>
  filter(!is.na(Priority_Number_fac)) |>
  count(Priority_Number_fac, sort = FALSE)

max_pn_info <- pn_counts |> filter(n == max(n))
busiest_pn <- max_pn_info |> slice(1) |> pull(Priority_Number_fac)
busiest_pn_num <- suppressWarnings(as.numeric(as.character(busiest_pn)))
busiest_pn_count <- max_pn_info |> slice(1) |> pull(n)

# Calculate percentage statistics for inline use
busiest_pn_percentage <- round(busiest_pn_count / nrow(df_priority) * 100, 1)
priority1_count <- sum(df_priority$Priority_Number_num == 1, na.rm = TRUE)
priority1_percentage <- round(priority1_count / nrow(df_priority) * 100, 1)

barPriority <- df_priority |> ggplot(aes(x=Priority_Number_fac, fill=Priority_Number_fac)) +
  geom_bar() +
  paletteer::scale_fill_paletteer_d("ggsci::cyan_material") +
  labs(title="Number of Calls for Service by Priority Level",
       x="Priority Level",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size = 3.5
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barPriority
```

The majority of calls received were Priority `r busiest_pn` calls. Priority `r busiest_pn` calls are `r busiest_pn_percentage` percent of the total number of calls, while Priority 1 calls are `r priority1_percentage` percent of the total number of calls. This is a consistent pattern and more detailed analyses of the high priority, and specifically priority 1 calls, can be found below.

```{r}
#| label: priority-narrative-insights
#| echo: false
#| results: asis

# Analyze priority distribution patterns
priority_dist <- df %>%
  count(Priority_Number) %>%
  mutate(pct = round(n / sum(n) * 100, 1)) %>%
  arrange(desc(n))

# Calculate priority performance
priority_perf <- df %>%
  filter(Priority_Number %in% c("1", "2", "3", "4")) %>%
  group_by(Priority_Number) %>%
  summarise(
    median_ttd = median(Time_To_Dispatch, na.rm = TRUE),
    p90_ttd = quantile(Time_To_Dispatch, 0.9, na.rm = TRUE),
    .groups = 'drop'
  )

cat("\n**Priority Distribution Insights:**\n\n")

# Check if high-priority calls are appropriately fast
p1_ttd <- priority_perf %>% filter(Priority_Number == "1") %>% pull(median_ttd)
p2_ttd <- priority_perf %>% filter(Priority_Number == "2") %>% pull(median_ttd)

if (!is.na(p1_ttd) && !is.na(p2_ttd)) {
  if (p1_ttd < p2_ttd) {
    cat(paste0("The system appropriately prioritizes urgent calls, with Priority 1 calls ",
               "dispatched ", round(p2_ttd - p1_ttd, 1), " seconds faster than Priority 2 on average. "))
  } else {
    cat(paste0("**Attention**: Priority 1 calls show longer dispatch times (", round(p1_ttd, 1),
               "s) than Priority 2 (", round(p2_ttd, 1), "s), suggesting potential process issues. "))
  }
}

# Assess overall priority balance
routine_calls <- priority_dist %>% filter(Priority_Number %in% c("3", "4")) %>% pull(n) %>% sum()
total_calls <- sum(priority_dist$n)
routine_pct <- round(routine_calls / total_calls * 100, 1)

if (routine_pct > 60) {
  cat(paste0("With ", routine_pct, "% of calls classified as routine (Priority 3-4), ",
             "there may be opportunities to optimize resource allocation for non-urgent responses.\n\n"))
} else if (routine_pct < 40) {
  cat(paste0("The relatively low proportion of routine calls (", routine_pct, 
             "%) indicates sustained demand for urgent response, requiring consistent staffing levels.\n\n"))
}
```

```{r}
#| label: agency-distribution
#| echo: false
#| fig-cap: "Number of calls for service by discipline."
#| cache: true
# ggplot2
agency_counts <- df |>
  count(Agency, sort = TRUE)

max_agency_info <- agency_counts |> filter(n == max(n))
busiest_agency <- max_agency_info |> slice(1) |> pull(Agency)
busiest_agency_count <- max_agency_info |> slice(1) |> pull(n)

# Calculate percentage for Police calls
police_percentage <- round((sum(df$Agency == "POLICE", na.rm = TRUE) / nrow(df)) * 100, 1)

barDiscipline <- df |> ggplot(aes(x=Agency, fill=Agency)) +
  geom_bar() +
  scale_fill_manual(
    values = c(POLICE = "#1f77b4", FIRE = "#d62728", EMS = "#2ca02c"),
    name = "Agency"
  ) +
  labs(title="Number of Calls for Service by Discipline",
       x="Discipline",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    position = position_stack(vjust = 0.5),
    size = 3.5,
    color = "white",
    fontface = "bold"
  ) +
  theme_minimal() +
  theme(legend.position="right",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.title = element_text(size = 9),
        legend.text = element_text(size = 9))

barDiscipline
```

As expected, the majority of calls are for `r busiest_agency`. They represent `r police_percentage`
percent of the total number of calls. This is fairly consistent with previous analyses. We can also examine the way in which we are receiving the calls by looking at the Call_Reception column. That chart is below.

```{r}
#| label: call-reception-distribution
#| echo: false
#| fig-cap: "Number of calls for service by call reception."
#| cache: true

# ggplot2
cr_counts <- df |>
  count(Call_Reception, sort = TRUE)

max_cr_info <- cr_counts |> filter(n == max(n))
busiest_cr <- max_cr_info |> slice(1) |> pull(Call_Reception)
busiest_cr_count <- max_cr_info |> slice(1) |> pull(n)

# Calculate statistics for inline use
e911_count <- sum(df$Call_Reception == "E-911", na.rm = TRUE)
not_captured_count <- sum(df$Call_Reception == "NOT CAPTURED", na.rm = TRUE)
e911_percentage <- round((e911_count / nrow(df)) * 100, 2)
not_captured_percentage <- round((not_captured_count / nrow(df)) * 100, 1)

barReception <- df |> ggplot(aes(x=Call_Reception, fill=Call_Reception)) +
  geom_bar() +
  paletteer::scale_fill_paletteer_d("ggsci::cyan_material") +
  labs(title="Number of Calls for Service by Call Reception",
       x="Call Reception",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size = 3.5
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barReception
```

Most of the calls arrived by `r busiest_cr`. 911 trunk line calls were `r e911_percentage` percent of all calls. There were `r not_captured_count` calls where we did not indicate how the service call was received. This is `r not_captured_percentage` percent of the total number of calls. These calls should be investigated to determine why call origination is not being tracked.

The following is a chart of the disposition codes that have been used this week.

```{r}
#| label: disposition-codes
#| echo: false
#| fig-cap: "Number of calls for service by disposition code."
#| cache: true
# ggplot2

dispo_counts <- df |>
  count(Disposition, sort = TRUE)

max_dispo_info <- dispo_counts |> filter(n == max(n))
mostused_dispo <- max_dispo_info |> slice(1) |> pull(Disposition)
mostused_dispo_count <- max_dispo_info |> slice(1) |> pull(n)

# Calculate statistics for inline use
not_captured_count <- sum(df$Disposition == "UNDEFINED", na.rm = TRUE)
not_captured_percentage <- round((not_captured_count / nrow(df)) * 100, 1)

## Use the precomputed cyan base palette and expand it to the number of disposition levels
df_dispo_plot <- df |> droplevels()
n_dispo <- max(1, n_distinct(df_dispo_plot$Disposition))
dispo_pal <- grDevices::colorRampPalette(palette_cyan_base)(n_dispo)

barDisposition <- df_dispo_plot |> ggplot(aes(x=Disposition, fill=Disposition)) +
  geom_bar() +
  scale_fill_manual(values = dispo_pal, na.value = "grey80") +
  labs(title="Number of Calls for Service by Call Disposition",
       x="Call Disposition Code",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size = 3.5
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barDisposition
```

As can be seen from the bar chart, `r mostused_dispo` was the most common disposition code used with `r mostused_dispo_count` calls coded that way. It is interesting to note that no calls, this week, have been closed without a disposition.

```{r}
#| label: disposition-narrative-insights
#| echo: false
#| results: asis

# Analyze disposition patterns
dispo_analysis <- df %>%
  filter(!is.na(Disposition), Disposition != "UNDEFINED") %>%
  count(Disposition, sort = TRUE) %>%
  mutate(pct = round(n / sum(n) * 100, 1))

# Identify concerning patterns
low_completion_dispos <- c("CANCELLED", "DUPLICATE", "UNABLE TO LOCATE")
concern_calls <- df %>%
  filter(Disposition %in% low_completion_dispos) %>%
  nrow()

concern_pct <- round(concern_calls / nrow(df) * 100, 1)

cat("\n**Disposition Pattern Analysis:**\n\n")

if (concern_pct > 10) {
  cat(paste0("**Notable**: ", concern_pct, "% of calls were cancelled, duplicated, or unable to locate—",
             "a relatively high proportion that may indicate opportunities for improved call screening or coordination. "))
} else if (concern_pct > 5) {
  cat(paste0("Approximately ", concern_pct, "% of calls resulted in non-service dispositions, ",
             "which is within normal operational range. "))
}

# Check disposition diversity
dispo_diversity <- nrow(dispo_analysis)
if (dispo_diversity > 15) {
  cat(paste0("The use of ", dispo_diversity, " distinct disposition codes suggests comprehensive ",
             "call outcome tracking, though consolidation might improve reporting clarity.\n\n"))
}
```

The following is a chart of the top 10 call types. The data is limited to ensure visual clarity and legibility of the information.

```{r}
#| label: problem-type-top10
#| echo: false
#| fig-cap: "Number of calls for service by call type."
#| cache: true
# ggplot2
ct_counts <- df |>
  count(Problem, sort = TRUE)

max_ct_info <- ct_counts |> filter(n == max(n))
busiest_ct <- max_ct_info |> slice(1) |> pull(Problem)
busiest_ct_count <- max_ct_info |> slice(1) |> pull(n)

# Find the most common Problem for each Agency
agency_top_problems <- df |>
  count(Agency, Problem, sort = TRUE) |>
  group_by(Agency) |>
  slice_max(n, n = 1, with_ties = FALSE) |>
  ungroup() |>
  dplyr::select(Agency, Problem, n)

# Create named vectors for easy access in inline code
agency_top_problem_names <- setNames(agency_top_problems$Problem, agency_top_problems$Agency)
agency_top_problem_counts <- setNames(agency_top_problems$n, agency_top_problems$Agency)

problem_counts <- df |>
  count(Problem, sort = TRUE) |>
  slice_head(n = 10)

barProblem <- problem_counts |>
  ggplot(aes(x=reorder(Problem, -n), y=n, fill=Problem)) +
  geom_bar(stat="identity") +
  paletteer::scale_fill_paletteer_d("ggsci::cyan_material") +
  labs(title="Number of Calls for Service by Call Type",
       x="Call Type",
       y="Number of Calls") +
  geom_text(
    aes(label = n),
    vjust = -0.5,
    size = 3.5) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barProblem
```

This week, the most common problem nature was `r busiest_ct`. For AFD, the most common was `r agency_top_problem_names["FIRE"]`. The most common medical call for service was `r agency_top_problem_names["EMS"]`.

We can also look at the number of calls taken by telecommunicators. Again, like the problem types, we will limit the chart to the top 10 telecommunicators to ensure visual clarity and legibility of the information.

```{r}
#| label: call-taker-top10
#| echo: false
#| fig-cap: "Number of calls for service by telecommunicator."
#| cache: true
# ggplot2
tc_counts <- df |>
  count(Call_Taker, sort = TRUE)

max_tc_info <- tc_counts |> filter(n == max(n))
busiest_tc <- max_tc_info |> slice(1) |> pull(Call_Taker)
busiest_tc_count <- max_tc_info |> slice(1) |> pull(n)

tc_counts <- df |>
  count(Call_Taker, sort = TRUE) |>
  slice_head(n = 10)

barCallTaker <- tc_counts |>
  ggplot(aes(x=reorder(Call_Taker, -n), y=n, fill=Call_Taker)) +
  geom_bar(stat="identity") +
  paletteer::scale_fill_paletteer_d("ggsci::cyan_material") +
  labs(title="Number of Calls for Service by Call Taker",
       x="Call Taker",
       y="Number of Calls") +
  geom_text(
    aes(label = n),
    vjust = -0.5,
    size = 3.5) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barCallTaker
```

It is interesting to note that the top "call taker" is `r busiest_tc` again this week with `r busiest_tc_count` calls.

```{r}
#| label: time-on-phone
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Total Time on Phone for Top 10 calltakers"
#| cache: true

# Two tables: top 10 Call_Taker by cumulative Time_To_Queue and Phone_Time

# Ensure numeric seconds for aggregation
df_times <- df |>
  mutate(
    Time_To_Queue = suppressWarnings(as.numeric(Time_To_Queue)),
    Phone_Time    = suppressWarnings(as.numeric(Phone_Time))
  )

# Summarize cumulative seconds by Call_Taker
ttq_by_ct <- df_times |>
  filter(!is.na(Call_Taker)) |>
  group_by(Call_Taker) |>
  dplyr::summarise(
    N_TTQ = sum(!is.na(Time_To_Queue)),
    Total_Time_To_Queue_Sec = sum(Time_To_Queue, na.rm = TRUE),
    .groups = "drop"
  ) |>
  arrange(desc(Total_Time_To_Queue_Sec)) |>
  slice_head(n = 10) |>
  mutate(
    Total_Time_To_Queue_Min = round(Total_Time_To_Queue_Sec / 60, 1),
    Mean_TTQ_Sec = ifelse(N_TTQ > 0, round(Total_Time_To_Queue_Sec / N_TTQ), NA_real_)
  )

  most_time_ct <- ttq_by_ct$Call_Taker[1]
  most_time_ct_total <- ttq_by_ct$Total_Time_To_Queue_Sec[1]
  most_time_ct_mean <- ttq_by_ct$Mean_TTQ_Sec[1]

phone_by_ct <- df_times |>
  filter(!is.na(Call_Taker)) |>
  group_by(Call_Taker) |>
  dplyr::summarise(
    N_Phone = sum(!is.na(Phone_Time)),
    Total_Phone_Time_Sec = sum(Phone_Time, na.rm = TRUE),
    .groups = "drop"
  ) |>
  arrange(desc(Total_Phone_Time_Sec)) |>
  slice_head(n = 10) |>
  mutate(
    Total_Phone_Time_Min = round(Total_Phone_Time_Sec / 60, 1),
    Mean_Phone_Sec = ifelse(N_Phone > 0, round(Total_Phone_Time_Sec / N_Phone), NA_real_)
  )

  longest_phone_ct <- phone_by_ct$Call_Taker[1]
  longest_phone_ct_total <- phone_by_ct$Total_Phone_Time_Sec[1]
  longest_phone_ct_mean <- phone_by_ct$Mean_Phone_Sec[1]

# Render tables with accessible Word-friendly formatting
#to_ft(
#  ttq_by_ct |> dplyr::select(Call_Taker, N_TTQ, Total_Time_To_Queue_Sec, Total_Time_To_Queue_Min, Mean_TTQ_Sec),
#  caption = "Top 10 Call Takers by Cumulative Time To Queue",
#  header_map = list(
#    Call_Taker = "Call Taker",
#    N_TTQ = "Calls",
#    Total_Time_To_Queue_Sec = "Total TTQ (sec)",
#    Total_Time_To_Queue_Min = "Total TTQ (min)",
#    Mean_TTQ_Sec = "Mean TTQ (sec)"
#  ),
#  digits = 0
#)

to_ft(
  phone_by_ct |> dplyr::select(Call_Taker, N_Phone, Total_Phone_Time_Sec, Total_Phone_Time_Min, Mean_Phone_Sec),
  caption = "Top 10 Call Takers by Cumulative Phone Time",
  header_map = list(
    Call_Taker = "Call Taker",
    N_Phone = "Calls",
    Total_Phone_Time_Sec = "Total Phone (sec)",
    Total_Phone_Time_Min = "Total Phone (min)",
    Mean_Phone_Sec = "Mean Phone (sec)"
  ),
  digits = 0
)
```

This table shows the call takers with the highest cumulative time on the phone. This week, `r longest_phone_ct` had the largest average time on the phone with an average of `r longest_phone_ct_mean` seconds per call. This could be due to a number of factors, including the complexity of the calls faced.

### Multi-Week Performance Trends

```{r}
#| label: load-historical-weeks
#| echo: false
#| message: false
#| warning: false
#| cache: true

# Load last 4 weeks for trend analysis
weeks_to_analyze <- 4
historical_data <- list()

for (i in 0:(weeks_to_analyze - 1)) {
  week_to_load <- current_week_num - i
  year_to_load <- current_year
  
  # Handle year boundary
  if (week_to_load < 1) {
    week_to_load <- week_to_load + 52
    year_to_load <- year_to_load - 1
  }
  
  data_file <- file.path("data", "current_year", paste0("week", week_to_load, ".csv"))
  
  if (file.exists(data_file)) {
    week_df <- vroom::vroom(
      data_file,
      col_types = vroom::cols(.default = "c"),
      na = c("", "NA", "NULL"),
      progress = FALSE,
      show_col_types = FALSE
    ) %>%
      dplyr::mutate(
        week_num = week_to_load,
        week_year = year_to_load,
        Time_To_Queue = suppressWarnings(as.numeric(Time_To_Queue)),
        Time_To_Dispatch = suppressWarnings(as.numeric(Time_To_Dispatch)),
        Phone_Time = suppressWarnings(as.numeric(Phone_Time)),
        Processing_Time = suppressWarnings(as.numeric(Processing_Time))
      )
    historical_data[[length(historical_data) + 1]] <- week_df
  }
}

# Combine all historical weeks
df_historical <- bind_rows(historical_data)

# Calculate weekly aggregates
weekly_metrics <- df_historical %>%
  group_by(week_num) %>%
  summarise(
    total_calls = n(),
    median_ttq = median(Time_To_Queue, na.rm = TRUE),
    median_ttd = median(Time_To_Dispatch, na.rm = TRUE),
    median_phone = median(Phone_Time, na.rm = TRUE),
    mean_ttq = mean(Time_To_Queue, na.rm = TRUE),
    mean_ttd = mean(Time_To_Dispatch, na.rm = TRUE),
    mean_phone = mean(Phone_Time, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  arrange(week_num)
```

```{r}
#| label: trend-charts
#| echo: false
#| fig-cap: "4-Week Performance Trends"
#| fig-width: 12
#| fig-height: 8

# Create trend visualizations
p_volume <- ggplot(weekly_metrics, aes(x = week_num, y = total_calls)) +
  geom_line(linewidth = 1, color = "#2C3E50") +
  geom_point(size = 2.5, color = "#2C3E50") +
  geom_text(aes(label = scales::comma(total_calls)), vjust = -1, size = 3) +
  labs(title = "Call Volume Trend", x = NULL, y = "Total Calls") +
  scale_y_continuous(labels = scales::comma) +
  theme_minimal() +
  theme(plot.title = element_text(size = 11, face = "bold"),
        axis.text = element_text(size = 9))

p_ttq <- ggplot(weekly_metrics, aes(x = week_num, y = median_ttq)) +
  geom_line(linewidth = 1, color = "#3498DB") +
  geom_point(size = 2.5, color = "#3498DB") +
  geom_text(aes(label = round(median_ttq, 1)), vjust = -1, size = 3) +
  labs(title = "Median Time to Queue", x = NULL, y = "Seconds") +
  theme_minimal() +
  theme(plot.title = element_text(size = 11, face = "bold"),
        axis.text = element_text(size = 9))

p_ttd <- ggplot(weekly_metrics, aes(x = week_num, y = median_ttd)) +
  geom_line(linewidth = 1, color = "#E74C3C") +
  geom_point(size = 2.5, color = "#E74C3C") +
  geom_text(aes(label = round(median_ttd, 1)), vjust = -1, size = 3) +
  labs(title = "Median Time to Dispatch", x = "Week Number", y = "Seconds") +
  theme_minimal() +
  theme(plot.title = element_text(size = 11, face = "bold"),
        axis.text = element_text(size = 9))

p_phone <- ggplot(weekly_metrics, aes(x = week_num, y = median_phone)) +
  geom_line(linewidth = 1, color = "#16A085") +
  geom_point(size = 2.5, color = "#16A085") +
  geom_text(aes(label = round(median_phone, 1)), vjust = -1, size = 3) +
  labs(title = "Median Phone Time", x = "Week Number", y = "Seconds") +
  theme_minimal() +
  theme(plot.title = element_text(size = 11, face = "bold"),
        axis.text = element_text(size = 9))

# Combine plots using patchwork
(p_volume + p_ttq) / (p_ttd + p_phone)
```

The trend charts above show performance metrics over the last 4 weeks, allowing identification of patterns and emerging issues. Week `r current_week_num` shows `r ifelse(call_volume_change > 0, "an increase", "a decrease")` of `r abs(call_volume_change)` calls compared to week `r last_week_num`.

#### Dual-Axis: Volume & Efficiency Trend

```{r}
#| label: dual-axis-trends
#| echo: false
#| fig-cap: "Call Volume vs Median Response Time - 4 Week Trend"
#| fig-width: 12
#| fig-height: 6

# Scale factor for secondary axis
scale_factor <- max(weekly_metrics$total_calls, na.rm = TRUE) / max(weekly_metrics$median_ttd, na.rm = TRUE)

ggplot(weekly_metrics, aes(x = week_num)) +
  # Volume bars
  geom_col(aes(y = total_calls), fill = "#3498DB", alpha = 0.6, width = 0.8) +
  # TTD line
  geom_line(aes(y = median_ttd * scale_factor), color = "#E74C3C", linewidth = 1.5) +
  geom_point(aes(y = median_ttd * scale_factor), color = "#E74C3C", size = 3) +
  scale_y_continuous(
    name = "Total Calls",
    labels = scales::comma,
    sec.axis = sec_axis(~ . / scale_factor, name = "Median TTD (seconds)")
  ) +
  labs(
    title = "Call Volume vs Response Time Efficiency",
    subtitle = "Blue bars: Call volume | Red line: Median Time to Dispatch",
    x = "Week Number"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 11, hjust = 0.5),
    axis.title.y.left = element_text(color = "#3498DB", face = "bold", size = 11),
    axis.title.y.right = element_text(color = "#E74C3C", face = "bold", size = 11),
    axis.text = element_text(size = 10),
    axis.title.x = element_text(size = 11, face = "bold")
  )
```

This dual-axis chart allows quick visual assessment of whether call volume changes correlate with dispatch efficiency. Inverse relationships (volume ↑, TTD ↑) may indicate capacity constraints.

### Priority-Level Efficiency Analysis

```{r}
#| label: priority-efficiency
#| echo: false
#| message: false
#| warning: false
#| cache: false

# Analyze handling times by priority level
priority_efficiency <- df %>%
  filter(!is.na(Priority_Number), !is.na(Time_To_Dispatch)) %>%
  mutate(Priority_Number = as.character(Priority_Number)) %>%  # Convert to character to ensure proper grouping
  group_by(Priority_Number) %>%
  summarise(
    Call_Count = n(),
    Median_TTQ = round(median(Time_To_Queue, na.rm = TRUE), 1),
    Mean_TTQ = round(mean(Time_To_Queue, na.rm = TRUE), 1),
    Median_TTD = round(median(Time_To_Dispatch, na.rm = TRUE), 1),
    Mean_TTD = round(mean(Time_To_Dispatch, na.rm = TRUE), 1),
    Median_Phone = round(median(Phone_Time, na.rm = TRUE), 1),
    Mean_Phone = round(mean(Phone_Time, na.rm = TRUE), 1),
    Median_Processing = round(median(Processing_Time, na.rm = TRUE), 1),
    .groups = 'drop'
  ) %>%
  ungroup() %>%
  mutate(
    Priority_Numeric = as.numeric(Priority_Number)  # Keep numeric for sorting
  ) %>%
  arrange(Priority_Numeric) %>%  # Sort by numeric priority
  mutate(
    SLA_Target = case_when(
      Priority_Number == "1" ~ "60s",
      Priority_Number == "2" ~ "120s",
      Priority_Number == "3" ~ "600s",
      Priority_Number == "4" ~ "3600s",
      TRUE ~ "—"
    )
  ) %>%
  dplyr::select(-Priority_Numeric)  # Remove sorting helper

# Save a copy for visualization before converting to flextable
priority_efficiency_data <- priority_efficiency
```

```{r}
#| label: priority-efficiency-display
#| echo: false

# Pad Priority_Number with leading zeros for proper alphabetical sorting
priority_efficiency_display <- priority_efficiency_data %>%
  mutate(Priority_Number = sprintf("%02d", as.numeric(Priority_Number)))

# Display the table
to_ft(
  priority_efficiency_display,
  caption = "Call Handling Efficiency by Priority Level",
  header_map = list(
    Priority_Number = "Priority",
    Call_Count = "Calls",
    Median_TTQ = "Med TTQ (s)",
    Median_TTD = "Med TTD (s)",
    Median_Phone = "Med Phone (s)",
    Median_Processing = "Med Process (s)",
    SLA_Target = "SLA Target"
  ),
  digits = 1
)
```

```{r}
#| label: priority-efficiency-viz
#| echo: false
#| fig-cap: "Response Time Comparison by Priority Level"
#| fig-width: 10
#| fig-height: 6

priority_efficiency_long <- priority_efficiency_data %>%
  dplyr::select(Priority_Number, Median_TTQ, Median_TTD, Median_Phone) %>%
  mutate(Priority_Numeric = as.numeric(Priority_Number)) %>%  # Add numeric for sorting
  pivot_longer(cols = c(Median_TTQ, Median_TTD, Median_Phone),
               names_to = "Metric",
               values_to = "Seconds") %>%
  mutate(Metric = factor(case_when(
    Metric == "Median_TTQ" ~ "Time to Queue",
    Metric == "Median_TTD" ~ "Time to Dispatch",
    Metric == "Median_Phone" ~ "Phone Time",
    TRUE ~ Metric
  ), levels = c("Time to Queue", "Time to Dispatch", "Phone Time")))

ggplot(priority_efficiency_long, aes(x = reorder(Priority_Number, Priority_Numeric), y = Seconds, fill = Metric)) +
  geom_col(position = position_dodge(width = 0.7), width = 0.7) +
  geom_text(aes(label = round(Seconds, 0)), 
            position = position_dodge(width = 0.7),
            vjust = -0.5, size = 3) +
  scale_fill_manual(
    name = "Metric",
    values = c("Time to Queue" = "#3498DB", 
               "Time to Dispatch" = "#E74C3C", 
               "Phone Time" = "#16A085"),
    labels = c("Time to Queue", "Time to Dispatch", "Phone Time")
  ) +
  labs(
    title = "Response Time Metrics by Priority Level",
    subtitle = "Comparison of median handling times across priority levels",
    x = "Priority Level",
    y = "Median Time (seconds)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 13, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 10, hjust = 0.5, color = "gray40"),
    legend.position = "right",
    legend.title = element_text(size = 10, face = "bold"),
    legend.text = element_text(size = 9),
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 11, face = "bold"),
    panel.grid.major.x = element_blank()
  )
```

```{r}
#| label: priority-efficiency-narrative
#| echo: false
#| results: asis

p1_median_ttd <- priority_efficiency_data %>% 
  filter(Priority_Number == "1") %>% 
  pull(Median_TTD) %>% 
  first()

p1_status <- ifelse(p1_median_ttd <= 60, "within", "outside")

cat(sprintf("Priority 1 calls have a median Time to Dispatch of %s seconds, which is %s the SLA target of 60 seconds.", p1_median_ttd, p1_status))
```

### Call Type Complexity Analysis

```{r}
#| label: call-complexity-scoring
#| echo: false
#| message: false
#| warning: false

# Calculate complexity scores based on handling duration
call_complexity <- df %>%
  filter(!is.na(Problem), !is.na(Processing_Time)) %>%
  group_by(Problem, Agency) %>%
  summarise(
    Call_Count = n(),
    Median_Processing = median(Processing_Time, na.rm = TRUE),
    Mean_Processing = mean(Processing_Time, na.rm = TRUE),
    Median_Phone = median(Phone_Time, na.rm = TRUE),
    SD_Processing = sd(Processing_Time, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  filter(Call_Count >= 5) %>%  # Minimum threshold for reliability
  mutate(
    # Complexity score: normalized combination of duration and variability
    Duration_Score = scales::rescale(Mean_Processing, to = c(0, 100)),
    Variability_Score = scales::rescale(SD_Processing, to = c(0, 100)),
    Complexity_Index = round((Duration_Score * 0.7 + Variability_Score * 0.3), 1),
    Complexity_Category = case_when(
      Complexity_Index >= 75 ~ "Very High",
      Complexity_Index >= 50 ~ "High",
      Complexity_Index >= 25 ~ "Moderate",
      TRUE ~ "Low"
    )
  ) %>%
  arrange(desc(Complexity_Index))

# Top 10 most complex call types
top_complex <- call_complexity %>%
  slice_head(n = 10)

to_ft(
  top_complex %>% 
    dplyr::select(Problem, Agency, Call_Count, Median_Processing, Mean_Processing, Complexity_Index, Complexity_Category),
  caption = "Top 10 Most Complex Call Types (by handling duration)",
  header_map = list(
    Problem = "Call Type",
    Agency = "Agency",
    Call_Count = "Calls",
    Median_Processing = "Med Process (s)",
    Mean_Processing = "Mean Process (s)",
    Complexity_Index = "Complexity",
    Complexity_Category = "Category"
  ),
  digits = 1
)
```

```{r}
#| label: complexity-visualization
#| echo: false
#| fig-cap: "Call Type Complexity Distribution"
#| fig-width: 10
#| fig-height: 6

ggplot(top_complex, aes(x = reorder(Problem, Complexity_Index), y = Complexity_Index, fill = Agency)) +
  geom_col(width = 0.7) +
  geom_text(aes(label = round(Complexity_Index)), hjust = -0.2, size = 3) +
  coord_flip() +
  scale_fill_manual(
    values = c(POLICE = "#1f77b4", FIRE = "#d62728", EMS = "#2ca02c"),
    name = "Agency"
  ) +
  labs(
    title = "Most Complex Call Types by Handling Duration",
    subtitle = "Complexity Index: 0-100 scale based on processing time and variability",
    x = "Call Type",
    y = "Complexity Index"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 13, face = "bold"),
    plot.subtitle = element_text(size = 10),
    axis.text = element_text(size = 9),
    axis.title = element_text(size = 11, face = "bold"),
    legend.position = "right"
  )
```

```{r}
#| label: complexity-summary
#| echo: false
#| results: asis

most_complex_type <- top_complex$Problem[1]
complexity_score <- round(top_complex$Complexity_Index[1], 1)
mean_processing <- format(round(top_complex$Mean_Processing[1]), big.mark = ",", scientific = FALSE)

cat(sprintf("The most complex call type this week was **%s** with a complexity index of %s, requiring an average of %s seconds to process.", 
            most_complex_type, complexity_score, mean_processing))
```

## Advanced Analytics & Strategic Insights

This section provides advanced analytics, staffing insights, and automated recommendations to complement the operational metrics above.

### All Dispatcher Performance Analysis

This section analyzes performance across all dispatchers to identify workload distribution, efficiency patterns, and development opportunities.

```{r}
#| label: all-dispatcher-performance
#| echo: false
#| message: false
#| warning: false

# Comprehensive dispatcher analysis
dispatcher_performance <- df %>%
  # Exclude non-person dispatcher values from peer comparisons
  filter(!is.na(Dispatcher)) %>%
  filter(!(Dispatcher %in% c("Automatic Dispatch", "CAD to CAD Service 1", "NULL"))) %>%
  group_by(Dispatcher) %>%
  summarise(
    Total_Calls = n(),
    Median_TTD = round(median(Time_To_Dispatch, na.rm = TRUE), 1),
    Mean_TTD = round(mean(Time_To_Dispatch, na.rm = TRUE), 1),
    SD_TTD = round(sd(Time_To_Dispatch, na.rm = TRUE), 1),
    P1_Calls = sum(Priority_Number == "1", na.rm = TRUE),
    P2_Calls = sum(Priority_Number == "2", na.rm = TRUE),
    High_Priority_Pct = round((sum(Priority_Number %in% c("1", "2"), na.rm = TRUE) / n()) * 100, 1),
    .groups = 'drop'
  ) %>%
  arrange(desc(Total_Calls)) %>%
  mutate(
    Performance_Quartile = ntile(Median_TTD, 4),
    Workload_Quartile = ntile(Total_Calls, 4)
  )

# Create performance table
to_ft(
  dispatcher_performance %>% 
    dplyr::select(Dispatcher, Total_Calls, Median_TTD, Mean_TTD, P1_Calls, P2_Calls, High_Priority_Pct),
  caption = "Dispatcher Performance Summary - Current Week",
  header_map = list(
    Dispatcher = "Dispatcher",
    Total_Calls = "Total Calls",
    Median_TTD = "Median TTD (s)",
    Mean_TTD = "Mean TTD (s)",
    P1_Calls = "P1 Calls",
    P2_Calls = "P2 Calls",
    High_Priority_Pct = "High Priority %"
  ),
  digits = 1
)
```

#### Automated/CAD-to-CAD/Null Dispatches (Separate View)

These entries are operational mechanisms (e.g., system auto-routes or inter-CAD transfers) and are shown separately to avoid skewing peer comparisons of individual dispatchers.

```{r}
#| label: automated-cad-null-summary
#| echo: false
#| message: false
#| warning: false

auto_cad_null <- df %>%
  mutate(Dispatcher_Categorized = dplyr::case_when(
    is.na(Dispatcher) ~ "[NA]",
    Dispatcher %in% c("Automatic Dispatch", "CAD to CAD Service 1", "NULL") ~ Dispatcher,
    TRUE ~ NA_character_
  )) %>%
  filter(!is.na(Dispatcher_Categorized)) %>%
  group_by(Dispatcher_Categorized) %>%
  summarise(
    Total_Calls = n(),
    Median_TTQ = round(median(Time_To_Queue, na.rm = TRUE), 1),
    Median_TTD = round(median(Time_To_Dispatch, na.rm = TRUE), 1),
    Median_Phone = round(median(Phone_Time, na.rm = TRUE), 1),
    .groups = 'drop'
  ) %>%
  arrange(desc(Total_Calls))

to_ft(
  auto_cad_null,
  caption = "Automated/CAD-to-CAD/Null Dispatch Summary (Excluded from Peer Comparison)",
  header_map = list(
    Dispatcher_Categorized = "Category",
    Total_Calls = "Total Calls",
    Median_TTQ = "Med TTQ (s)",
    Median_TTD = "Med TTD (s)",
    Median_Phone = "Med Phone (s)"
  ),
  digits = 1
)
```

#### Workload Distribution

```{r}
#| label: workload-distribution
#| fig-cap: "Dispatcher Workload Distribution"
#| fig-width: 10
#| fig-height: 6

# Top 15 dispatchers by call volume
top_dispatchers <- dispatcher_performance %>%
  slice_head(n = 15)

ggplot(top_dispatchers, aes(x = reorder(Dispatcher, Total_Calls), y = Total_Calls)) +
  geom_col(aes(fill = High_Priority_Pct), width = 0.7) +
  geom_text(aes(label = Total_Calls), hjust = -0.2, size = 3) +
  scale_fill_gradient2(
    name = "High Priority %",
    low = "#2ECC71",
    mid = "#F39C12",
    high = "#E74C3C",
    midpoint = 30
  ) +
  coord_flip() +
  labs(
    title = "Top 15 Dispatchers by Call Volume",
    subtitle = "Color indicates percentage of high-priority (P1/P2) calls",
    x = "Dispatcher",
    y = "Total Calls"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 13, face = "bold"),
    plot.subtitle = element_text(size = 10),
    axis.text = element_text(size = 9),
    legend.position = "right"
  )
```

### Staff Development Insights

```{r}
#| label: performance-quartiles
#| echo: false
#| results: asis

# Identify top and bottom performers
top_performers <- dispatcher_performance %>%
  # Redundant safety filter to ensure system/non-human codes never appear
  filter(!(Dispatcher %in% c("Automatic Dispatch", "CAD to CAD Service 1", "NULL"))) %>%
  filter(Total_Calls >= 10) %>%  # Minimum call threshold
  arrange(Median_TTD) %>%
  slice_head(n = 5)

needs_support <- dispatcher_performance %>%
  filter(!(Dispatcher %in% c("Automatic Dispatch", "CAD to CAD Service 1", "NULL"))) %>%
  filter(Total_Calls >= 10) %>%
  arrange(desc(Median_TTD)) %>%
  slice_head(n = 5)
```

#### Top Performers (Fastest Median TTD)

```{r}
#| label: top-performers-table

to_ft(
  top_performers %>% dplyr::select(Dispatcher, Total_Calls, Median_TTD, High_Priority_Pct),
  caption = "Top 5 Performers - Fastest Dispatch Times (min 10 calls)",
  header_map = list(
    Dispatcher = "Dispatcher",
    Total_Calls = "Calls",
    Median_TTD = "Median TTD (s)",
    High_Priority_Pct = "High Priority %"
  ),
  digits = 1
)
```

#### Development Opportunities (Slowest Median TTD)

```{r}
#| label: needs-support-table

to_ft(
  needs_support %>% dplyr::select(Dispatcher, Total_Calls, Median_TTD, High_Priority_Pct),
  caption = "Dispatchers with Longest Dispatch Times (min 10 calls)",
  header_map = list(
    Dispatcher = "Dispatcher",
    Total_Calls = "Calls",
    Median_TTD = "Median TTD (s)",
    High_Priority_Pct = "High Priority %"
  ),
  digits = 1
)
```

**Coaching Recommendations:**

- Consider pairing dispatchers from the development opportunities list with top performers for mentoring
- Analyze call types handled by slower dispatchers - are they inherently more complex?
- Review procedures and protocols with dispatchers showing high TTD variance

### Automated Recommendations

```{r}
#| label: automated-recommendations
#| echo: false
#| results: asis

recommendations <- list()

# Staffing recommendations based on volume trends
if (exists("call_volume_change") && call_volume_change > 100) {
  recommendations <- append(recommendations, 
    paste0("📊 **Staffing**: Call volume increased by ", call_volume_change, " calls. Consider reviewing staffing levels for peak periods."))
}

# Performance recommendations
if (exists("ttq_change") && ttq_change > 5) {
  recommendations <- append(recommendations,
    paste0("⏱️ **Efficiency**: Time to Queue increased by ", round(ttq_change, 1), " seconds. Review call intake procedures and staffing during peak hours."))
}

# Training recommendations based on dispatcher performance
high_variance_dispatchers <- dispatcher_performance %>%
  # Safety exclusion for non-human/system dispatcher codes
  filter(!(Dispatcher %in% c("Automatic Dispatch", "CAD to CAD Service 1", "NULL"))) %>%
  filter(Total_Calls >= 10, SD_TTD > 100) %>%
  nrow()

if (high_variance_dispatchers > 3) {
  recommendations <- append(recommendations,
    paste0("🎓 **Training**: ", high_variance_dispatchers, " dispatchers show high variability in dispatch times. Consider consistency training and standard operating procedure reviews."))
}

# SLA-based recommendations from KPI section
if (exists("law_p1_sla_current") && law_p1_sla_current < 90) {
  recommendations <- append(recommendations,
    "🚨 **Compliance**: LAW P1 SLA compliance below 90% target. See KPI Summary section for details. Immediate review recommended.")
}

if (exists("law_p2_sla_current") && law_p2_sla_current < 85) {
  recommendations <- append(recommendations,
    "🚨 **Compliance**: LAW P2 SLA compliance below 85% target. See KPI Summary section for details. Immediate review recommended.")
}
```

#### Priority Action Items

```{r}
#| label: recommendations-output
#| echo: false
#| results: asis

if (length(recommendations) > 0) {
  cat("\n**Recommended Actions:**\n\n")
  for (i in seq_along(recommendations)) {
    cat(paste0(i, ". ", recommendations[[i]], "\n\n"))
  }
} else {
  cat("\n✅ **No critical action items identified - Operations within normal parameters**\n")
}
```

### Process Improvement Opportunities

```{r}
#| label: process-improvements
#| echo: false
#| results: asis

# Identify problem types with longest handling times (using existing call_complexity data if available)
if (exists("call_complexity")) {
  complex_call_types_top <- call_complexity %>%
    arrange(desc(Mean_Processing)) %>%
    slice_head(n = 5)
  
  if (nrow(complex_call_types_top) > 0) {
    cat("\n**Complex Call Types Requiring Extended Handling:**\n\n")
    for (i in 1:nrow(complex_call_types_top)) {
      cat(paste0("- **", complex_call_types_top$Problem[i], "**: ", 
                 round(complex_call_types_top$Mean_Processing[i]), 
                 " seconds mean processing time (", 
                 complex_call_types_top$Call_Count[i], " calls)\n"))
    }
    cat("\nConsider developing specialized protocols or additional training for these call types.\n")
  }
}
```

---

### Summary statsitcs and analyses

In this section, we will analyse the continuous variables that represent the elapsed time for various segments of the call process. The variables of interest include: Time_To_Queue, Time_To_Dispatch, Phone_Time, Processing_Time, Rollout_Time, Transit_Time, and Total_Call_Time. They are defined as follows:

- Time_To_Queue
: The time from the start of the call to the time it is released to queue for dispatch.

- Time_To_Dispatch
: The time from the time the call is released for dispatch to the time the first unit is assigned.

- Phone_Time
: The time from the start of the call to the time the phone call ended.

- Processing_Time
: The time from the start of the call until the first unit is assigned.

- Rollout_Time
: The time from the assignment of the first unit to the first unit marking en route to the call.

- Transit_Time
: The time from the first unit marking en route to the call to the first unit arriving on scene.

- Total_Call_Time
: The total time from the start of the call to the time the call was closed. If the call is re-opened, then this clock stops with the first closure.

```{r}
#| label: custom-summary
#| echo: false
#| message: false
#| warning: false
#| cache: true

# Initialize variables
summary_table <- NULL
median_time_to_queue <- NA_real_
median_time_to_dispatch <- NA_real_

# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time",
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- df %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

    # Extract key metrics for use in text
  median_time_to_queue <- summary_table |>
  filter(Variable == "Time To Queue") |>
      pull(Median)
    median_time_to_queue_ca <- median_time_to_queue
    median_time_to_queue_ems <- median_time_to_queue
    median_time_to_queue_afd <- median_time_to_queue
    # Preserve overall median for reuse later
    median_time_to_queue_overall <- median_time_to_queue

  median_time_to_dispatch <- summary_table |>
  filter(Variable == "Time To Dispatch") |>
      pull(Median)
    median_time_to_dispatch_ca <- median_time_to_dispatch
    median_time_to_dispatch_ems <- median_time_to_dispatch
    median_time_to_dispatch_afd <- median_time_to_dispatch
    # Preserve overall median for reuse later
    median_time_to_dispatch_overall <- median_time_to_dispatch

    median_processing_time <- summary_table |>
      filter(Variable == "Processing Time") |>
      pull(Median)
    median_processing_time_ca <- median_processing_time
    median_processing_time_ems <- median_processing_time
    median_processing_time_afd <- median_processing_time
    # Preserve overall median for reuse later
    median_processing_time_overall <- median_processing_time

    median_phone_time <- summary_table |>
      filter(Variable == "Phone Time") |>
      pull(Median)
    median_phone_time_ca <- median_phone_time
    median_phone_time_ems <- median_phone_time
    median_phone_time_afd <- median_phone_time
    # Preserve overall median for reuse later
    median_phone_time_overall <- median_phone_time

    # Save overall summary table for later comparison
    summary_table_overall <<- summary_table

    to_ft(
      summary_table %>% dplyr::select(Variable, Minimum, Mean, Median),
      caption = "Weekly Elapsed Time Summary Table — Statistical summary of call processing times",
      header_map = list(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median"
      ),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})
```

The values from this table describe operations for the week being analyzed. In this case, the median time for a call to be placed in queue is `r median_time_to_queue` seconds. This is still in line with what has been seen in the last two weeks. The median time in queue was `r median_time_to_dispatch` seconds. These are comparable numbers with the prior weeks.

```{r}
#| label: ttq-plots
#| echo: false
#| message: false
#| warning: false
#| include: false

# Helper to format seconds as mm:ss or h:mm:ss
sec_label <- function(x) {
  s <- round(x)
  h <- s %/% 3600
  m <- (s %% 3600) %/% 60
  sec <- s %% 60
  ifelse(h > 0, sprintf("%d:%02d:%02d", h, m, sec), sprintf("%d:%02d", m, sec))
}

# Prepare TTQ values in seconds and basic stats
ttq <- df |>
  transmute(ttq_sec = as.numeric(Time_To_Queue)) |>
  filter(!is.na(ttq_sec), ttq_sec >= 0)

n_ttq <- nrow(ttq)
ttq_med <- median(ttq$ttq_sec, na.rm = TRUE)
ttq_p90 <- as.numeric(quantile(ttq$ttq_sec, 0.90, na.rm = TRUE))
ttq_p99 <- as.numeric(quantile(ttq$ttq_sec, 0.99, na.rm = TRUE))
n_clipped <- sum(ttq$ttq_sec > ttq_p99, na.rm = TRUE)

# Adaptive bin width (Freedman–Diaconis) with nicening
fd <- 2 * IQR(ttq$ttq_sec, na.rm = TRUE) / (n_ttq^(1/3))
binw <- fd
if (!is.finite(binw) || binw <= 0) binw <- 5
binw <- dplyr::case_when(
  binw < 1 ~ 1,
  binw < 2 ~ 2,
  binw < 5 ~ 5,
  binw < 10 ~ 10,
  binw < 15 ~ 15,
  TRUE ~ round(binw, -1)
)

# Clip extreme tail for readability (ensure at least 20s to show reference lines)
x_max <- max(20, ttq_p99)

# Single plot: histogram (normalized to density) with density overlay and markers
ttq_hist_dens <- ggplot(ttq, aes(x = ttq_sec)) +
  # Histogram scaled to density for alignment with density curve
  geom_histogram(aes(y = after_stat(density)),
                 binwidth = binw, boundary = 0, closed = "left",
                 fill = "#1c5789", color = "white", alpha = 0.6) +
  # Density overlay
  geom_density(color = "#1c5789", fill = "#1c5789", alpha = 0.2, adjust = 1.1, linewidth = 1) +
  # Median and P90 reference lines
  geom_vline(xintercept = ttq_med, linetype = "dashed", color = "#d62728", linewidth = 0.8) +
  geom_vline(xintercept = ttq_p90, linetype = "dotted", color = "#ff7f0e", linewidth = 0.8) +
  # NENA/NFPA reference lines at 0:15 and 0:20
  geom_vline(xintercept = 15, linetype = "longdash", color = "#2ca02c", linewidth = 0.7) +
  geom_vline(xintercept = 20, linetype = "longdash", color = "#9467bd", linewidth = 0.7) +
  # Labels for markers (placed above the density peak area)
  annotate("label", x = ttq_med, y = Inf, vjust = 1.2,
           label = paste0("Median: ", round(ttq_med), "s"), size = 4, fill = "white") +
  annotate("label", x = ttq_p90, y = Inf, vjust = 1.2,
           label = paste0("P90: ", round(ttq_p90), "s"), size = 4, fill = "white") +
  annotate("label", x = 15, y = Inf, vjust = 1.2,
           label = "NENA 0:15", size = 4, fill = "white") +
  annotate("label", x = 20, y = Inf, vjust = 1.2,
           label = "NFPA 0:20", size = 4, fill = "white") +
  scale_x_continuous(name = "Time to Queue (mm:ss)",
                     limits = c(0, x_max),
                     breaks = scales::pretty_breaks(8),
                     labels = sec_label) +
  scale_y_continuous(name = "Density") +
  labs(title = "Time to Queue — Histogram with Density",
    subtitle = paste0("Histogram normalized to density; clipped at 99th percentile (", scales::comma(n_clipped), " removed).\n",
          "Median (dashed red), 90th percentile (dotted orange). Reference lines at 0:15 (NENA) and 0:20 (NFPA 1225)."),
       caption = "Data: CAD") +
  theme_minimal(base_size = 9) +
  theme(
    plot.title = element_text(face = "bold", size = 9),
    plot.subtitle = element_text(size = 9),
    axis.title.x = element_text(size = 11),
    axis.title.y = element_text(size = 11),
    axis.text.x = element_text(size = 9),
    axis.text.y = element_text(size = 9)
  )

ttq_hist_dens

```

These combined histogram and density plots are designed to show the distribution of the elapsed time between events in the call's lifecycle. Using the 90^th^ percentile, we can state that 90% of all service calls are ready to be dispatched within `r ttq_p90` seconds. The same 90^th^ lines are reflected in the remaining plots below.

### Elapsed Time Summary {.tabset}

#### Overall

```{r}
#| label: elapsed-time-summary
#| echo: false
#| message: false
#| warning: false

# Helper to render seconds as mm:ss or h:mm:ss
sec_label <- function(x) {
  s <- round(x)
  h <- s %/% 3600
  m <- (s %% 3600) %/% 60
  sec <- s %% 60
  ifelse(h > 0, sprintf("%d:%02d:%02d", h, m, sec), sprintf("%d:%02d", m, sec))
}

# Long format of elapsed-time metrics
long_df <- bind_rows(
  df |> transmute(metric = "Time to Queue",     value = suppressWarnings(as.numeric(Time_To_Queue))),
  df |> transmute(metric = "Time to Dispatch",  value = suppressWarnings(as.numeric(Time_To_Dispatch))),
  df |> transmute(metric = "Phone Time",        value = suppressWarnings(as.numeric(Phone_Time))),
  df |> transmute(metric = "Processing Time",   value = suppressWarnings(as.numeric(Processing_Time)))
) |> filter(is.finite(value) & value >= 0)

# Summary stats per metric
stats <- long_df |>
  group_by(metric) |>
  summarise(
    N = n(),
    Median = median(value),
    P90 = as.numeric(quantile(value, 0.90)),
    P99 = as.numeric(quantile(value, 0.99)),
    Pct_le_15 = round(mean(value <= 15) * 100, 1),
    Pct_le_20 = round(mean(value <= 20) * 100, 1),
    .groups = "drop"
  )

# Present concise summary table
to_ft(
  stats,
  caption = "Elapsed Time Summary (Overall)",
  header_map = list(
    metric = "Metric",
    N = "N",
    Median = "Median (s)",
    P90 = "P90 (s)",
    P99 = "P99 (s)",
    Pct_le_15 = "% ≤ 15s",
    Pct_le_20 = "% ≤ 20s"
  ),
  digits = 1
)

# Export overall summary
dir.create("report_files", showWarnings = FALSE, recursive = TRUE)
readr::write_csv(stats, file.path("report_files", "elapsed_time_summary_overall.csv"))

# Annotation data for ECDF labels/lines
ann <- stats |>
  transmute(
    metric,
    has_sla = metric %in% c("Time to Queue", "Time to Dispatch"),
    Pct_le_15, Pct_le_20,
    x15 = 15, y15 = pmin(0.98, pmax(0.05, (Pct_le_15/100))),
    x20 = 20, y20 = pmin(0.98, pmax(0.10, (Pct_le_20/100))),
    label15 = paste0(Pct_le_15, "% ≤15s"),
    label20 = paste0(Pct_le_20, "% ≤20s"),
    med = Median, p90 = P90
  )

# ECDF facets by metric with SLA markers and medians
ecdf_plot <- ggplot(long_df, aes(x = value)) +
  stat_ecdf(color = "#1c5789", linewidth = 0.9) +
  facet_wrap(~ metric, scales = "free_x") +
  geom_vline(data = ann, aes(xintercept = med), linetype = "dashed", color = "#d62728", linewidth = 0.7) +
  geom_vline(data = ann, aes(xintercept = p90), linetype = "dotted", color = "#ff7f0e", linewidth = 0.7) +
  geom_vline(data = dplyr::filter(ann, has_sla), xintercept = 15, linetype = "longdash", color = "#2ca02c", linewidth = 0.6) +
  geom_vline(data = dplyr::filter(ann, has_sla), xintercept = 20, linetype = "longdash", color = "#9467bd", linewidth = 0.6) +
  geom_label(data = dplyr::filter(ann, has_sla), aes(x = x15, y = y15, label = label15), size = 3, fill = "white") +
  geom_label(data = dplyr::filter(ann, has_sla), aes(x = x20, y = y20, label = label20), size = 3, fill = "white") +
  scale_x_continuous(name = "Elapsed Time (mm:ss)", labels = sec_label) +
  scale_y_continuous(name = "Cumulative Proportion", labels = scales::percent_format(accuracy = 1)) +
  labs(
    title = "Elapsed Time ECDF by Metric",
    subtitle = "Dashed: Median. Dotted: P90. Longdash: NENA 15s and NFPA 20s reference.",
    caption = "Data: CAD"
  ) +
  theme_minimal(base_size = 9) +
  theme(
    plot.title = element_text(face = "bold", size = 11),
    strip.text = element_text(face = "bold"),
    axis.title.x = element_text(size = 10),
    axis.title.y = element_text(size = 10)
  )

ecdf_plotly <- plotly::ggplotly(ecdf_plot, tooltip = c("x","y"))
ecdf_plotly
```

#### By Agency

```{r}
#| label: elapsed-time-agency
#| echo: false
#| message: false
#| warning: false

# Build long form with Agency
long_agency <- bind_rows(
  df |> transmute(metric = "Time to Queue",     value = suppressWarnings(as.numeric(Time_To_Queue)),    Agency),
  df |> transmute(metric = "Time to Dispatch",  value = suppressWarnings(as.numeric(Time_To_Dispatch)), Agency),
  df |> transmute(metric = "Phone Time",        value = suppressWarnings(as.numeric(Phone_Time)),       Agency),
  df |> transmute(metric = "Processing Time",   value = suppressWarnings(as.numeric(Processing_Time)),  Agency)
) |> filter(is.finite(value) & value >= 0 & !is.na(Agency))

stats_ag <- long_agency |>
  group_by(metric, Agency) |>
  summarise(
    N = n(),
    Median = median(value),
    P90 = as.numeric(quantile(value, 0.90)),
    Pct_le_15 = round(mean(value <= 15) * 100, 1),
    Pct_le_20 = round(mean(value <= 20) * 100, 1),
    .groups = "drop"
  )

# Medians/P90 per facet+group for vlines
ann_ag <- stats_ag |>
  transmute(metric, Agency, has_sla = metric %in% c("Time to Queue","Time to Dispatch"),
            med = Median, p90 = P90)

# ECDF colored by Agency; SLA markers only for TTQ/TTD
ecdf_agency <- ggplot(long_agency, aes(x = value, color = Agency)) +
  stat_ecdf(linewidth = 0.8) +
  facet_wrap(~ metric, scales = "free_x") +
  geom_vline(data = ann_ag, aes(xintercept = med, color = Agency), linetype = "dashed", linewidth = 0.5, show.legend = FALSE) +
  geom_vline(data = ann_ag, aes(xintercept = p90, color = Agency), linetype = "dotted", linewidth = 0.5, show.legend = FALSE) +
  geom_vline(data = dplyr::filter(ann_ag, has_sla), xintercept = 15, linetype = "longdash", color = "#2ca02c", linewidth = 0.5, inherit.aes = FALSE) +
  geom_vline(data = dplyr::filter(ann_ag, has_sla), xintercept = 20, linetype = "longdash", color = "#9467bd", linewidth = 0.5, inherit.aes = FALSE) +
  scale_x_continuous(name = "Elapsed Time (mm:ss)", labels = sec_label) +
  scale_y_continuous(name = "Cumulative Proportion", labels = scales::percent_format(accuracy = 1)) +
  labs(title = "Elapsed Time ECDF by Metric — Agency", caption = "Data: CAD") +
  theme_minimal(base_size = 9) +
  theme(plot.title = element_text(face = "bold", size = 11), strip.text = element_text(face = "bold"))

to_ft(stats_ag, caption = "Elapsed Time Summary by Agency", header_map = list(
  metric = "Metric", Agency = "Agency", N = "N", Median = "Median (s)", P90 = "P90 (s)", Pct_le_15 = "% ≤ 15s", Pct_le_20 = "% ≤ 20s"
), digits = 1)

# Export by-agency summary
dir.create("report_files", showWarnings = FALSE, recursive = TRUE)
readr::write_csv(stats_ag, file.path("report_files", "elapsed_time_summary_by_agency.csv"))

ecdf_agency_plotly <- plotly::ggplotly(ecdf_agency, tooltip = c("x","y","colour"))
ecdf_agency_plotly
```

#### By Priority

```{r}
#| label: elapsed-time-priority
#| echo: false
#| message: false
#| warning: false

# Build long form with Priority_Number (ensure character for labeling)
long_pri <- bind_rows(
  df |> transmute(metric = "Time to Queue",     value = suppressWarnings(as.numeric(Time_To_Queue)),    Priority = as.character(Priority_Number)),
  df |> transmute(metric = "Time to Dispatch",  value = suppressWarnings(as.numeric(Time_To_Dispatch)), Priority = as.character(Priority_Number)),
  df |> transmute(metric = "Phone Time",        value = suppressWarnings(as.numeric(Phone_Time)),       Priority = as.character(Priority_Number)),
  df |> transmute(metric = "Processing Time",   value = suppressWarnings(as.numeric(Processing_Time)),  Priority = as.character(Priority_Number))
) |> filter(is.finite(value) & value >= 0 & !is.na(Priority))

stats_pr <- long_pri |>
  group_by(metric, Priority) |>
  summarise(
    N = n(),
    Median = median(value),
    P90 = as.numeric(quantile(value, 0.90)),
    Pct_le_15 = round(mean(value <= 15) * 100, 1),
    Pct_le_20 = round(mean(value <= 20) * 100, 1),
    .groups = "drop"
  )

ann_pr <- stats_pr |>
  transmute(metric, Priority, has_sla = metric %in% c("Time to Queue","Time to Dispatch"),
            med = Median, p90 = P90)

ecdf_priority <- ggplot(long_pri, aes(x = value, color = Priority)) +
  stat_ecdf(linewidth = 0.8) +
  facet_wrap(~ metric, scales = "free_x") +
  geom_vline(data = ann_pr, aes(xintercept = med, color = Priority), linetype = "dashed", linewidth = 0.5, show.legend = FALSE) +
  geom_vline(data = ann_pr, aes(xintercept = p90, color = Priority), linetype = "dotted", linewidth = 0.5, show.legend = FALSE) +
  geom_vline(data = dplyr::filter(ann_pr, has_sla), xintercept = 15, linetype = "longdash", color = "#2ca02c", linewidth = 0.5, inherit.aes = FALSE) +
  geom_vline(data = dplyr::filter(ann_pr, has_sla), xintercept = 20, linetype = "longdash", color = "#9467bd", linewidth = 0.5, inherit.aes = FALSE) +
  scale_x_continuous(name = "Elapsed Time (mm:ss)", labels = sec_label) +
  scale_y_continuous(name = "Cumulative Proportion", labels = scales::percent_format(accuracy = 1)) +
  labs(title = "Elapsed Time ECDF by Metric — Priority", caption = "Data: CAD") +
  theme_minimal(base_size = 9) +
  theme(plot.title = element_text(face = "bold", size = 11), strip.text = element_text(face = "bold"))

to_ft(stats_pr, caption = "Elapsed Time Summary by Priority", header_map = list(
  metric = "Metric", Priority = "Priority", N = "N", Median = "Median (s)", P90 = "P90 (s)", Pct_le_15 = "% ≤ 15s", Pct_le_20 = "% ≤ 20s"
), digits = 1)

# Export by-priority summary
dir.create("report_files", showWarnings = FALSE, recursive = TRUE)
readr::write_csv(stats_pr, file.path("report_files", "elapsed_time_summary_by_priority.csv"))

ecdf_priority_plotly <- plotly::ggplotly(ecdf_priority, tooltip = c("x","y","colour"))
ecdf_priority_plotly
```

```{r}
#| label: elapsed-time-plots
#| echo: false
#| message: false
#| warning: false
#| include: false

# Reusable plot helper: histogram normalized to density with density overlay and markers
plot_time_hist_dens <- function(data, var, title_text) {
  vals <- as.numeric(data[[var]])
  vals <- vals[is.finite(vals) & vals >= 0]
  if (length(vals) < 2) return(NULL)

  # Stats
  n <- length(vals)
  med <- median(vals)
  p90 <- as.numeric(quantile(vals, 0.90))
  p99 <- as.numeric(quantile(vals, 0.99))
  n_clipped <- sum(vals > p99)

  # Freedman–Diaconis bin width with nicening
  fd <- 2 * IQR(vals) / (n^(1/3))
  binw <- fd
  if (!is.finite(binw) || binw <= 0) binw <- 5
  binw <- dplyr::case_when(
    binw < 1 ~ 1,
    binw < 2 ~ 2,
    binw < 5 ~ 5,
    binw < 10 ~ 10,
    binw < 15 ~ 15,
    TRUE ~ round(binw, -1)
  )

  x_max <- max(20, p99)

  # mm:ss (or h:mm:ss) labels
  sec_label <- function(x) {
    s <- round(x)
    h <- s %/% 3600
    m <- (s %% 3600) %/% 60
    sec <- s %% 60
    ifelse(h > 0, sprintf("%d:%02d:%02d", h, m, sec), sprintf("%d:%02d", m, sec))
  }

  ggplot(data.frame(x = vals), aes(x = x)) +
    geom_histogram(aes(y = after_stat(density)),
                   binwidth = binw, boundary = 0, closed = "left",
                   fill = "#1c5789", color = "white", alpha = 0.6) +
    geom_density(color = "#1c5789", fill = "#1c5789", alpha = 0.2, adjust = 1.1, linewidth = 1) +
    geom_vline(xintercept = med, linetype = "dashed", color = "#d62728", linewidth = 0.8) +
    geom_vline(xintercept = p90, linetype = "dotted", color = "#ff7f0e", linewidth = 0.8) +
    geom_vline(xintercept = 15, linetype = "longdash", color = "#2ca02c", linewidth = 0.7) +
    geom_vline(xintercept = 20, linetype = "longdash", color = "#9467bd", linewidth = 0.7) +
  annotate("label", x = med, y = Inf, vjust = 1.2,
       label = "Median", size = 3.5, fill = "white") +
  annotate("label", x = p90, y = Inf, vjust = 1.2,
       label = "P90", size = 3.5, fill = "white") +
  annotate("label", x = 15, y = Inf, vjust = 1.2,
       label = "NENA", size = 3.5, fill = "white") +
  annotate("label", x = 20, y = Inf, vjust = 1.2,
       label = "NFPA", size = 3.5, fill = "white") +
    scale_x_continuous(name = "Time (mm:ss)",
                       limits = c(0, x_max),
                       breaks = scales::pretty_breaks(8),
                       labels = sec_label) +
    scale_y_continuous(name = "Density") +
    labs(title = title_text,
      subtitle = NULL,
      caption = "Data: CAD") +
    theme_minimal(base_size = 9) +
    theme(
      plot.title = element_text(face = "bold", size = 9),
      axis.title.x = element_text(size = 11),
      axis.title.y = element_text(size = 11),
      axis.text.x = element_text(size = 9),
      axis.text.y = element_text(size = 9)
    )
}

# Build and print plots for other elapsed-time metrics
p_dispatch   <- plot_time_hist_dens(df, "Time_To_Dispatch",  "Time to Dispatch — Histogram with Density")
p_phone      <- plot_time_hist_dens(df, "Phone_Time",       "Phone Time — Histogram with Density")
p_processing <- plot_time_hist_dens(df, "Processing_Time",  "Processing Time — Histogram with Density")
#p_rollout    <- plot_time_hist_dens(df, "Rollout_Time",     "Rollout Time — Histogram with Density")
#p_transit    <- plot_time_hist_dens(df, "Transit_Time",     "Transit Time — Histogram with Density")
#p_total      <- plot_time_hist_dens(df, "Total_Call_Time",  "Total Call Time — Histogram with Density")

if (!is.null(p_dispatch))   print(p_dispatch)
if (!is.null(p_phone))      print(p_phone)
if (!is.null(p_processing)) print(p_processing)
#if (!is.null(p_rollout))    print(p_rollout)
#if (!is.null(p_transit))    print(p_transit)
#if (!is.null(p_total))      print(p_total)
```

```{r}
#| label: elapsed-time-grid
#| echo: false
#| message: false
#| warning: false
#| include: false

# Short-title versions for grid
p_dispatch_grid   <- plot_time_hist_dens(df, "Time_To_Dispatch",  "Time to Dispatch")
p_phone_grid      <- plot_time_hist_dens(df, "Phone_Time",       "Phone Time")
p_processing_grid <- plot_time_hist_dens(df, "Processing_Time",  "Processing Time")
#p_rollout_grid    <- plot_time_hist_dens(df, "Rollout_Time",     "Rollout Time")
#p_transit_grid    <- plot_time_hist_dens(df, "Transit_Time",     "Transit Time")
#p_total_grid      <- plot_time_hist_dens(df, "Total_Call_Time",  "Total Call Time")

# Arrange elapsed-time plots in an adaptive grid (skip NULL plots)
plots_list <- list(
  p_dispatch_grid,  # Time To Dispatch
  p_phone_grid,     # Phone Time
  p_processing_grid # Processing Time
  # p_rollout_grid,   # Rollout Time
  # p_transit_grid,   # Transit Time
  # p_total_grid      # Total Call Time
)
plots_list <- Filter(Negate(is.null), plots_list)

if (length(plots_list) > 0) {
  n_plots <- length(plots_list)
  # Prefer 2x2 when exactly 4 plots; otherwise use up to 3 columns
  ncol <- if (n_plots == 4) 2 else min(3, n_plots)
  nrow <- ceiling(n_plots / ncol)
  grid <- ggpubr::ggarrange(plotlist = plots_list, ncol = ncol, nrow = nrow, align = "hv")
  print(grid)
}
```

**Plot Key:**

| Line Type/Color      | Meaning                |
|---------------------|------------------------|
| **Dashed Red**    | Median                 |
| **Dotted Orange** | 90th Percentile (P90)  |
| **Longdash Green** | NENA 0:15 Standard     |
| **Longdash Purple** | NFPA 0:20 Standard     |

These show that the processing times for DECC are well within the NENA and NFPA guidelines. This is good operational data to show how well we are performing with respect to those guidelines. Over time, we can track these metrics to ensure that we continue to meet or exceed those standards.

## Discipline Analyses

As discussed earlier, we can create additional subsets from this data to look at specific areas of interest. We will create several new datasets from this weekly set for further analysis. The first will be a dataset that combines APD Priority 1 calls with AFD Priority 1 and 2 calls and evaluates those as emergency calls. We will also create specific datasets for law, fire, and EMS for specific analyses of the disciplines. We will also create datasets that identify calls that exceed certain parameters that have been defined from other reports. Finally, because we have been evaluating Cardiac Arrest calls for some time, we'll create and analyze that dataset.

```{r}
#| label: new-datasets
#| echo: false
#| message: false
#| warning: false
#| cache: true

df_hp <- df |>
  filter((Agency == "POLICE" & as.numeric(Priority_Number) < 2) |
         (Agency %in% c("FIRE", "EMS") & as.numeric(Priority_Number) < 3))

lw_hp <- df_last |>
  filter((Agency == "POLICE" & as.numeric(Priority_Number) < 2) |
         (Agency %in% c("FIRE", "EMS") & as.numeric(Priority_Number) < 3))

df_lp <- df |>
  filter((Agency == "POLICE" & as.numeric(Priority_Number) >= 2) |
         (Agency %in% c("FIRE", "EMS") & as.numeric(Priority_Number) >= 3))

lw_lp <- df_last |>
  filter((Agency == "POLICE" & as.numeric(Priority_Number) >= 2) |
         (Agency %in% c("FIRE", "EMS") & as.numeric(Priority_Number) >= 3))

df_law <- df |> filter(Agency == "POLICE")
df_fire <- df |> filter(Agency == "FIRE")
df_ems <- df |> filter(Agency == "EMS")
df_ttq_delay <- df_hp |> filter(Time_To_Queue > 60)
df_ttd_delay <- df_hp |> filter(Time_To_Dispatch > 60)
df_ca <- df |> filter(Problem == "CARDIAC ARREST ALS 2- SUPV")

lw_law <- df_last |> filter(Agency == "POLICE")
lw_fire <- df_last |> filter(Agency == "FIRE")
lw_ems <- df_last |> filter(Agency == "EMS")
lw_ttq_delay <- last_week |> filter(Time_To_Queue > 60)
lw_ttd_delay <- last_week |> filter(Time_To_Dispatch > 60)
lw_cs <- df_last |> filter(Problem == "CARDIAC ARREST ALS 2- SUPV")

mental_health <- c("MUTUAL PSYCHOLOGICAL EMERGENCY", "PSYCHIATRIC EMERGENCY ALS 1", "PSYCHIATRIC EMERGENCY VIOLENT", "WELFARE CHECK", "JUMPER FROM WWB", "MENTAL HEALTH CASE", "SUICIDE DELAY", "SUICIDE IN PROG NO INJ", "SUICIDE IN PROG INJ/PILLS", "SUICIDE IN PROG TRAUMA")

hybrids <- c("PICKERAL, LAUREN", "VALENTIN, EMERITA", "FARRI, ANGELINE", "PATTERSON, JAHAIRA", "MAIOBELLO, BERMARI")

# Mental health related calls subset
# - Filters rows where `Problem` is one of the values in `mental_health`
# - Uses `%in%` and handles potential NA values safely with `is.na()` check
# - Creates a new dataset `df_mh` for downstream analysis
df_mh <- df |> dplyr::filter(!is.na(Problem) & Problem %in% mental_health)
lw_mh <- df_last |> dplyr::filter(!is.na(Problem) & Problem %in% mental_health)

# Hybrid call taker subset
# - Filters rows where the Call_Taker is one of the Hybrid call takers
df_hybrid <- df |> dplyr::filter(!is.na(Call_Taker) & Call_Taker %in% hybrids)
lw_hybrid <- df_last |> dplyr::filter(!is.na(Call_Taker) & Call_Taker %in% hybrids)

df_nonhybrid <- df |> dplyr::filter(!is.na(Call_Taker) & !(Call_Taker %in% hybrids))
lw_nonhybrid <- df_last |> dplyr::filter(!is.na(Call_Taker) & !(Call_Taker %in% hybrids))

# E-911 Calls subset
# - Filters rows where `Call_Reception` is `E-911`
# - Creates a new dataset `df_911` for downstream analysis
df_911 <- df |> dplyr::filter(!is.na(Call_Reception) & Call_Reception == "E-911")
df_non_e <- df |> dplyr::filter(!is.na(Call_Reception) & Call_Reception != "E-911")
lw_911 <- df_last |> dplyr::filter(!is.na(Call_Reception) & Call_Reception == "E-911")
lw_non_e <- df_last |> dplyr::filter(!is.na(Call_Reception) & Call_Reception != "E-911")

# Call Reception Not Recorded subset
# - Filters rows where `Call_Reception` was not recorded/captured or is NA
# - Normalize values for robust matching
df_nrr <- df |>
  dplyr::filter(
    is.na(Call_Reception) |
      toupper(trimws(Call_Reception)) %in% c("NOT RECORDED", "NOT CAPTURED")
  )

lw_nrr <- df_last |>
  dplyr::filter(
    is.na(Call_Reception) |
    toupper(trimws(Call_Reception)) %in% c("NOT RECORDED", "NOT CAPTURED")
  )
```

```{r}
#| label: precomputed-summaries
#| echo: false
#| message: false
#| warning: false
#| cache: true

# Overall
dow_counts <- df |> count(DOW, sort = TRUE)
hour_counts <- df |> count(Hour, sort = TRUE)
cr_counts <- df |> count(Call_Reception, sort = TRUE)
pri_counts <- df |> count(Priority_Number, sort = TRUE)

lw_dow_counts <- df_last |> count(DOW, sort = TRUE)
lw_hour_counts <- df_last |> count(Hour, sort = TRUE)
lw_cr_counts <- df_last |> count(Call_Reception, sort = TRUE)
lw_pri_counts <- df_last |> count(Priority_Number, sort = TRUE)

# APD / Law
dow_counts_law <- df_law |> count(DOW, sort = TRUE)
hour_counts_law <- df_law |> count(Hour, sort = TRUE)
cr_counts_law <- df_law |> count(Call_Reception, sort = TRUE)
pri_counts_law <- df_law |> count(Priority_Number, sort = TRUE)

lw_dow_counts_law <- lw_law |> count(DOW, sort = TRUE)
lw_hour_counts_law <- lw_law |> count(Hour, sort = TRUE)
lw_cr_counts_law <- lw_law |> count(Call_Reception, sort = TRUE)
lw_pri_counts_law <- lw_law |> count(Priority_Number, sort = TRUE)

# AFD / Fire
dow_counts_fire <- df_fire |> count(DOW, sort = TRUE)
hour_counts_fire <- df_fire |> count(Hour, sort = TRUE)
cr_counts_fire <- df_fire |> count(Call_Reception, sort = TRUE)
pri_counts_fire <- df_fire |> count(Priority_Number, sort = TRUE)

lw_dow_counts_fire <- lw_fire |> count(DOW, sort = TRUE)
lw_hour_counts_fire <- lw_fire |> count(Hour, sort = TRUE)
lw_cr_counts_fire <- lw_fire |> count(Call_Reception, sort = TRUE)
lw_pri_counts_fire <- lw_fire |> count(Priority_Number, sort = TRUE)

# EMS
dow_counts_ems <- df_ems |> count(DOW, sort = TRUE)
hour_counts_ems <- df_ems |> count(Hour, sort = TRUE)
cr_counts_ems <- df_ems |> count(Call_Reception, sort = TRUE)
pri_counts_ems <- df_ems |> count(Priority_Number, sort = TRUE)

lw_dow_counts_ems <- lw_ems |> count(DOW, sort = TRUE)
lw_hour_counts_ems <- lw_ems |> count(Hour, sort = TRUE)
lw_cr_counts_ems <- lw_ems |> count(Call_Reception, sort = TRUE)
lw_pri_counts_ems <- lw_ems |> count(Priority_Number, sort = TRUE)

# E-911 / Non-911
dow_counts_911 <- df_911 |> count(DOW, sort = TRUE)
hour_counts_911 <- df_911 |> count(Hour, sort = TRUE)
cr_counts_911 <- df_911 |> count(Call_Reception, sort = TRUE)

lw_dow_counts_911 <- lw_911 |> count(DOW, sort = TRUE)
lw_hour_counts_911 <- lw_911 |> count(Hour, sort = TRUE)
lw_cr_counts_911 <- lw_911 |> count(Call_Reception, sort = TRUE)

dow_counts_non911 <- df_non_e |> count(DOW, sort = TRUE)
hour_counts_non911 <- df_non_e |> count(Hour, sort = TRUE)
cr_counts_non911 <- df_non_e |> count(Call_Reception, sort = TRUE)

lw_dow_counts_non911 <- lw_non_e |> count(DOW, sort = TRUE)
lw_hour_counts_non911 <- lw_non_e |> count(Hour, sort = TRUE)
lw_cr_counts_non911 <- lw_non_e |> count(Call_Reception, sort = TRUE)

# Mental Health subset
dow_counts_mh <- df_mh |> count(DOW, sort = TRUE)
hour_counts_mh <- df_mh |> count(Hour, sort = TRUE)
cr_counts_mh <- df_mh |> count(Call_Reception, sort = TRUE)
pri_counts_mh <- df_mh |> count(Priority_Number, sort = TRUE)

lw_dow_counts_mh <- lw_mh |> count(DOW, sort = TRUE)
lw_hour_counts_mh <- lw_mh |> count(Hour, sort = TRUE)
lw_cr_counts_mh <- lw_mh |> count(Call_Reception, sort = TRUE)
lw_pri_counts_mh <- lw_mh |> count(Priority_Number, sort = TRUE)

# High-priority subset
pri_counts_hp <- df_hp |> count(Priority_Number, sort = TRUE)

lw_pri_counts_hp <- lw_hp |> count(Priority_Number, sort = TRUE)
```

By defining these datasets, we can now add to our analyses. For example, we can reuse the same information from above to drill down into APD and AFD calls. Starting with APD calls for service, we can examine everything as we did above.

### APD Analyses

```{r}
#| label: apd-dow-distribution
#| echo: false
#| fig-cap: "Number of calls for service by day of the week."
# ggplot2 (precomputed in precomputed-summaries)
dow_counts_law <- dow_counts_law

max_dow_info_law <- dow_counts_law |> filter(n == max(n))
busiest_day_abbr_law <- max_dow_info_law |> slice(1) |> pull(DOW)
busiest_day_count_law <- max_dow_info_law |> slice(1) |> pull(n)

min_dow_info_law <- dow_counts_law |> filter(n == min(n))
slowest_day_abbr_law <- min_dow_info_law |> slice(1) |> pull(DOW)
slowest_day_count_law <- min_dow_info_law |> slice(1) |> pull(n)

# Create mapping from abbreviations to full day names
day_names <- c(
  "SUN" = "Sunday",
  "MON" = "Monday",
  "TUE" = "Tuesday",
  "WED" = "Wednesday",
  "THU" = "Thursday",
  "FRI" = "Friday",
  "SAT" = "Saturday"
)

# Convert abbreviations to full day names
busiest_day_law <- day_names[busiest_day_abbr_law]
slowest_day_law <- day_names[slowest_day_abbr_law]

barDOW_APD <- df_law |> ggplot(aes(x = DOW, fill = DOW)) +
    geom_bar() +
    paletteer::scale_fill_paletteer_d("ggsci::blue_material") +
    labs(
        title = "Number of Calls for Service for APD by Day of the Week",
        x = "Day of the Week",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barDOW_APD
```

This week, `r busiest_day_law` was the busiest day of the week for APD service calls. `r slowest_day_law`, being the lightest day of the week overall, was the lightest day for the APD as well.

```{r}
#| label: apd-hour-distribution
#| echo: false
#| fig-cap: "Number of calls for service by hour of the day."
#| cache: true
# ggplot2 (precomputed in precomputed-summaries)
hour_counts_law <- hour_counts_law

max_hour_info_law <- hour_counts_law |> filter(n == max(n))
busiest_hour_law <- sprintf("%02d", max_hour_info_law |> slice(1) |> pull(Hour))
busiest_hour_count_law <- max_hour_info_law |> slice(1) |> pull(n)

min_hour_info_law <- hour_counts_law |> filter(n == min(n))
slowest_hour_law <- sprintf("%02d", min_hour_info_law |> slice(1) |> pull(Hour))
slowest_hour_count_law <- min_hour_info_law |> slice(1) |> pull(n)

barHour_APD <- df_law |> ggplot(aes(x = Hour, fill = Hour)) +
    geom_bar() +
  # Use precomputed blue palette to avoid repeated paletteer calls
  scale_fill_manual(values = palette_blue_24) +
    labs(
        title = "Number of Calls for Service for APD by Hour of the Day",
        x = "Hour of the Day",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barHour_APD
```

The busiest hours for the week were 1000 and 1700 hours. There is still a consistent pattern to the morning and afternoon rush hours being the busiest times of the week.

```{r}
#| label: apd-reception-distribution
#| echo: false
#| fig-cap: "Number of calls for service by call reception."
# ggplot2 (precomputed in precomputed-summaries)
cr_counts_law <- cr_counts_law

max_cr_info_law <- cr_counts_law |> filter(n == max(n))
busiest_cr_law <- max_cr_info_law |> slice(1) |> pull(Call_Reception)
busiest_cr_count_law <- max_cr_info_law |> slice(1) |> pull(n)

barReception_APD <- df_law |> ggplot(aes(x = Call_Reception, fill = Call_Reception)) +
    geom_bar() +
    paletteer::scale_fill_paletteer_d("ggsci::blue_material") +
    labs(
        title = "Number of Calls for Service for by Call Reception",
        x = "Call Reception",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barReception_APD
```

As can be seen, the majority of calls came through `r busiest_cr_law`. This stands in contrast to `r busiest_cr` being the vehicle for the most calls overall for the week.

```{r}
#| label: apd-problem-distribution
#| echo: false
#| fig-cap: "Number of calls for service by call type."
# ggplot2
prob_counts_law <- df_law |>
  count(Problem, sort = TRUE)

max_prob_info_law <- prob_counts_law |> filter(n == max(n))
busiest_prob_law <- max_prob_info_law |> slice(1) |> pull(Problem)
busiest_prob_count_law <- max_prob_info_law |> slice(1) |> pull(n)

problem_counts_APD <- df_law |>
  count(Problem, sort = TRUE) |>
  slice_head(n = 10)

barProblem_APD <- problem_counts_APD |>
    ggplot(aes(x = reorder(Problem, -n), y = n, fill = Problem)) +
    geom_bar(stat = "identity") +
    paletteer::scale_fill_paletteer_d("ggsci::blue_material") +
    labs(
        title = "Number of Calls for Service by Call Type",
        x = "Call Type",
        y = "Number of Calls"
    ) +
    geom_text(
        aes(label = n),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barProblem_APD
```

The largest call type was for `r busiest_prob_law`, which was also the largest call type for the week overall. This could be something to monitor over time to see how the trend changes over time.

```{r}
#| label: apd-priority-distribution
#| echo: false
#| fig-cap: "Number of calls for service by priority level."
# ggplot2
# Ensure Priority_Number is parsed to numeric and ordered
df_law_priority <- df_law %>%
  mutate(
    Priority_Number_num = suppressWarnings(as.numeric(gsub("[^0-9]", "", as.character(Priority_Number)))),
    Priority_Number_fac = factor(
      Priority_Number_num,
      levels = sort(unique(na.omit(Priority_Number_num))),
      ordered = TRUE
    )
  )

pri_counts_law <- df_law_priority |>
  filter(!is.na(Priority_Number_fac)) |>
  count(Priority_Number_fac, sort = FALSE)

max_pri_info_law <- pri_counts_law |> filter(n == max(n))
busiest_pri_law <- max_pri_info_law |> slice(1) |> pull(Priority_Number_fac)
busiest_pri_law_num <- suppressWarnings(as.numeric(as.character(busiest_pri_law)))
busiest_pri_count_law <- max_pri_info_law |> slice(1) |> pull(n)

# Calculate percentage for APD priority calls
busiest_pri_law_percentage <- round((sum(df_law_priority$Priority_Number_num == busiest_pri_law_num, na.rm = TRUE) / nrow(df_law_priority)) * 100, 1)

# Calculate percentage for P1 calls
p1_law_percentage <- round((sum(df_law_priority$Priority_Number_num == 1, na.rm = TRUE) / nrow(df_law_priority)) * 100, 1)

barPriority_APD <- df_law_priority |> ggplot(aes(x = Priority_Number_fac, fill = Priority_Number_fac)) +
    geom_bar() +
    paletteer::scale_fill_paletteer_d("ggsci::blue_material") +
    labs(
        title = "Number of Calls for Service by Priority Level",
        x = "Priority Level",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barPriority_APD
```

As expected, the largest number of calls were Priority `r busiest_pri_law` calls which represent `r busiest_pri_law_percentage` percent of all APD calls. Again, this comports with the overall weekly trends.

```{r}
#| label: apd-custom-summary
#| echo: false
#| message: false
#| warning: false

# Initialize variables
summary_table <- NULL
median_phone_time <- NA_real_
median_processing_time <- NA_real_

# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time",
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_law)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- df_law %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

    # Extract key metrics for use in text
    median_time_to_queue <- summary_table |>
      filter(Variable == "Time To Queue") |>
      pull(Median)
  # Keep a namespaced copy for APD section
  median_time_to_queue_apd <- median_time_to_queue

    median_time_to_dispatch <- summary_table |>
      filter(Variable == "Time To Dispatch") |>
      pull(Median)
  # Keep a namespaced copy for APD section
  median_time_to_dispatch_apd <- median_time_to_dispatch

    median_processing_time <- summary_table |>
      filter(Variable == "Processing_Time") |>
      pull(Median)
  # Keep a namespaced copy for APD section
  median_processing_time_apd <- median_processing_time

    median_phone_time <- summary_table |>
      filter(Variable == "Phone_Time") |>
      pull(Median)
  # Keep a namespaced copy for APD section
  median_phone_time_apd <- median_phone_time

  apd_p1_calls <- df_law |>
    dplyr::filter(Priority_Number == 1)

  apd_p1_spec <- df_law |>
    dplyr::filter(Priority_Number == 1 & Time_To_Dispatch <= 120)

  apd_p2_calls <- df_law |>
    dplyr::filter(Priority_Number == 2)

  apd_p2_spec <- df_law |>
    dplyr::filter(Priority_Number == 2 & Time_To_Dispatch <= 600)

  apd_p3_calls <- df_law |>
    dplyr::filter(Priority_Number == 3)

  apd_p3_spec <- df_law |>
    dplyr::filter(Priority_Number == 3 & Time_To_Dispatch <= 1200)

  apd_p4_calls <- df_law |>
    dplyr::filter(Priority_Number == 4)

  apd_p4_spec <- df_law |>
    dplyr::filter(Priority_Number == 4 & Time_To_Dispatch <= 3600)

  apd_p1_compliance_pct <- round((nrow(apd_p1_spec) / nrow(apd_p1_calls)) * 100, 2)
  apd_p2_compliance_pct <- round((nrow(apd_p2_spec) / nrow(apd_p2_calls)) * 100, 2)
  apd_p3_compliance_pct <- round((nrow(apd_p3_spec) / nrow(apd_p3_calls)) * 100, 2)
  apd_p4_compliance_pct <- round((nrow(apd_p4_spec) / nrow(apd_p4_calls)) * 100, 2)

    to_ft(
      summary_table %>% dplyr::select(Variable, Minimum, Mean, Median),
      caption = "Weekly Elapsed Time Summary Table — Statistical summary of call processing times",
      header_map = list(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median"
      ),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})

# Extract key metrics for inline use (outside tryCatch to ensure they're available)
median_phone_time <- tryCatch({
  if (exists("summary_table")) {
    summary_table |> filter(Variable == "Phone Time") |> pull(Median)
  } else {
    NA_real_
  }
}, error = function(e) NA_real_)

median_processing_time <- tryCatch({
  if (exists("summary_table")) {
  summary_table |> filter(Variable == "Processing Time") |> pull(Median)
  } else {
    NA_real_
  }
}, error = function(e) NA_real_)

# Calculate P4 percentage for inline use
p4_percentage_apd <- round((sum(df_law$Priority_Number == "4", na.rm = TRUE) / nrow(df_law)) * 100, 1)
```

This table shows that overall, we have a median time on the phones of about `r median_phone_time` seconds and it takes about double that for a call to start and be dispatched, `r median_processing_time` seconds. Some of that difference is going to be due to having to hold Priority 4 and above calls until there is a unit available. Since the P4 calls are `r p4_percentage_apd` percent of APD calls, this could have a measureable impact on service times for DECC staff.

| Priority | Dispatched in SLA | Total Calls | Pct in SLA |
|:--------:|:--------:|:--------:|:--------:|
| 1 | `r nrow(apd_p1_spec)` | `r nrow(apd_p1_calls)` | `r apd_p1_compliance_pct`% |
| 2 | `r nrow(apd_p2_spec)` | `r nrow(apd_p2_calls)` | `r apd_p2_compliance_pct`% |
| 3 | `r nrow(apd_p3_spec)` | `r nrow(apd_p3_calls)` | `r apd_p3_compliance_pct`% |
| 4 | `r nrow(apd_p4_spec)` | `r nrow(apd_p4_calls)` | `r apd_p4_compliance_pct`% |

: SLA Compliance for APD Calls (P1 & P2) {.striped .hover}

As you will note, these numbers are significantly lower than those found in the other weekend reports. In those reports, there was an additional filter which removed all calls where the time in queue was longer than 30 minutes. This report reflects a more accurate view of actual compliance with our partners' SLAs. This table will be repeated for the fire and medical calls later in the report.

### AFD FIRE Analyses

Because AFD calls for service can be split into two distinct disciplines, fire-related calls and medical-related calls, DECC has, historically, separated the two disciplines for analytical purposes. So this section will look at fire-related calls for service for the week.

```{r}
#| label: afd-fire-dow-distribution
#| echo: false
#| fig-cap: "Number of calls for service by day of the week."
# ggplot2 (precomputed in precomputed-summaries)
dow_counts_fire <- dow_counts_fire

max_dow_info_fire <- dow_counts_fire |> filter(n == max(n))
busiest_day_abbr_fire <- max_dow_info_fire |> slice(1) |> pull(DOW)
busiest_day_count_fire <- max_dow_info_fire |> slice(1) |> pull(n)

min_dow_info_fire <- dow_counts_fire |> filter(n == min(n))
slowest_day_abbr_fire <- min_dow_info_fire |> slice(1) |> pull(DOW)
slowest_day_count_fire <- min_dow_info_fire |> slice(1) |> pull(n)

# Create mapping from abbreviations to full day names
day_names <- c(
  "SUN" = "Sunday",
  "MON" = "Monday",
  "TUE" = "Tuesday",
  "WED" = "Wednesday",
  "THU" = "Thursday",
  "FRI" = "Friday",
  "SAT" = "Saturday"
)

# Convert abbreviations to full day names
busiest_day_fire <- day_names[busiest_day_abbr_fire]
slowest_day_fire <- day_names[slowest_day_abbr_fire]

barDOW_AFD <- df_fire |> ggplot(aes(x = DOW, fill = DOW)) +
    geom_bar() +
    paletteer::scale_fill_paletteer_d("ggsci::red_material") +
    labs(
        title = "Number of Calls for Service for AFD by Day of the Week",
        x = "Day of the Week",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barDOW_AFD
```

This week, the busiest day for fire-related calls was `r busiest_day_fire` with `r busiest_day_count_fire` calls for service. `r slowest_day_fire` was the lightest day for fire-related calls with `r slowest_day_count_fire` calls for service.

```{r}
#| label: afd-fire-hour-distribution
#| echo: false
#| fig-cap: "Number of calls for service by hour of the day."
#| cache: true
# ggplot2 (precomputed in precomputed-summaries)
hour_counts_fire <- hour_counts_fire

max_hour_info_fire <- hour_counts_fire |> filter(n == max(n))
busiest_hour_fire <- sprintf("%02d", max_hour_info_fire |> slice(1) |> pull(Hour))
busiest_hour_count_fire <- max_hour_info_fire |> slice(1) |> pull(n)

min_hour_info_fire <- hour_counts_fire |> filter(n == min(n))
slowest_hour_fire <- sprintf("%02d", min_hour_info_fire |> slice(1) |> pull(Hour))
slowest_hour_count_fire <- min_hour_info_fire |> slice(1) |> pull(n)

barHour_AFD <- df_fire |> ggplot(aes(x = Hour, fill = Hour)) +
    geom_bar() +
  # Use precomputed red palette to avoid repeated paletteer calls
  scale_fill_manual(values = palette_red_24) +
    labs(
        title = "Number of Calls for Service for AFD by Hour of the Day",
        x = "Hour of the Day",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barHour_AFD
```

Fire-related calls are much more spread out through the day as can be seen in the graph above. However, `r busiest_hour_fire`00 hours was the busiest hour for the week There hasn't been an overall trend identified in the four weeks of this report. However, we will continue to observe the patterns to see if any trends emerge in fire related calls.

```{r}
#| label: afd-fire-reception-distribution
#| echo: false
#| fig-cap: "Number of calls for service by call reception."
# ggplot2 (precomputed in precomputed-summaries)
cr_counts_fire <- cr_counts_fire

max_cr_info_fire <- cr_counts_fire |> filter(n == max(n))
busiest_cr_fire <- max_cr_info_fire |> slice(1) |> pull(Call_Reception)
busiest_cr_count_fire <- max_cr_info_fire |> slice(1) |> pull(n)

busiest_cr_pct_fire <- round((sum(df_fire$Call_Reception == busiest_cr_fire, na.rm = TRUE) / nrow(df_fire)) * 100, 2)
cr_phone_pct_fire <- round((sum(df_fire$Call_Reception == "Phone", na.rm = TRUE) / nrow(df_fire)) * 100, 2)

barReception_AFD <- df_fire |> ggplot(aes(x = Call_Reception, fill = Call_Reception)) +
    geom_bar() +
    paletteer::scale_fill_paletteer_d("ggsci::red_material") +
    labs(
        title = "Number of Calls for Service for AFD by Call Reception",
        x = "Call Reception",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barReception_AFD
```

The majority of fire-related calls came in via `r busiest_cr_fire`. That accounts for `r busiest_cr_pct_fire` percent of all fire-related calls. However the numbers for Mutual Aid and E-911 were larger percentages of the overall volume. In this case, Phone, not necessarily E-911 represented `r cr_phone_pct_fire` percent of all fire-related service calls received.

```{r}
#| label: afd-fire-problem-distribution
#| echo: false
#| fig-cap: "Number of calls for service by call type."
# ggplot2
prob_counts_fire <- df_fire |>
  count(Problem, sort = TRUE)

max_prob_info_fire <- prob_counts_fire |> filter(n == max(n))
busiest_prob_fire <- max_prob_info_fire |> slice(1) |> pull(Problem)
busiest_prob_count_fire <- max_prob_info_fire |> slice(1) |> pull(n)

problem_counts_AFD <- df_fire |>
  count(Problem, sort = TRUE) |>
  slice_head(n = 10)

barProblem_AFD <- problem_counts_AFD |>
    ggplot(aes(x = reorder(Problem, -n), y = n, fill = Problem)) +
    geom_bar(stat = "identity") +
    paletteer::scale_fill_paletteer_d("ggsci::red_material") +
    labs(
        title = "Number of Calls for Service for AFD by Call Type",
        x = "Call Type",
        y = "Number of Calls"
    ) +
    geom_text(
        aes(label = n),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barProblem_AFD
```

The greatest number of fire-related service calls were for `r busiest_prob_fire`. That is an interesting observation and should be watched through the future.

```{r}
#| label: afd-fire-priority-distribution
#| echo: false
#| fig-cap: "Number of calls for service by priority level."
# ggplot2
# Ensure Priority_Number is parsed to numeric and ordered
df_fire_priority <- df_fire %>%
  mutate(
    Priority_Number_num = suppressWarnings(as.numeric(gsub("[^0-9]", "", as.character(Priority_Number)))),
    Priority_Number_fac = factor(
      Priority_Number_num,
      levels = sort(unique(na.omit(Priority_Number_num))),
      ordered = TRUE
    )
  )

pri_counts_fire <- df_fire_priority |>
  filter(!is.na(Priority_Number_fac)) |>
  count(Priority_Number_fac, sort = FALSE)
max_pri_info_fire <- pri_counts_fire |> filter(n == max(n))
busiest_pri_fire <- max_pri_info_fire |> slice(1) |> pull(Priority_Number_fac)
busiest_pri_fire_num <- suppressWarnings(as.numeric(as.character(busiest_pri_fire)))
busiest_pri_count_fire <- max_pri_info_fire |> slice(1) |> pull(n)

# Calculate percentage for AFD-Fire priority calls
busiest_pri_fire_percentage <- round((sum(df_fire_priority$Priority_Number_num == busiest_pri_fire_num, na.rm = TRUE) / nrow(df_fire_priority)) * 100, 1)

# Calculate percentage for P1 calls
p1_fire_percentage <- round((sum(df_fire_priority$Priority_Number_num == 1, na.rm = TRUE) / nrow(df_fire_priority)) * 100, 1)

barPriority_AFD <- df_fire_priority |> ggplot(aes(x = Priority_Number_fac, fill = Priority_Number_fac)) +
    geom_bar() +
    paletteer::scale_fill_paletteer_d("ggsci::red_material") +
    labs(
        title = "Number of Calls for Service by Priority Level",
        x = "Priority Level",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barPriority_AFD
```

The most-used priority for fire-related calls was P`r busiest_pri_fire`. P1 calls account for `r p1_fire_percentage` percent of all fire-related calls this week.

```{r}
#| label: afd-custom-summary
#| echo: false
#| message: false
#| warning: false

# Initialize variables
summary_table <- NULL
median_processing_time <- NA_real_
median_phone_time <- NA_real_
mean_phone_time <- NA_real_

# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time",
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_fire)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- df_fire %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

    # Extract key metrics for use in text
    median_time_to_queue <- summary_table |>
      filter(Variable == "Time To Queue") |>
      pull(Median)

    median_time_to_dispatch <- summary_table |>
      filter(Variable == "Time To Dispatch") |>
      pull(Median)
    median_time_to_dispatch_afd <- median_time_to_dispatch

    median_processing_time <- summary_table |>
      filter(Variable == "Processing_Time") |>
      pull(Median)
    median_processing_time_afd <- median_processing_time

    median_phone_time <- summary_table |>
      filter(Variable == "Phone_Time") |>
      pull(Median)
    median_phone_time_afd <- median_phone_time

    mean_phone_time <- summary_table |>
      filter(Variable == "Phone_Time") |>
      pull(Mean)
    mean_phone_time_afd <- mean_phone_time

    fire_hp_calls <- df_fire |>
      dplyr::filter(Priority_Number <= 2)

    fire_64_calls <- df_fire |>
      dplyr::filter(Priority_Number <= 2 & Time_To_Dispatch <= 64)

    fire_106_calls <- df_fire |>
      dplyr::filter(Priority_Number <= 2 & Time_To_Dispatch <= 106)

    fire_64_compliance_pct <- round((nrow(fire_64_calls) / nrow(fire_hp_calls)) * 100, 3)
    fire_106_compliance_pct <- round((nrow(fire_106_calls) / nrow(fire_hp_calls)) * 100, 3)

    to_ft(
      summary_table %>% dplyr::select(Variable, Minimum, Mean, Median),
      caption = "Weekly Elapsed Time Summary Table — Statistical summary of call processing times",
      header_map = list(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median"
      ),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})
```

Overall, DECC operations appear to be very efficient at getting fire-related service calls out to the field. The median processing time was only `r median_processing_time` seconds. This shows that we can easily be in compliance with all necessary NENA and NFPA guidelines. The median time on the phone was `r median_phone_time` seconds. The mean time was `r mean_phone_time` seconds, which is still amazing.

| Calls dispatched in 64 seconds | Calls dispatched in 106 seconds | P1 & P2 Calls | Pct in 64 sec. | Pct in 106 sec. |
|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|
| `r nrow(fire_64_calls)` | `r nrow(fire_106_calls)` | `r nrow(fire_hp_calls)` | `r fire_64_compliance_pct`% | `r fire_106_compliance_pct`% |

: SLA Compliance for AFD Fire Calls (P1 & P2) {.striped .hover}

These numbers show that we are meeting and exceeding our SLAs with the AFD.

### AFD EMS Analyses

Because AFD calls for service can be split into two distinct disciplines, fire-related calls and medical-related calls, DECC has, historically, separated the two disciplines for analytical purposes. So this section will look at medical-related calls for service for the week.

```{r}
#| label: afd-ems-dow-distribution
#| echo: false
#| fig-cap: "Number of calls for service by day of the week."
# ggplot2 (precomputed in precomputed-summaries)
dow_counts_ems <- dow_counts_ems

max_dow_info_ems <- dow_counts_ems |> filter(n == max(n))
busiest_day_abbr_ems <- max_dow_info_ems |> slice(1) |> pull(DOW)
busiest_day_count_ems <- max_dow_info_ems |> slice(1) |> pull(n)

min_dow_info_ems <- dow_counts_ems |> filter(n == min(n))
slowest_day_abbr_ems <- min_dow_info_ems |> slice(1) |> pull(DOW)
slowest_day_count_ems <- min_dow_info_ems |> slice(1) |> pull(n)

# Create mapping from abbreviations to full day names
day_names <- c(
  "SUN" = "Sunday",
  "MON" = "Monday",
  "TUE" = "Tuesday",
  "WED" = "Wednesday",
  "THU" = "Thursday",
  "FRI" = "Friday",
  "SAT" = "Saturday"
)

# Convert abbreviations to full day names
busiest_day_ems <- day_names[busiest_day_abbr_ems]
slowest_day_ems <- day_names[slowest_day_abbr_ems]

barDOW_EMS <- df_ems |> ggplot(aes(x = DOW, fill = DOW)) +
    geom_bar() +
    paletteer::scale_fill_paletteer_d("ggsci::green_material") +
    labs(
        title = "Number of Calls for Service for EMS by Day of the Week",
        x = "Day of the Week",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barDOW_EMS
```

 This week, there is a spike in medical calls on `r busiest_day_ems`. This appears to correlate to the information that we saw earlier in the report. Outside of `r busiest_day_ems`, the remainder of the week appears to be consistent for the number of medical calls handled.

```{r}
#| label: afd-ems-hour-distribution
#| echo: false
#| fig-cap: "Number of calls for service by hour of the day."
# ggplot2 (precomputed in precomputed-summaries)
hour_counts_ems <- hour_counts_ems

max_hour_info_ems <- hour_counts_ems |> filter(n == max(n))
busiest_hour_ems <- sprintf("%02d", max_hour_info_ems |> slice(1) |> pull(Hour))
busiest_hour_count_ems <- max_hour_info_ems |> slice(1) |> pull(n)

min_hour_info_ems <- hour_counts_ems |> filter(n == min(n))
slowest_hour_ems <- sprintf("%02d", min_hour_info_ems |> slice(1) |> pull(Hour))
slowest_hour_count_ems <- min_hour_info_ems |> slice(1) |> pull(n)

barHour_EMS <- df_ems |> ggplot(aes(x = Hour, fill = Hour)) +
    geom_bar() +
    {
        ems_base_cols <- tryCatch({
            as.character(paletteer::paletteer_d("cartography::green.pal"))
        }, error = function(e) {
            c("#00441B", "#006D2C", "#238B45", "#41AE76", "#66C2A4", "#99D8C9", "#C7E9C0", "#E5F5E0")
        })
        scale_fill_manual(values = grDevices::colorRampPalette(ems_base_cols)(nlevels(df_ems$Hour)))
    } +
    labs(
        title = "Number of Calls for Service for EMS by Hour of the Day",
        x = "Hour of the Day",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barHour_EMS
```

This week, the busiest hour was `r busiest_hour_ems` hours. The afternoon to evening hours, this week, stayed consistently busy which appears to continue the trends previously observed.

```{r}
#| label: afd-ems-reception-distribution
#| echo: false
#| fig-cap: "Number of calls for service by call reception."
# ggplot2 (precomputed in precomputed-summaries)
cr_counts_ems <- cr_counts_ems

max_cr_info_ems <- cr_counts_ems |> filter(n == max(n))
busiest_cr_ems <- max_cr_info_ems |> slice(1) |> pull(Call_Reception)
busiest_cr_count_ems <- max_cr_info_ems |> slice(1) |> pull(n)

cr_nr_pct_ems <- round((sum(df_ems$Call_Reception == "NOT CAPTURED") / nrow(df_ems)) * 100, 1)

barReception_EMS <- df_ems |> ggplot(aes(x = Call_Reception, fill = Call_Reception)) +
    geom_bar() +
    paletteer::scale_fill_paletteer_d("ggsci::green_material") +
    labs(
        title = "Number of Calls for Service for EMS by Call Reception",
        x = "Call Reception",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barReception_EMS
```

As expected, the vast majority of medical calls arrived via `r busiest_cr_ems`. However, `r cr_nr_pct_ems` percent of medical calls arrived without a method by which we recevied the call. We should continue to monitor and investigate why these are occurring.

```{r}
#| label: afd-ems-problem-distribution
#| echo: false
#| fig-cap: "Number of calls for service by call type."
# ggplot2
prob_counts_med <- df_ems |>
  count(Problem, sort = TRUE)

max_prob_info_med <- prob_counts_med |> filter(n == max(n))
busiest_prob_med <- max_prob_info_med |> slice(1) |> pull(Problem)
busiest_prob_count_med <- max_prob_info_med |> slice(1) |> pull(n)

ems_ma_call <- sum(startsWith(df_ems$Problem, "MUTUAL"), na.rm = TRUE)

problem_counts_EMS <- df_ems |>
  count(Problem, sort = TRUE) |>
  slice_head(n = 10)

barProblem_EMS <- problem_counts_EMS |>
    ggplot(aes(x = reorder(Problem, -n), y = n, fill = Problem)) +
    geom_bar(stat = "identity") +
    paletteer::scale_fill_paletteer_d("ggsci::green_material") +
    labs(
        title = "Number of Calls for Service for EMS by Call Type",
        x = "Call Type",
        y = "Number of Calls"
    ) +
    geom_text(
        aes(label = n),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barProblem_EMS
```

The most prevalent medical service type this week was `r busiest_prob_med`. Further we had `r ems_ma_call` mutual aid medical calls this week.

```{r}
#| label: afd-ems-priority-distribution
#| echo: false
#| fig-cap: "Number of calls for service by priority level."
# ggplot2
# Ensure Priority_Number is parsed to numeric and ordered
df_ems_priority <- df_ems %>%
  mutate(
    Priority_Number_num = suppressWarnings(as.numeric(gsub("[^0-9]", "", as.character(Priority_Number)))),
    Priority_Number_fac = factor(
      Priority_Number_num,
      levels = sort(unique(na.omit(Priority_Number_num))),
      ordered = TRUE
    )
  )

pri_counts_ems <- df_ems_priority |>
  filter(!is.na(Priority_Number_fac)) |>
  count(Priority_Number_fac, sort = FALSE)
max_pri_info_ems <- pri_counts_ems |> filter(n == max(n))
busiest_pri_ems <- max_pri_info_ems |> slice(1) |> pull(Priority_Number_fac)
busiest_pri_ems_num <- suppressWarnings(as.numeric(as.character(busiest_pri_ems)))
busiest_pri_count_ems <- max_pri_info_ems |> slice(1) |> pull(n)

# Calculate percentage for AFD-EMS priority calls
busiest_pri_ems_percentage <- round((sum(df_ems_priority$Priority_Number_num == busiest_pri_ems_num, na.rm = TRUE) / nrow(df_ems_priority)) * 100, 1)

# Calculate percentage for P1 calls
p1_ems_percentage <- round((sum(df_ems_priority$Priority_Number_num == 1, na.rm = TRUE) / nrow(df_ems_priority)) * 100, 1)

barPriority_EMS <- df_ems_priority |> ggplot(aes(x = Priority_Number_fac, fill = Priority_Number_fac)) +
    geom_bar() +
    paletteer::scale_fill_paletteer_d("ggsci::green_material") +
    labs(
        title = "Number of Calls for Service by Priority Level",
        x = "Priority Level",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )


barPriority_EMS
```

 The majority of medical service calls are P`r busiest_pri_ems`, which is to be expected.

```{r}
#| label: ems-custom-summary
#| echo: false
#| message: false
#| warning: false

# Initialize variables
summary_table <- NULL
median_processing_time <- NA_real_
median_phone_time <- NA_real_

# Create a summary table of elapsed time variables
# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time",
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_ems)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- df_ems %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

    # Extract key metrics for use in text
    median_time_to_queue <- summary_table |>
      filter(Variable == "Time To Queue") |>
      pull(Median)
    median_time_to_queue_ems <- median_time_to_queue

    median_time_to_dispatch <- summary_table |>
      filter(Variable == "Time To Dispatch") |>
      pull(Median)
    median_time_to_dispatch_ems <- median_time_to_dispatch

    median_processing_time <- summary_table |>
      filter(Variable == "Processing_Time") |>
      pull(Median)
    median_processing_time_ems <- median_processing_time

    median_phone_time <- summary_table |>
      filter(Variable == "Phone_Time") |>
      pull(Median)
    median_phone_time_ems <- median_phone_time

    ems_hp_calls <- df_ems |>
      dplyr::filter(Priority_Number <= 2)

    ems_64_calls <- df_ems |>
      dplyr::filter(Priority_Number <= 2 & Processing_Time <= 64)

    ems_106_calls <- df_ems |>
      dplyr::filter(Priority_Number <= 2 & Processing_Time <= 106)

    ems_64_compliance_pct <- round((nrow(ems_64_calls) / nrow(ems_hp_calls)) * 100, 1)
    ems_106_compliance_pct <- round((nrow(ems_106_calls) / nrow(ems_hp_calls)) * 100, 1)

    to_ft(
      summary_table %>% dplyr::select(Variable, Minimum, Mean, Median),
      caption = "Weekly Elapsed Time Summary Table — Statistical summary of call processing times",
      header_map = list(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median"
      ),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})
```

The median time to process medical calls was `r median_processing_time_ems` seconds. Again, this puts us in good form when examening our operational efficiency. The median time on phones, `r median_phone_time_ems` seconds, is longer than the overall median. That is to be expected with these calls taking longer to triage.

| Calls dispatched in 64 seconds | Calls dispatched in 106 seconds | P1 & P2 Calls | Pct in 64 sec. | Pct in 106 sec. |
|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|
| `r nrow(ems_64_calls)` | `r nrow(ems_106_calls)` | `r nrow(ems_hp_calls)` | `r ems_64_compliance_pct`% | `r ems_106_compliance_pct`% |
: SLA Compliance for AFD Medical Calls (P1 & P2) {.striped .hover}

These numbers show that there is some room for improvement in meeting the SLAs for AFD medical calls. Like the APD SLA analysis, the change in numbers can be explained by the removal of the filter for calls that remained in queue over 30 minutes. Originally, this filter was implemented to address calls that appeared to be defective, but this may not be the case and further investigation is merited.

## Additional Analyses

Earlier, for this report, we created some additional datasets that we can investigate in the course of our analysis. The first two are lists of calls where the elapsed time prior to release to queue or the time spent in dispatch is greater than 60 seconds for *emergency* calls. For the first, there are `r nrow(df_ttq_delay)` emergency service calls where the elapsed time from call start to the call entering the queue for dispatch was over 60 seconds. There are also `r nrow(df_ttd_delay)` emergency service calls where the elapsed time from entering queue to the first unit assigned was over 60 seconds.

### Possible Service Delays

We can look at the datasets and see if there are telecommunicators who may experience more challenging calls during the week. First will be a table of telecommunicators who worked emergency calls that took longer than 60 seconds to go from start to queue. The second will be a table of dispatchers who assigned an emergency call that waited in queue longer than 60 seconds.

```{r}
#| label: call-taker-delay-frequency
#| echo: false
#| message: false
#| warning: false

# Table: Call_Taker frequency in df_ttq_delay (descending)
library(dplyr)
library(knitr)

call_taker_counts <- df_ttq_delay %>%
  count(Call_Taker, sort = TRUE)

to_ft(
  call_taker_counts,
  caption = "Frequency of Call Taker in Delayed TTQ Calls (Descending)",
  header_map = list(Call_Taker = "Call Taker", n = "Count"),
  digits = 0
)
```

From this, since there are a small number of telecommunicators who have more than one call in the table above, there may not be any need for amerlioration. This, however, could be something that is included in the report template in order to monitor. Should a telecommunicator appear multiple times in this table over a period of time, additional training or mentoring may be called for.

```{r}
#| label: dispatcher-delay-frequency
#| echo: false
#| message: false
#| warning: false

# Table: Dispatcher frequency in df_ttd_delay (descending)
dispatcher_counts <- df_ttd_delay %>%
  count(Dispatcher, sort = TRUE)

to_ft(
  dispatcher_counts,
  caption = "Frequency of Dispatcher in Delayed TTD Calls (Descending)",
  header_map = list(Dispatcher = "Dispatcher", n = "Count"),
  digits = 0
)
```

This list is fairly short and could simply be monitored in future should the need arise.

## High-Priority and Critical Calls

In this section, we will focus on the calls that are deemed high-priority or critical. This includes APD Priority 1 calls and AFD Priority 1 and 2 calls. We have identified these calls in the `df_hp` dataset created earlier.

### High-Priority Call Types

```{r}
#| label: high-priority-call-types
#| echo: false
#| fig-cap: "Top High-Priority Call Types"
# ggplot2
hp_call_types <- df_hp |>
  count(Problem, sort = TRUE) |>
  slice_head(n = 10)

ggplot(hp_call_types, aes(x = reorder(Problem, n), y = n, fill = Problem)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  paletteer::scale_fill_paletteer_d("ggsci::deep_orange_material") +
  labs(title = "Top High-Priority Call Types",
       x = "Call Type",
       y = "Number of Calls") +
  geom_text(
    aes(label = n),
    hjust = -0.2,
    size = 3
  ) +
  theme_minimal() +
  theme(
    legend.position = "none",
    plot.title = element_text(hjust = 0.5, size = 12),
    axis.text.x = element_text(size = 9),
    axis.text.y = element_text(size = 9),
    axis.title.x = element_text(size = 11),
    axis.title.y = element_text(size = 11)
  )
```

Almost all of the problem types in this graph belong to AFD and are medical calls. Based on the information above, this is to be expected.

### High-Priority Response Times

```{r}
#| label: hp-call-summary
#| echo: false
#| message: false
#| warning: false

# Initialize variables
summary_table <- NULL
median_processing_time <- NA_real_
median_phone_time <- NA_real_

# Create a summary table of elapsed time variables
# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time",
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_hp)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- df_hp %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

    # Extract key metrics for use in text
  median_time_to_queue <- summary_table |>
  filter(Variable == "Time To Queue") |>
      pull(Median)

  median_time_to_dispatch <- summary_table |>
  filter(Variable == "Time To Dispatch") |>
      pull(Median)

    median_processing_time <- summary_table |>
      filter(Variable == "Processing_Time") |>
      pull(Median)

    median_phone_time <- summary_table |>
      filter(Variable == "Phone_Time") |>
      pull(Median)


    # Always assign these for inline use
    median_phone_time_hp <- tryCatch({
      summary_table |> filter(Variable == "Phone Time") |> pull(Median)
    }, error = function(e) NA_real_)

    median_processing_time_hp <- tryCatch({
      summary_table |> filter(Variable == "Processing Time") |> pull(Median)
    }, error = function(e) NA_real_)

    to_ft(
      summary_table,
      caption = "Weekly Elapsed Time Summary Table — Statistical summary of high priority call processing times",
      header_map = list(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median",
        Std_Dev = "Std Dev",
        Skewness = "Skew",
        Kurtosis = "Kurt"
      ),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
  median_phone_time_hp <- NA_real_
  median_processing_time_hp <- NA_real_
})

```

## E-911 Service Call Analyses

This section will analyze calls that arrived to DECC via E-911 trunk lines. Last week, the median processing time for high-priority calls was `r median_processing_time_hp` seconds.

### E-911 Call Response Summary

```{r}
#| label: e911-calls
#| echo: false
#| message: false
#| warning: false

# Initialize variables
summary_table <- NULL
median_processing_time <- NA_real_
median_phone_time <- NA_real_

# Create a summary table of elapsed time variables
# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time",
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_911)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- df_911 %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

    # Extract key metrics for use in text
  median_time_to_queue <- summary_table |>
  filter(Variable == "Time To Queue") |>
      pull(Median)

  median_time_to_dispatch <- summary_table |>
  filter(Variable == "Time To Dispatch") |>
      pull(Median)

    median_processing_time <- summary_table |>
      filter(Variable == "Processing_Time") |>
      pull(Median)

    median_phone_time <- summary_table |>
      filter(Variable == "Phone_Time") |>
      pull(Median)

    to_ft(
      summary_table,
      caption = "Weekly Elapsed Time Summary Table — Statistical summary of 9-1-1 call processing times",
      header_map = list(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median",
        Std_Dev = "Std Dev",
        Skewness = "Skew",
        Kurtosis = "Kurt"
      ),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})

    # Always assign these for inline use
    median_phone_time_911 <- tryCatch({
      summary_table |> filter(Variable == "Phone Time") |> pull(Median)
    }, error = function(e) NA_real_)

    median_processing_time_911 <- tryCatch({
      summary_table |> filter(Variable == "Processing Time") |> pull(Median)
    }, error = function(e) NA_real_)
```

The table above shows the information about the handling of calls that arrived by 9-1-1 trunk lines. The median time to process those calls for dispatch was `r median_processing_time_911` seconds.

```{r}
#| label: restore-overall-medians
#| echo: false
# If overall aliases exist, restore the generic median variables to overall values
if (exists("median_time_to_queue_overall")) median_time_to_queue <- median_time_to_queue_overall
if (exists("median_time_to_dispatch_overall")) median_time_to_dispatch <- median_time_to_dispatch_overall
if (exists("median_processing_time_overall")) median_processing_time <- median_processing_time_overall
if (exists("median_phone_time_overall")) median_phone_time <- median_phone_time_overall
```

### E-911 Call Breakdowns

```{r}
#| label: e911-dow-chart
#| echo: false
#| fig-cap: "E-911 Call Volume by Day of the Week"
# ggplot2 (precomputed in precomputed-summaries)

max_dow_info_911 <- dow_counts_911 |> filter(n == max(n))
busiest_day_abbr_911 <- max_dow_info_911 |> slice(1) |> pull(DOW)
busiest_day_count_911 <- max_dow_info_911 |> slice(1) |> pull(n)

min_dow_info_911 <- dow_counts_911 |> filter(n == min(n))
slowest_day_abbr_911 <- min_dow_info_911 |> slice(1) |> pull(DOW)
slowest_day_count_911 <- min_dow_info_911 |> slice(1) |> pull(n)

# Create mapping from abbreviations to full day names
day_names <- c(
  "SUN" = "Sunday",
  "MON" = "Monday",
  "TUE" = "Tuesday",
  "WED" = "Wednesday",
  "THU" = "Thursday",
  "FRI" = "Friday",
  "SAT" = "Saturday"
)

# Convert abbreviations to full day names
busiest_day_911 <- day_names[busiest_day_abbr_911]
slowest_day_911 <- day_names[slowest_day_abbr_911]

barDOW_911 <- df_911 |> ggplot(aes(x = DOW, fill = DOW)) +
    geom_bar() +
    paletteer::scale_fill_paletteer_d("ggsci::indigo_material") +
    labs(
        title = "Number of Calls for Service for 911 cals by Day of the Week",
        x = "Day of the Week",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barDOW_911
```

As can be seen `r busiest_day_911` was the busiest day for E-911 calls this week. This is inline with the busiest day of the week for all calls to the center.

```{r}
#| label: e911-hour-chart
#| echo: false
#| fig-cap: "E-911 Call Volume by Hour of the Day"
#| warning: false
# ggplot2 (precomputed in precomputed-summaries)

hour_counts_911 <- hour_counts_911

max_hour_info_911 <- hour_counts_911 |> filter(n == max(n))
busiest_hour_numeric_911 <- max_hour_info_911 |>
  slice(1) |>
  pull(Hour) |>
  as.character() |>
  as.numeric()
busiest_hour_911 <- sprintf("%02d", busiest_hour_numeric_911)
busiest_hour_count_911 <- max_hour_info_911 |> slice(1) |> pull(n)

min_hour_info_911 <- hour_counts_911 |> filter(n == min(n))
slowest_hour_numeric_911 <- min_hour_info_911 |>
  slice(1) |>
  pull(Hour) |>
  as.character() |>
  as.numeric()
slowest_hour_911 <- sprintf("%02d", slowest_hour_numeric_911)
slowest_hour_count_911 <- min_hour_info_911 |> slice(1) |> pull(n)

barHour <- df_911 |> ggplot(aes(x=Hour, fill=Hour)) +
  geom_bar() +
  # Use ggsci::indigo_material palette with robust fallback
  {
    indigo_material_cols <- tryCatch(
      {
        as.character(paletteer::paletteer_d("ggsci::indigo_material"))
      },
      error = function(e) {
        # Fallback indigo colors if palette not available
        c("#1A237E", "#283593", "#3949AB", "#3F51B5", "#5C6BC0", "#7986CB", "#9FA8DA", "#C5CAE9")
      }
    )
    scale_fill_manual(values = grDevices::colorRampPalette(indigo_material_cols)(nlevels(df_911$Hour)))
  } +
  labs(title="Number of Calls for Service by Hour of the Day",
       x="Hour of the Day",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size = 3.5
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barHour
```

This shows that while the overall pattern for 9-1-1 calls is in keeping with the general weekly trend. However, the spike at `r busiest_hour_911`00 hours stands out. Was there a significant call for service during that hour that generated a lot of calls on the 9-1-1 lines for the floor?

### Non 9-1-1 Call Breakdowns

Additionally, we've been asked to report on calls that were received by other means than the 9-1-1 trunk lines. These will include calls arriving on the administrative trunk lines, officer generated call, walk-in calls at 3600 Wheeler, etc. There is an interest in paying attention to these calls to identify trends and determine if there are operational; improvement opportunities present in handling these non-9-1-1 calls.

```{r}
#| label: non-e911-calls
#| echo: false
#| message: false
#| warning: false

# Initialize variables
summary_table <- NULL
median_processing_time_non911 <- NA_real_
median_phone_time_non911 <- NA_real_

# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time",
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_non_e)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <<- df_non_e %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

    # Extract key metrics for use in text (use <<- to assign to parent environment)
    median_time_to_queue_non911 <<- summary_table |>
      filter(Variable == "Time To Queue") |>
      pull(Median)

    median_time_to_dispatch_non911 <<- summary_table |>
      filter(Variable == "Time To Dispatch") |>
      pull(Median)

    median_processing_time_non911 <<- summary_table |>
      filter(Variable == "Processing Time") |>
      pull(Median)

    median_phone_time_non911 <<- summary_table |>
      filter(Variable == "Phone Time") |>
      pull(Median)

    to_ft(
      summary_table,
      caption = "Weekly Elapsed Time Summary Table — Statistical summary of non E-911 call processing times",
      header_map = list(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median",
        Std_Dev = "Std Dev",
        Skewness = "Skew",
        Kurtosis = "Kurt"
      ),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df_non_e:", paste(names(df_non_e), collapse = ", "), "\n")
})
```

It is interesting to note that the median processing time, `r median_processing_time_non911` seconds, is not that much longer than the 9-1-1 median processing time. Also interesting to note is that the median time on the phones, `r median_phone_time_non911` seconds is less than the same for E-911 calls. This is likely because the calls coming in on the E-911 lines will require more triage and support during the call's lifespan.

```{r}
#| label: non-911-dow-chart
#| echo: false
#| fig-cap: "Non E-911 Call Volume by Day of the Week"
# ggplot2 (precomputed in precomputed-summaries)

max_dow_info_non911 <- dow_counts_non911 |> filter(n == max(n))
busiest_day_abbr_non911 <- max_dow_info_non911 |> slice(1) |> pull(DOW)
busiest_day_count_non911 <- max_dow_info_non911 |> slice(1) |> pull(n)

min_dow_info_non911 <- dow_counts_non911 |> filter(n == min(n))
slowest_day_abbr_non911 <- min_dow_info_non911 |> slice(1) |> pull(DOW)
slowest_day_count_non911 <- min_dow_info_non911 |> slice(1) |> pull(n)

# Create mapping from abbreviations to full day names
day_names <- c(
  "SUN" = "Sunday",
  "MON" = "Monday",
  "TUE" = "Tuesday",
  "WED" = "Wednesday",
  "THU" = "Thursday",
  "FRI" = "Friday",
  "SAT" = "Saturday"
)

# Convert abbreviations to full day names
busiest_day_non911 <- day_names[busiest_day_abbr_non911]
slowest_day_non911 <- day_names[slowest_day_abbr_non911]

barDOW_non911 <- df_non_e |> ggplot(aes(x = DOW, fill = DOW)) +
    geom_bar() +
    paletteer::scale_fill_paletteer_d("ggsci::indigo_material") +
    labs(
        title = "Number of Calls for Service for Non-911 cals by Day of the Week",
        x = "Day of the Week",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barDOW_911
```

As can be seen `r busiest_day_non911` was the busiest day for Non-E-911 calls this week. This is inline with the busiest day of the week for all calls to the center.

```{r}
#| label: non-911-hour-chart
#| echo: false
#| fig-cap: "Non-E-911 Call Volume by Hour of the Day"
#| warning: false
# ggplot2 (precomputed in precomputed-summaries)

hour_counts_non911 <- hour_counts_non911

max_hour_info_non911 <- hour_counts_non911 |> filter(n == max(n))
busiest_hour_numeric_non911 <- max_hour_info_non911 |>
  slice(1) |>
  pull(Hour) |>
  as.character() |>
  as.numeric()
busiest_hour_non911 <- sprintf("%02d", busiest_hour_numeric_non911)
busiest_hour_count_non911 <- max_hour_info_non911 |> slice(1) |> pull(n)

min_hour_info_non911 <- hour_counts_non911 |> filter(n == min(n))
slowest_hour_numeric_non911 <- min_hour_info_non911 |>
  slice(1) |>
  pull(Hour) |>
  as.character() |>
  as.numeric()
slowest_hour_non911 <- sprintf("%02d", slowest_hour_numeric_non911)
slowest_hour_count_non911 <- min_hour_info_non911 |> slice(1) |> pull(n)

barHour <- df_non_e |> ggplot(aes(x=Hour, fill=Hour)) +
  geom_bar() +
  # Use ggsci::indigo_material palette with robust fallback
  {
    indigo_material_cols <- tryCatch(
      {
        as.character(paletteer::paletteer_d("ggsci::indigo_material"))
      },
      error = function(e) {
        # Fallback indigo colors if palette not available
        c("#1A237E", "#283593", "#3949AB", "#3F51B5", "#5C6BC0", "#7986CB", "#9FA8DA", "#C5CAE9")
      }
    )
    scale_fill_manual(values = grDevices::colorRampPalette(indigo_material_cols)(nlevels(df_non_e$Hour)))
  } +
  labs(title="Number of Calls for Service by Hour of the Day",
       x="Hour of the Day",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size = 3.5
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barHour
```

## Topical Analyses

These specific analyses have been included regardless of the emergency status of the call. They include types of calls that we have been requested to monitor and looking at outlier calls for any existing patterns.

### Cardiac Arrest Calls Analysis

Finally, we will look into the specific subset of calls that are related to cardiac arrests. These calls have been identified in the `df_ca` dataset.

```{r}
#| label: cardiac-arrest-dow
#| echo: false
#| fig-cap: "Cardiac Arrest Call Volume by Day and Hour"
# ggplot2
barDOW_CA <- df_ca |> ggplot(aes(x=DOW, fill=DOW)) +
  geom_bar() +
  paletteer::scale_fill_paletteer_d("ggsci::light_green_material") +
  labs(title="Number of Calls for Service for Cardiac Arrest by Day of the Week",
       x="Day of the Week",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size = 3.5
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barDOW_CA
```

As we can see, with a very limited number of cardiac arrest calls for the week.

```{r}
#| label: cardiac-arrest-response-times
#| echo: false
#| fig-cap: "Cardiac Arrest Call Response Times"
#| warning: false
#| message: false
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time",
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_ca)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- df_ca %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

     #Extract key metrics for use in text
  median_time_to_queue <- summary_table |>
  filter(Variable == "Time To Queue") |>
      pull(Median)

  median_time_to_dispatch <- summary_table |>
  filter(Variable == "Time To Dispatch") |>
      pull(Median)

  median_processing_time <- summary_table |>
      filter(Variable == "Processing_Time") |>
      pull(Median)

  median_phone_time <- summary_table |>
      filter(Variable == "Phone_Time") |>
      pull(Median)

    to_ft(
      summary_table,
      caption = "Weekly Elapsed Time Summary Table — Statistical summary of call processing times",
      header_map = list(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median",
        Std_Dev = "Std Dev",
        Skewness = "Skew",
        Kurtosis = "Kurtosis"
      ),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})

    # Always assign these for inline use
    median_phone_time_ca <- tryCatch({
      summary_table |> filter(Variable == "Phone Time") |> pull(Median)
    }, error = function(e) NA_real_)

    median_processing_time_ca <- tryCatch({
      summary_table |> filter(Variable == "Processing Time") |> pull(Median)
    }, error = function(e) NA_real_)
```

However, we can see that the median time to process a cardiac arrest and get the units rolling is about `r median_processing_time_ca` seconds. The median time that we are on the phone is significantly longer, `r median_phone_time_ca` seconds. That is to be expected since the calltaker is likely giving T-CPR instructions while the units are en route.

### Mental Health Analyses

With the advent of Marcus' Law in Virginia, there has been an emphasis on how mental health calls are processed and serviced. The following analyses will focus on the mental health calls that have been defined as such after consultation with DCHS.

```{r}
#| label: mental-health-narrative-insights
#| echo: false
#| results: asis

# Comprehensive mental health analysis
if (exists("df_mh") && nrow(df_mh) > 0) {
  mh_stats <- df_mh %>%
    summarise(
      total_mh = n(),
      median_phone = median(Phone_Time, na.rm = TRUE),
      median_ttd = median(Time_To_Dispatch, na.rm = TRUE),
      p1_count = sum(Priority_Number == "1", na.rm = TRUE),
      .groups = 'drop'
    )
  
  cat("\n**Mental Health Call Insights:**\n\n")
  
  cat(paste0("This week saw ", mh_stats$total_mh, " mental health-related calls, representing ",
             round(mh_stats$total_mh / nrow(df) * 100, 1), "% of total call volume. "))
  
  if (!is.na(mh_stats$median_phone) && mh_stats$median_phone > 180) {
    cat(paste0("Mental health calls required significantly longer call-taking time (median ",
               round(mh_stats$median_phone, 1), " seconds), reflecting the complexity and care needed ",
               "for crisis assessment. "))
  }
  
  if (mh_stats$p1_count > 0) {
    cat(paste0("Notably, ", mh_stats$p1_count, " mental health calls were classified as Priority 1, ",
               "indicating acute crisis situations requiring immediate response. "))
  }
  
  cat("\n\nThese patterns underscore the critical role of dispatch centers in mental health crisis response and the need for specialized training and protocols.\n\n")
}
```

```{r}
#| label: mental-health-dow
#| echo: false
#| fig-cap: "Number of calls for service by day of the week."
# ggplot2 (precomputed in precomputed-summaries)
dow_counts_mh <- dow_counts_mh

max_dow_info_mh <- dow_counts_mh |> filter(n == max(n))
busiest_day_abbr_mh <- max_dow_info_mh |> slice(1) |> pull(DOW)
busiest_day_count_mh <- max_dow_info_mh |> slice(1) |> pull(n)

min_dow_info_mh <- dow_counts_mh |> filter(n == min(n))
slowest_day_abbr_mh <- min_dow_info_mh |> slice(1) |> pull(DOW)
slowest_day_count_mh <- min_dow_info_mh |> slice(1) |> pull(n)

# Create mapping from abbreviations to full day names
day_names <- c(
  "SUN" = "Sunday",
  "MON" = "Monday",
  "TUE" = "Tuesday",
  "WED" = "Wednesday",
  "THU" = "Thursday",
  "FRI" = "Friday",
  "SAT" = "Saturday"
)

# Convert abbreviations to full day names
busiest_day_mh <- day_names[busiest_day_abbr_mh]
slowest_day_mh <- day_names[slowest_day_abbr_mh]

barDOW_MH <- df_mh |> ggplot(aes(x=DOW, fill=DOW)) +
  geom_bar() +
  paletteer::scale_fill_paletteer_d("ggsci::purple_material") +
  labs(title="Number of Mental Health related calls by Day of the Week",
       x="Day of the Week",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size = 3.5
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barDOW_MH
```

The busiest day of the week for mental health calls was `r busiest_day_mh` with `r busiest_day_count_mh` service calls.

```{r}
#| label: mental-health-hour-distribution
#| echo: false
#| fig-cap: "Number of calls for service by hour of the day."
# ggplot2 (precomputed in precomputed-summaries)
hour_counts_mh <- hour_counts_mh

max_hour_info_mh <- hour_counts_mh |> filter(n == max(n))
busiest_day_mh <- max_hour_info_mh |> slice(1) |> pull(Hour)
busiest_day_count_mh <- max_hour_info_mh |> slice(1) |> pull(n)

min_hour_info_mh <- hour_counts_mh |> filter(n == min(n))
slowest_day_mh <- min_hour_info_mh |> slice(1) |> pull(Hour)
slowest_day_count_mh <- min_hour_info_mh |> slice(1) |> pull(n)

barHour_MH <- df_mh |> ggplot(aes(x=Hour, fill=Hour)) +
  geom_bar() +
  # Use ggsci::purple_material palette scaled to 24 levels for hours
  {
    purple_material_cols <- tryCatch(
      {
        as.character(paletteer::paletteer_d("ggsci::purple_material"))
      },
      error = function(e) {
        # Fallback purple colors if palette not available
        c("#F3E5F5", "#E1BEE7", "#CE93D8", "#BA68C8", "#AB47BC", "#9C27B0", "#8E24AA", "#7B1FA2", "#6A1B9A", "#4A148C")
      }
    )
    scale_fill_manual(values = grDevices::colorRampPalette(purple_material_cols)(24))
  } +
  labs(title="Number of Calls for Service for APD by Hour of the Day",
       x="Hour of the Day",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size = 3.5
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barHour_MH
```

Most of these calls arrived, for this past week, in the late mornings through evenings. Again, should this data prove to be part of a trend, then we should adjust the availability of repsonders to address the community's needs.

```{r}
#| label: mental-health-reception
#| echo: false
#| fig-cap: "Number of calls for service by call reception."
# ggplot2
cr_counts_mh <- df_mh |>
  count(Call_Reception, sort = TRUE)

max_cr_info_mh <- cr_counts_mh |> filter(n == max(n))
busiest_cr_mh <- max_cr_info_mh |> slice(1) |> pull(Call_Reception)
busiest_cr_count_mh <- max_cr_info_mh |> slice(1) |> pull(n)

min_cr_info_mh <- cr_counts_mh |> filter(n == min(n))
slowest_cr_mh <- min_cr_info_mh |> slice(1) |> pull(Call_Reception)
slowest_cr_count_mh <- min_cr_info_mh |> slice(1) |> pull(n)

barReception_MH <- df_mh |> ggplot(aes(x=Call_Reception, fill=Call_Reception)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Calls for Service for by Call Reception",
       x="Call Reception",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size = 3.5
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barReception_MH
```

This week, most of our mental health calls `r busiest_cr_mh` Further analysis could be understaken to determine if any of these are transfer calls from our local 988 provider partner.

```{r}
#| label: mental-health-problem-types
#| echo: false
#| fig-cap: "Number of calls for service by call type."
# ggplot2
ct_counts_mh <- df_mh |>
  count(Problem, sort = TRUE)

max_ct_info_mh <- ct_counts_mh |> filter(n == max(n))
busiest_ct_mh <- max_ct_info_mh |> slice(1) |> pull(Problem)
busiest_ct_count_mh <- max_ct_info_mh |> slice(1) |> pull(n)

min_ct_info_mh <- ct_counts_mh |> filter(n == min(n))
slowest_ct_mh <- min_ct_info_mh |> slice(1) |> pull(Problem)
slowest_ct_count_mh <- min_ct_info_mh |> slice(1) |> pull(n)

problem_counts_MH <- df_mh |>
  count(Problem, sort = TRUE) |>
  slice_head(n = 10)

barProblem_MH <- problem_counts_MH |>
  ggplot(aes(x=reorder(Problem, -n), y=n, fill=Problem)) +
  geom_bar(stat="identity") +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Calls for Service by Call Type",
       x="Call Type",
       y="Number of Calls") +
    geom_text(
        aes(label = n),
        vjust = -0.7,
        size = 3) +
    theme_minimal(base_size = 9) +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9)
    )

barProblem_MH
```

The most used call type was `r busiest_ct_mh` which is expected.

```{r}
#| label: mental-health-priority
#| echo: false
#| fig-cap: "Number of calls for service by priority level."
# ggplot2
pri_counts_mh <- df_mh |>
  count(Priority_Number, sort = TRUE)

max_pri_info_mh <- pri_counts_mh |> filter(n == max(n))
busiest_pri_mh <- max_pri_info_mh |> slice(1) |> pull(Priority_Number)
busiest_pri_count_mh <- max_pri_info_mh |> slice(1) |> pull(n)

min_pri_info_mh <- pri_counts_mh |> filter(n == min(n))
slowest_pri_mh <- min_pri_info_mh |> slice(1) |> pull(Priority_Number)
slowest_pri_count_mh <- min_pri_info_mh |> slice(1) |> pull(n)

barPriority_MH <- df_mh |> ggplot(aes(x=Priority_Number, fill=Priority_Number)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Calls for Service by Priority Level",
       x="Priority Level",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size = 3.5
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barPriority_MH
```

Since `r busiest_ct_mh` was the most used call type and is a P2 call, Priority `r busiest_pri_mh` is the most used priority. The question, in the future, will be does these calls need to changed to a higher priority?

```{r}
#| label: mh-custom-summary
#| echo: false
#| message: false
#| warning: false

# Initialize variables
summary_table <- NULL
median_processing_time <- NA_real_

# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time",
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_mh)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- df_mh %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

    #Extract key metrics for use in text
  median_time_to_queue <- summary_table |>
  filter(Variable == "Time To Queue") |>
      pull(Median)

  median_time_to_dispatch <- summary_table |>
  filter(Variable == "Time To Dispatch") |>
      pull(Median)

    median_processing_time <- summary_table |>
      filter(Variable == "Processing_Time") |>
      pull(Median)

    median_phone_time <- summary_table |>
      filter(Variable == "Phone_Time") |>
      pull(Median)

    to_ft(
      summary_table %>% dplyr::select(Variable, Minimum, Mean, Median),
      caption = "Weekly Elapsed Time Summary Table — Statistical summary of call processing times",
      header_map = list(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median"
      ),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})

    # Always assign these for inline use
    median_phone_time_mh <- tryCatch({
      summary_table |> filter(Variable == "Phone Time") |> pull(Median)
    }, error = function(e) NA_real_)

    median_processing_time_mh <- tryCatch({
      summary_table |> filter(Variable == "Processing Time") |> pull(Median)
    }, error = function(e) NA_real_)
```

Processing times for these calls are longer, somewhere around `r median_processing_time_mh` seconds. There are several factors that can impact this. The time to make it dispatchable was longer, implying that with these types of calls, it take calltakers longer to get the information necessary in the initial triage to accurately locate and classify the call. Another possible issue, in reviewing the dispatch times is that these calls require specialized training and skill sets on the part of the field responders. If those responders are already assigned to other calls, this could create the delay as seen here. As these values change over time, we should be able to build better pictures and determine the delay points and create strategies to ameliorate them.

### Call Source Unrecorded

As discussed earlier in the document, there are `r nrow(df_nrr)` calls where the call reception method was not recorded. In order to determine if there are any operational issues behind this, we can deep dive into these calls and see what information we can obtain from them.

```{r}
#| label: source-unknown
#| echo: false
#| message: false
#| warning: false

#| fig.width: 8
#| fig.height: 5

if (nrow(df_nrr) == 0) {
  cat("No calls with unrecorded or uncaptured call source this week.\n")
} else {
  nrr_tc_counts <- df_nrr |>
    count(Call_Taker, sort = TRUE) |>
    mutate(color_id = dplyr::row_number())

  # Guard in case Call_Taker is all NA
  if (nrow(nrr_tc_counts) == 0 || all(is.na(nrr_tc_counts$Call_Taker))) {
    cat("No call taker information available for unrecorded call sources.\n")
  } else {
    nrr_max_tc_info <- nrr_tc_counts |> dplyr::filter(n == max(n))
    nrr_busiest_tc <- nrr_max_tc_info |> dplyr::slice(1) |> dplyr::pull(Call_Taker)
    nrr_busiest_tc_count <- nrr_max_tc_info |> dplyr::slice(1) |> dplyr::pull(n)

    # Get number of call takers for proper color scaling
    n_takers <- nrow(nrr_tc_counts)

    nrr_barCallTaker <- nrr_tc_counts |>
      ggplot(aes(x=reorder(Call_Taker, -n), y=n, fill=factor(color_id))) +
      geom_bar(stat="identity") +
      scale_fill_manual(values = grDevices::colorRampPalette(
        paletteer::paletteer_d("ggsci::teal_material", n = 10))(n_takers)) +
      labs(title="Call Source Not Recorded by Call Taker",
           x="Call Taker",
           y="Number of Call Source Not Recorded Calls") +
      geom_text(
        aes(label = n),
        vjust = -0.5,
        size = 3.5) +
      theme_minimal() +
      theme(legend.position="none",
            plot.title = element_text(hjust = 0.5, size = 12),
            axis.text.x = element_text(angle=45, hjust=1, size=8),
            axis.text.y = element_text(size=8),
            axis.title.x = element_text(size=10),
            axis.title.y = element_text(size=10),
            plot.margin = margin(5, 5, 5, 5))

    nrr_barCallTaker
  }
}
```

There are a few calltakers with more than one call where the call source went unrecorded. `r nrr_busiest_tc` had the most calls without recording the source with `r nrr_busiest_tc_count` calls. We should look into those calls to see what additional factors exist that caused this to occur and work to address those.

```{r}
#| label: source-unknown-agency-distribution
#| echo: false
#| message: false
#| warning: false
# ggplot2
nrr_agency_counts <- df_nrr |>
  count(Agency, sort = TRUE)

nrr_max_agency_info <- nrr_agency_counts |> filter(n == max(n))
nrr_busiest_agency <- nrr_max_agency_info |> slice(1) |> pull(Agency)
nrr_busiest_agency_count <- nrr_max_agency_info |> slice(1) |> pull(n)

# Calculate percentage for Police calls
nrr_police_percentage <- round((sum(df_nrr$Agency == "POLICE", na.rm = TRUE) / nrow(df_nrr)) * 100, 1)

nrr_barDiscipline <- df_nrr |> ggplot(aes(x=Agency, fill=Agency)) +
  geom_bar() +
  scale_fill_manual(
    values = c(POLICE = "#1f77b4", FIRE = "#d62728", EMS = "#2ca02c"),
    name = "Agency"
  ) +
  labs(title="Number of Calls for Service by Discipline with no source",
       x="Discipline",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    position = position_stack(vjust = 0.5),
    size = 3.5,
    color = "white",
    fontface = "bold"
  ) +
  theme_minimal() +
  theme(legend.position="right",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.title = element_text(size = 9),
        legend.text = element_text(size = 9))

nrr_barDiscipline
```

The majority of calls with no source were for `r nrr_busiest_agency` with `r nrr_busiest_agency_count` calls for service. This week, that is substatially higher than the other two agencies combined.

```{r}
#| label: source-unknown-problem
#| echo: false
#| message: false
#| warning: false
# ggplot2

nrr_problem_counts <- df_nrr |>
  count(Problem, sort = TRUE) |>
  mutate(color_id = row_number())

# Get number of problem types for proper color scaling
n_problems <- nrow(nrr_problem_counts)

nrr_barProblem <- nrr_problem_counts |> 
  ggplot(aes(x=reorder(Problem, -n), y=n, fill=factor(color_id))) +
  geom_bar(stat="identity") +
  scale_fill_manual(values = grDevices::colorRampPalette(
    paletteer::paletteer_d("ggsci::teal_material", n = 10))(n_problems)) +
  labs(title="Number of Unrecorded calls by Call Type",
       x="Call Type",
       y="Number of Calls") +
  geom_text(
    aes(label = n),
    vjust = -0.5,
    size = 3.5) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

nrr_barProblem
```

### Hybrid Call Takers

This week, the hybrid call takers took `r nrow(df_hybrid)` calls compared to `r nrow(lw_hybrid)` calls last week. The following bar chart shows the distribution of calls taken by the hybrid call takers.

```{r}
#| label: hybrid-calls-taken
#| echo: false
#| message: false
#| warning: false


hct_counts <- df_hybrid |>
  count(Call_Taker, sort = TRUE)

max_hct_info <- hct_counts |> filter(n == max(n))
busiest_hct <- max_hct_info |> slice(1) |> pull(Call_Taker)
busiest_hct_count <- max_hct_info |> slice(1) |> pull(n)

barHybridCallTaker <- hct_counts |>
  ggplot(aes(x=Call_Taker, y=n, fill=Call_Taker)) +
  geom_bar(stat="identity") +
  paletteer::scale_fill_paletteer_d("ggsci::light_green_material") +
  labs(title="Number of Calls for Service by Hybrid Call Taker",
       x="Call Taker",
       y="Number of Calls") +
  geom_text(
    aes(label = n),
    vjust = -0.5,
    size = 3.5) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barHybridCallTaker
```

From this we can see that `r busiest_hct` took the most calls among the hybrid call takers with `r busiest_hct_count` calls for service. 

This chart will show the number of calls taken by call reception type. There should be very few E-911 calls present in this bar chart.

```{r}
#| label: hybrid-call-reception
#| echo: false
#| message: false
#| warning: false
# ggplot2

hct_received <- df_hybrid |>
  count(Call_Reception, sort = TRUE)

max_hct_recd_info <- hct_received |> filter(n == max(n))
most_used_hct_recd <- max_hct_recd_info |> slice(1) |> pull(Call_Reception)
most_used_hct_recd_count <- max_hct_recd_info |> slice(1) |> pull(n)

hct_911_count <- sum(df_hybrid$Call_Reception == "E-911", na.rm = TRUE)
hct_lw_911_count <- sum(lw_hybrid$Call_Reception == "E-911", na.rm = TRUE)

barHybridRecd <- hct_received |>
  ggplot(aes(x=Call_Reception, y=n, fill=Call_Reception)) +
  geom_bar(stat="identity") +
  paletteer::scale_fill_paletteer_d("ggsci::light_green_material") +
  labs(title="Call Reception Avenues by Hybrid Call Taker",
       x="Method of Call Reception",
       y="Number of Calls") +
  geom_text(
    aes(label = n),
    vjust = -0.5,
    size = 3.5) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barHybridRecd
```

This chart shows that `r most_used_hct_recd` was the method by which the Hybrid Call Takers received the most calls with `r most_used_hct_recd_count` calls for service. This represents `r round(most_used_hct_recd_count / nrow(df_hybrid) * 100, 2)`% of calls received by the hybrid call takers. 

The hybrid call takers answered `r hct_911_count` 911 trunk line calls this week, compared to `r hct_lw_911_count` calls last week.

The summary table below shows the number of calls with the mean and median times to get a call to queue and the times on the phone for the hybrid call takers.

```{r}
#| label: hybrid-summary
#| echo: false
#| message: false
#| warning: false

# Create summary statistics by Call_Taker for hybrid call takers
hybrid_summary_table <- df_hybrid %>%
  group_by(Call_Taker) %>%
  summarise(
    Calls = n(),
    Mean_TTQ = round(mean(Time_To_Queue, na.rm = TRUE), 2),
    Median_TTQ = round(median(Time_To_Queue, na.rm = TRUE), 2),
    Mean_Phone = round(mean(Phone_Time, na.rm = TRUE), 2),
    Median_Phone = round(median(Phone_Time, na.rm = TRUE), 2),
    .groups = 'drop'
  )

# Identify call takers with highest values for each metric
hct_most_calls <- hybrid_summary_table %>%
  filter(Calls == max(Calls, na.rm = TRUE)) %>%
  pull(Call_Taker)

hct_highest_mean_ttq <- hybrid_summary_table %>%
  filter(Mean_TTQ == max(Mean_TTQ, na.rm = TRUE)) %>%
  pull(Call_Taker)

hct_highest_median_ttq <- hybrid_summary_table %>%
  filter(Median_TTQ == max(Median_TTQ, na.rm = TRUE)) %>%
  pull(Call_Taker)

hct_highest_mean_phone <- hybrid_summary_table %>%
  filter(Mean_Phone == max(Mean_Phone, na.rm = TRUE)) %>%
  pull(Call_Taker)

hct_highest_median_phone <- hybrid_summary_table %>%
  filter(Median_Phone == max(Median_Phone, na.rm = TRUE)) %>%
  pull(Call_Taker)

hct_lowest_volume <- hybrid_summary_table %>%
  filter(Calls == min(Calls, na.rm = TRUE)) %>%
  pull(Call_Taker)

to_ft(
  hybrid_summary_table,
  caption = "Hybrid Call Taker Summary — Call counts and time metrics by hybrid call taker",
  header_map = list(
    Call_Taker = "Call_Taker",
    Calls = "Calls",
    Mean_TTQ = "Mean TTQ (sec)",
    Median_TTQ = "Median TTQ (sec)",
    Mean_Phone = "Mean Phone (sec)",
    Median_Phone = "Median Phone (sec)"
  ),
  digits = 2
)

```

This shows, from this information, that `r hct_highest_median_ttq` had the largest median time to get a call into queue at `r max(hybrid_summary_table$Median_TTQ, na.rm = TRUE)` seconds and `r hct_highest_median_phone` had the largest median time on the phone at `r max(hybrid_summary_table$Median_Phone, na.rm = TRUE)` seconds. In this case, the same hybrid call taker took the least number of calls for the week. This could be something that the supervisors will want to monitor.

## Outlier Identification

The calls in this set have been identified as outliers using four methods. The first is called the 'Z-score' method. This method calculates the number of standard deviations the observation is from the mean. When that number is greater than 3 or less than -3, the observation will be flagged as an outlier. The second method uses the Interquartile range (IQR), or the distance between the 25th and 75th percentiles. Each row is evaluated to see if the column in question contains a value that is greater than 1.5 times the IQR above the 75th percentile or less than 1.5 times the IQR below the 25th percentile.
The third is called the Hampel identifier. This method uses the median and the median absolute deviation (MAD) to identify outliers. The MAD is multiplied by 3 and added to and subtracted from the median to create upper and lower bounds for the data. Any value outside of these bounds is flagged as an outlier. This method is more robust than the Z-score method as it is less affected by extreme values in the data. This method was included because this data is non-parametric and we are relying on the median values in our reports. A final test that is used occasionally is to identify the rows that are in the 1st and 99th percentiles for the column in question. Each of these methods has advantages and disadvantages in use. Additionally, the IQR can pre-suppose a normal distribution of data, which as can be seen above, does not exist in our datasets. The output of this test should identify rows with values larger than the target values and very few, if any, that are lower than the target values. This is also due to the distribution of the data.

### Critical Priority Outlier Alerts

```{r}
#| label: p1-p2-outlier-alerts
#| echo: false
#| message: false
#| warning: false
#| results: asis

# Identify extreme outliers for Priority 1 and 2 calls
p1_calls <- df %>%
  filter(Priority_Number == "1", !is.na(Time_To_Queue), !is.na(Time_To_Dispatch))

p2_calls <- df %>%
  filter(Priority_Number == "2", !is.na(Time_To_Queue), !is.na(Time_To_Dispatch))

# Find top 5 worst TTQ for P1
p1_worst_ttq <- p1_calls %>%
  arrange(desc(Time_To_Queue)) %>%
  slice_head(n = 5) %>%
  dplyr::select(Master_Incident_Number, Agency, Problem, Call_Taker, Time_To_Queue, Time_To_Dispatch)

# Find top 5 worst TTD for P1
p1_worst_ttd <- p1_calls %>%
  arrange(desc(Time_To_Dispatch)) %>%
  slice_head(n = 5) %>%
  dplyr::select(Master_Incident_Number, Agency, Problem, Dispatcher, Time_To_Queue, Time_To_Dispatch)

# Find top 5 worst TTQ for P2
p2_worst_ttq <- p2_calls %>%
  arrange(desc(Time_To_Queue)) %>%
  slice_head(n = 5) %>%
  dplyr::select(Master_Incident_Number, Agency, Problem, Call_Taker, Time_To_Queue, Time_To_Dispatch)

# Find top 5 worst TTD for P2
p2_worst_ttd <- p2_calls %>%
  arrange(desc(Time_To_Dispatch)) %>%
  slice_head(n = 5) %>%
  dplyr::select(Master_Incident_Number, Agency, Problem, Dispatcher, Time_To_Queue, Time_To_Dispatch)

# Generate alerts
alerts_generated <- FALSE

if (nrow(p1_worst_ttq) > 0 && max(p1_worst_ttq$Time_To_Queue) > 120) {
  cat("\n⚠️ **CRITICAL ALERT - Priority 1 Time to Queue Outliers**\n\n")
  cat("The following Priority 1 calls had exceptionally long Time to Queue values:\n\n")
  print(kable(p1_worst_ttq, caption = "Top 5 P1 Calls by Time to Queue"))
  cat("\n")
  alerts_generated <- TRUE
}

if (nrow(p1_worst_ttd) > 0 && max(p1_worst_ttd$Time_To_Dispatch) > 180) {
  cat("\n⚠️ **CRITICAL ALERT - Priority 1 Time to Dispatch Outliers**\n\n")
  cat("The following Priority 1 calls had exceptionally long Time to Dispatch values:\n\n")
  print(kable(p1_worst_ttd, caption = "Top 5 P1 Calls by Time to Dispatch"))
  cat("\n")
  alerts_generated <- TRUE
}

if (nrow(p2_worst_ttq) > 0 && max(p2_worst_ttq$Time_To_Queue) > 180) {
  cat("\n⚠️ **WARNING - Priority 2 Time to Queue Outliers**\n\n")
  cat("The following Priority 2 calls had exceptionally long Time to Queue values:\n\n")
  print(kable(p2_worst_ttq, caption = "Top 5 P2 Calls by Time to Queue"))
  cat("\n")
  alerts_generated <- TRUE
}

if (nrow(p2_worst_ttd) > 0 && max(p2_worst_ttd$Time_To_Dispatch) > 240) {
  cat("\n⚠️ **WARNING - Priority 2 Time to Dispatch Outliers**\n\n")
  cat("The following Priority 2 calls had exceptionally long Time to Dispatch values:\n\n")
  print(kable(p2_worst_ttd, caption = "Top 5 P2 Calls by Time to Dispatch"))
  cat("\n")
  alerts_generated <- TRUE
}

if (!alerts_generated) {
  cat("\n✅ **No critical outliers detected for Priority 1 or Priority 2 calls**\n\n")
}

cat("\n**Recommendation:** Review these incidents to identify root causes and implement corrective actions to prevent recurrence.\n\n")
```

### General Outlier Analysis

```{r}
#| label: outliers
#| echo: false
#| message: false
#| warning: false

ttq_lbq <- quantile(df$Time_To_Queue, 0.025, na.rm = TRUE)
ttq_ubq <- quantile(df$Time_To_Queue, 0.975, na.rm = TRUE)
ttd_lbq <- quantile(df$Time_To_Dispatch, 0.025, na.rm = TRUE)
ttd_ubq <- quantile(df$Time_To_Dispatch, 0.975, na.rm = TRUE)
phone_lbq <- quantile(df$Phone_Time, 0.025, na.rm = TRUE)
phone_ubq <- quantile(df$Phone_Time, 0.975, na.rm = TRUE)
proc_lbq <- quantile(df$Processing_Time, 0.025, na.rm = TRUE)
proc_ubq <- quantile(df$Processing_Time, 0.975, na.rm = TRUE)

lw_ttq_lbq <- quantile(df_last$Time_To_Queue, 0.025, na.rm = TRUE)
lw_ttq_ubq <- quantile(df_last$Time_To_Queue, 0.975, na.rm = TRUE)
lw_ttd_lbq <- quantile(df_last$Time_To_Dispatch, 0.025, na.rm = TRUE)
lw_ttd_ubq <- quantile(df_last$Time_To_Dispatch, 0.975, na.rm = TRUE)
lw_phone_lbq <- quantile(df_last$Phone_Time, 0.025, na.rm = TRUE)
lw_phone_ubq <- quantile(df_last$Phone_Time, 0.975, na.rm = TRUE)
lw_proc_lbq <- quantile(df_last$Processing_Time, 0.025, na.rm = TRUE)
lw_proc_ubq <- quantile(df_last$Processing_Time, 0.975, na.rm = TRUE)

ttq_q1 <- quantile(df$Time_To_Queue, 0.25, na.rm = TRUE)
ttq_q3 <- quantile(df$Time_To_Queue, 0.75, na.rm = TRUE)
ttd_q1 <- quantile(df$Time_To_Dispatch, 0.25, na.rm = TRUE)
ttd_q3 <- quantile(df$Time_To_Dispatch, 0.75, na.rm = TRUE)
phone_q1 <- quantile(df$Phone_Time, 0.25, na.rm = TRUE)
phone_q3 <- quantile(df$Phone_Time, 0.75, na.rm = TRUE)
proc_q1 <- quantile(df$Processing_Time, 0.25, na.rm = TRUE)
proc_q3 <- quantile(df$Processing_Time, 0.75, na.rm = TRUE)

lw_ttq_q1 <- quantile(df_last$Time_To_Queue, 0.25, na.rm = TRUE)
lw_ttq_q3 <- quantile(df_last$Time_To_Queue, 0.75, na.rm = TRUE)
lw_ttd_q1 <- quantile(df_last$Time_To_Dispatch, 0.25, na.rm = TRUE)
lw_ttd_q3 <- quantile(df_last$Time_To_Dispatch, 0.75, na.rm = TRUE)
lw_phone_q1 <- quantile(df_last$Phone_Time, 0.25, na.rm = TRUE)
lw_phone_q3 <- quantile(df_last$Phone_Time, 0.75, na.rm = TRUE)
lw_proc_q1 <- quantile(df_last$Processing_Time, 0.25, na.rm = TRUE)
lw_proc_q3 <- quantile(df_last$Processing_Time, 0.75, na.rm = TRUE)

ttq_iqr <- IQR(df$Time_To_Queue, na.rm = TRUE)
ttd_iqr <- IQR(df$Time_To_Dispatch, na.rm = TRUE)
phone_iqr <- IQR(df$Phone_Time, na.rm = TRUE)
proc_iqr <- IQR(df$Processing_Time, na.rm = TRUE)

lw_ttq_iqr <- IQR(df_last$Time_To_Queue, na.rm = TRUE)
lw_ttd_iqr <- IQR(df_last$Time_To_Dispatch, na.rm = TRUE)
lw_phone_iqr <- IQR(df_last$Phone_Time, na.rm = TRUE)
lw_proc_iqr <- IQR(df_last$Processing_Time, na.rm = TRUE)

ttq_iql <- ttq_q1 - 1.5 * ttq_iqr
ttq_iqu <- ttq_q3 + 1.5 * ttq_iqr
ttd_iql <- ttd_q1 - 1.5 * ttd_iqr
ttd_iqu <- ttd_q3 + 1.5 * ttd_iqr
phone_iql <- phone_q1 - 1.5 * phone_iqr
phone_iqu <- phone_q3 + 1.5 * phone_iqr
proc_iql <- proc_q1 - 1.5 * proc_iqr
proc_iqu <- proc_q3 + 1.5 * proc_iqr

lw_ttq_iql <- lw_ttq_q1 - 1.5 * lw_ttq_iqr
lw_ttq_iqu <- lw_ttq_q3 + 1.5 * lw_ttq_iqr
lw_ttd_iql <- lw_ttd_q1 - 1.5 * lw_ttd_iqr
lw_ttd_iqu <- lw_ttd_q3 + 1.5 * lw_ttd_iqr
lw_phone_iql <- lw_phone_q1 - 1.5 * lw_phone_iqr
lw_phone_iqu <- lw_phone_q3 + 1.5 * lw_phone_iqr
lw_proc_iql <- lw_proc_q1 - 1.5 * lw_proc_iqr
lw_proc_iqu <- lw_proc_q3 + 1.5 * lw_proc_iqr

ttq_hampl <- median(df$Time_To_Queue, na.rm = TRUE) - 3*mad(df$Time_To_Queue, constant = 1, na.rm = TRUE)
ttq_hampu <- median(df$Time_To_Queue, na.rm = TRUE) + 3*mad(df$Time_To_Queue, constant = 1, na.rm = TRUE)
ttd_hampl <- median(df$Time_To_Dispatch, na.rm = TRUE) - 3*mad(df$Time_To_Dispatch, constant = 1, na.rm = TRUE)
ttd_hampu <- median(df$Time_To_Dispatch, na.rm = TRUE) + 3*mad(df$Time_To_Dispatch, constant = 1, na.rm = TRUE)
phone_hampl <- median(df$Phone_Time, na.rm = TRUE) - 3*mad(df$Phone_Time, constant = 1, na.rm = TRUE)
phone_hampu <- median(df$Phone_Time, na.rm = TRUE) + 3*mad(df$Phone_Time, constant = 1, na.rm = TRUE)
proc_hampl <- median(df$Processing_Time, na.rm = TRUE) - 3*mad(df$Processing_Time, constant = 1, na.rm = TRUE)
proc_hampu <- median(df$Processing_Time, na.rm = TRUE) + 3*mad(df$Processing_Time, constant = 1, na.rm = TRUE)

lw_ttq_hampl <- median(df_last$Time_To_Queue, na.rm = TRUE) - 3*mad(df_last$Time_To_Queue, constant = 1, na.rm = TRUE)
lw_ttq_hampu <- median(df_last$Time_To_Queue, na.rm = TRUE) + 3*mad(df_last$Time_To_Queue, constant = 1, na.rm = TRUE)
lw_ttd_hampl <- median(df_last$Time_To_Dispatch, na.rm = TRUE) - 3*mad(df_last$Time_To_Dispatch, constant = 1, na.rm = TRUE)
lw_ttd_hampu <- median(df_last$Time_To_Dispatch, na.rm = TRUE) + 3*mad(df_last$Time_To_Dispatch, constant = 1, na.rm = TRUE)
lw_phone_hampl <- median(df_last$Phone_Time, na.rm = TRUE) - 3*mad(df_last$Phone_Time, constant = 1, na.rm = TRUE)
lw_phone_hampu <- median(df_last$Phone_Time, na.rm = TRUE) + 3*mad(df_last$Phone_Time, constant = 1, na.rm = TRUE)
lw_proc_hampl <- median(df_last$Processing_Time, na.rm = TRUE) - 3*mad(df_last$Processing_Time, constant = 1, na.rm = TRUE)
lw_proc_hampu <- median(df_last$Processing_Time, na.rm = TRUE) + 3*mad(df_last$Processing_Time, constant = 1, na.rm = TRUE)

outlier_df <- df %>%
  mutate(
    TTQ_Z_Score = (as.numeric(Time_To_Queue) - mean(as.numeric(Time_To_Queue), na.rm = TRUE)) / sd(as.numeric(Time_To_Queue), na.rm = TRUE),
    TTD_Z_Score = (as.numeric(Time_To_Dispatch) - mean(as.numeric(Time_To_Dispatch), na.rm = TRUE)) / sd(as.numeric(Time_To_Dispatch), na.rm = TRUE),
    Phone_Z_Score = (as.numeric(Phone_Time) - mean(as.numeric(Phone_Time), na.rm = TRUE)) / sd(as.numeric(Phone_Time), na.rm = TRUE),
    Proc_Z_Score = (as.numeric(Processing_Time) - mean(as.numeric(Processing_Time), na.rm = TRUE)) / sd(as.numeric(Processing_Time), na.rm = TRUE),
    TTQ_Outlier_Z = ifelse(abs(TTQ_Z_Score) > 3, TRUE, FALSE),
    TTD_Outlier_Z = ifelse(abs(TTD_Z_Score) > 3, TRUE, FALSE),
    Phone_Outlier_Z = ifelse(abs(Phone_Z_Score) > 3, TRUE, FALSE),
    Proc_Outlier_Z = ifelse(abs(Proc_Z_Score) > 3, TRUE, FALSE),
    TTQ_Outlier_IQR = ifelse(Time_To_Queue < ttq_iql | Time_To_Queue > ttq_iqu, TRUE, FALSE),
    TTD_Outlier_IQR = ifelse(Time_To_Dispatch < ttd_iql | Time_To_Dispatch > ttd_iqu, TRUE, FALSE),
    Phone_Outlier_IQR = ifelse(Phone_Time < phone_iql | Phone_Time > phone_iqu, TRUE, FALSE),
    Proc_Outlier_IQR = ifelse(Processing_Time < proc_iql | Processing_Time > proc_iqu, TRUE, FALSE),
    TTQ_Outlier_Percentile = ifelse(Time_To_Queue < ttq_lbq | Time_To_Queue > ttq_ubq, TRUE, FALSE),
    TTD_Outlier_Percentile = ifelse(Time_To_Dispatch < ttd_lbq | Time_To_Dispatch > ttd_ubq, TRUE, FALSE),
    Phone_Outlier_Percentile = ifelse(Phone_Time < phone_lbq | Phone_Time > phone_ubq, TRUE, FALSE),
    Proc_Outlier_Percentile = ifelse(Processing_Time < proc_lbq | Processing_Time > proc_ubq, TRUE, FALSE),
    TTQ_Hampel = ifelse(Time_To_Queue < ttq_hampl | Time_To_Queue > ttq_hampu, TRUE, FALSE),
    TTD_Hampel = ifelse(Time_To_Dispatch < ttd_hampl | Time_To_Dispatch > ttd_hampu, TRUE, FALSE),
    Phone_Hampel = ifelse(Phone_Time < phone_hampl | Phone_Time > phone_hampu, TRUE, FALSE),
    Proc_Hampel = ifelse(Processing_Time < proc_hampl | Processing_Time > proc_hampu, TRUE, FALSE)
  ) %>%
  # Create composite flags: a row is an outlier for a variable if flagged by all 4 methods
  mutate(
    TTQ_Outlier_All = TTQ_Outlier_Z & TTQ_Outlier_IQR & TTQ_Outlier_Percentile & TTQ_Hampel,
    TTD_Outlier_All = TTD_Outlier_Z & TTD_Outlier_IQR & TTD_Outlier_Percentile & TTD_Hampel,
    Phone_Outlier_All = Phone_Outlier_Z & Phone_Outlier_IQR & Phone_Outlier_Percentile & Phone_Hampel,
    Proc_Outlier_All = Proc_Outlier_Z & Proc_Outlier_IQR & Proc_Outlier_Percentile & Proc_Hampel
  ) %>%
  # Filter to keep only rows where at least one variable is flagged by all 4 methods
  filter(
    TTQ_Outlier_All | TTD_Outlier_All | Phone_Outlier_All | Proc_Outlier_All
  )

outlier_lw <- df_last %>%
  mutate(
    LW_TTQ_Z_Score = (as.numeric(Time_To_Queue) - mean(as.numeric(Time_To_Queue), na.rm = TRUE)) / sd(as.numeric(Time_To_Queue), na.rm = TRUE),
    LW_TTD_Z_Score = (as.numeric(Time_To_Dispatch) - mean(as.numeric(Time_To_Dispatch), na.rm = TRUE)) / sd(as.numeric(Time_To_Dispatch), na.rm = TRUE),
    LW_Phone_Z_Score = (as.numeric(Phone_Time) - mean(as.numeric(Phone_Time), na.rm = TRUE)) / sd(as.numeric(Phone_Time), na.rm = TRUE),
    LW_Proc_Z_Score = (as.numeric(Processing_Time) - mean(as.numeric(Processing_Time), na.rm = TRUE)) / sd(as.numeric(Processing_Time), na.rm = TRUE),
    LW_TTQ_Outlier_Z = ifelse(abs(LW_TTQ_Z_Score) > 3, TRUE, FALSE),
    LW_TTD_Outlier_Z = ifelse(abs(LW_TTD_Z_Score) > 3, TRUE, FALSE),
    LW_Phone_Outlier_Z = ifelse(abs(LW_Phone_Z_Score) > 3, TRUE, FALSE),
    LW_Proc_Outlier_Z = ifelse(abs(LW_Proc_Z_Score) > 3, TRUE, FALSE),
    LW_TTQ_Outlier_IQR = ifelse(Time_To_Queue < lw_ttq_iql | Time_To_Queue > lw_ttq_iqu, TRUE, FALSE),
    LW_TTD_Outlier_IQR = ifelse(Time_To_Dispatch < lw_ttd_iql | Time_To_Dispatch > lw_ttd_iqu, TRUE, FALSE),
    LW_Phone_Outlier_IQR = ifelse(Phone_Time < lw_phone_iql | Phone_Time > lw_phone_iqu, TRUE, FALSE),
    LW_Proc_Outlier_IQR = ifelse(Processing_Time < lw_proc_iql | Processing_Time > lw_proc_iqu, TRUE, FALSE),
    LW_TTQ_Outlier_Percentile = ifelse(Time_To_Queue < lw_ttq_lbq | Time_To_Queue > lw_ttq_ubq, TRUE, FALSE),
    LW_TTD_Outlier_Percentile = ifelse(Time_To_Dispatch < lw_ttd_lbq | Time_To_Dispatch > lw_ttd_ubq, TRUE, FALSE),
    LW_Phone_Outlier_Percentile = ifelse(Phone_Time < lw_phone_lbq | Phone_Time > lw_phone_ubq, TRUE, FALSE),
    LW_Proc_Outlier_Percentile = ifelse(Processing_Time < lw_proc_lbq | Processing_Time > lw_proc_ubq, TRUE, FALSE),
    LW_TTQ_Hampel = ifelse(Time_To_Queue < lw_ttq_hampl | Time_To_Queue > lw_ttq_hampu, TRUE, FALSE),
    LW_TTD_Hampel = ifelse(Time_To_Dispatch < lw_ttd_hampl | Time_To_Dispatch > lw_ttd_hampu, TRUE, FALSE),
    LW_Phone_Hampel = ifelse(Phone_Time < lw_phone_hampl | Phone_Time > lw_phone_hampu, TRUE, FALSE),
    LW_Proc_Hampel = ifelse(Processing_Time < lw_proc_hampl | Processing_Time > lw_proc_hampu, TRUE, FALSE)
  ) %>%
  # Create composite flags: a row is an outlier for a variable if flagged by all 4 methods
  mutate(
    LW_TTQ_Outlier_All = LW_TTQ_Outlier_Z & LW_TTQ_Outlier_IQR & LW_TTQ_Outlier_Percentile & LW_TTQ_Hampel,
    LW_TTD_Outlier_All = LW_TTD_Outlier_Z & LW_TTD_Outlier_IQR & LW_TTD_Outlier_Percentile & LW_TTD_Hampel,
    LW_Phone_Outlier_All = LW_Phone_Outlier_Z & LW_Phone_Outlier_IQR & LW_Phone_Outlier_Percentile & LW_Phone_Hampel,
    LW_Proc_Outlier_All = LW_Proc_Outlier_Z & LW_Proc_Outlier_IQR & LW_Proc_Outlier_Percentile & LW_Proc_Hampel
  ) %>%
  # Filter to keep only rows where at least one variable is flagged by all 4 methods
  filter(
    LW_TTQ_Outlier_All | LW_TTD_Outlier_All | LW_Phone_Outlier_All | LW_Proc_Outlier_All
  )

  ttq_outliers <- df %>%
  mutate(
    TTQ_Z_Score = (as.numeric(Time_To_Queue) - mean(as.numeric(Time_To_Queue), na.rm = TRUE)) / sd(as.numeric(Time_To_Queue), na.rm = TRUE),
    TTQ_Outlier_Z = ifelse(abs(TTQ_Z_Score) > 3, TRUE, FALSE),
    TTQ_Outlier_IQR = ifelse(Time_To_Queue < ttq_iql | Time_To_Queue > ttq_iqu, TRUE, FALSE),
    TTQ_Outlier_Percentile = ifelse(Time_To_Queue < ttq_lbq | Time_To_Queue > ttq_ubq, TRUE, FALSE),
    TTQ_Hampel = ifelse(Time_To_Queue < ttq_hampl | Time_To_Queue > ttq_hampu, TRUE, FALSE)
  ) %>%
  mutate(
    TTQ_Outlier_All = TTQ_Outlier_Z & TTQ_Outlier_IQR & TTQ_Outlier_Percentile & TTQ_Hampel
  ) %>%
  filter (
    TTQ_Outlier_All
  )

lw_ttq_outliers <- df_last %>%
  mutate(
    LW_TTQ_Z_Score = (as.numeric(Time_To_Queue) - mean(as.numeric(Time_To_Queue), na.rm = TRUE)) / sd(as.numeric(Time_To_Queue), na.rm = TRUE),
    LW_TTQ_Outlier_Z = ifelse(abs(LW_TTQ_Z_Score) > 3, TRUE, FALSE),
    LW_TTQ_Outlier_IQR = ifelse(Time_To_Queue < lw_ttq_iql | Time_To_Queue > lw_ttq_iqu, TRUE, FALSE),
    LW_TTQ_Outlier_Percentile = ifelse(Time_To_Queue < lw_ttq_lbq | Time_To_Queue > lw_ttq_ubq, TRUE, FALSE),
    LW_TTQ_Hampel = ifelse(Time_To_Queue < lw_ttq_hampl | Time_To_Queue > lw_ttq_hampu, TRUE, FALSE)
  ) %>%
  mutate(
    LW_TTQ_Outlier_All = LW_TTQ_Outlier_Z & LW_TTQ_Outlier_IQR & LW_TTQ_Outlier_Percentile & LW_TTQ_Hampel
  ) %>%
  filter (
    LW_TTQ_Outlier_All
  )

ttd_outliers <- df %>%
  mutate(
    TTD_Z_Score = (as.numeric(Time_To_Dispatch) - mean(as.numeric(Time_To_Dispatch), na.rm = TRUE)) / sd(as.numeric(Time_To_Dispatch), na.rm = TRUE),
    TTD_Outlier_Z = ifelse(abs(TTD_Z_Score) > 3, TRUE, FALSE),
    TTD_Outlier_IQR = ifelse(Time_To_Dispatch < ttd_iql | Time_To_Dispatch > ttd_iqu, TRUE, FALSE),
    TTD_Outlier_Percentile = ifelse(Time_To_Dispatch < ttd_lbq | Time_To_Dispatch > ttd_ubq, TRUE, FALSE),
    TTD_Hampel = ifelse(Time_To_Dispatch < ttd_hampl | Time_To_Dispatch > ttd_hampu, TRUE, FALSE)
  ) %>%
  mutate(
    TTD_Outlier_All = TTD_Outlier_Z & TTD_Outlier_IQR & TTD_Outlier_Percentile & TTD_Hampel
  ) %>%
  filter (
    TTD_Outlier_All
  )

lw_ttd_outliers <- df_last %>%
  mutate(
    LW_TTD_Z_Score = (as.numeric(Time_To_Dispatch) - mean(as.numeric(Time_To_Dispatch), na.rm = TRUE)) / sd(as.numeric(Time_To_Dispatch), na.rm = TRUE),
    LW_TTD_Outlier_Z = ifelse(abs(LW_TTD_Z_Score) > 3, TRUE, FALSE),
    LW_TTD_Outlier_IQR = ifelse(Time_To_Dispatch < lw_ttd_iql | Time_To_Dispatch > lw_ttd_iqu, TRUE, FALSE),
    LW_TTD_Outlier_Percentile = ifelse(Time_To_Dispatch < lw_ttd_lbq | Time_To_Dispatch > lw_ttd_ubq, TRUE, FALSE),
    LW_TTD_Hampel = ifelse(Time_To_Dispatch < lw_ttd_hampl | Time_To_Dispatch > lw_ttd_hampu, TRUE, FALSE)
  ) %>%
  mutate(
    LW_TTD_Outlier_All = LW_TTD_Outlier_Z & LW_TTD_Outlier_IQR & LW_TTD_Outlier_Percentile & LW_TTD_Hampel
  ) %>%
  filter (
    LW_TTD_Outlier_All
  )

phone_outliers <- df %>%
  mutate(
    Phone_Z_Score = (as.numeric(Phone_Time) - mean(as.numeric(Phone_Time), na.rm = TRUE)) / sd(as.numeric(Phone_Time), na.rm = TRUE),
    Phone_Outlier_Z = ifelse(abs(Phone_Z_Score) > 3, TRUE, FALSE),
    Phone_Outlier_IQR = ifelse(Phone_Time < phone_iql | Phone_Time > phone_iqu, TRUE, FALSE),
    Phone_Outlier_Percentile = ifelse(Phone_Time < phone_lbq | Phone_Time > phone_ubq, TRUE, FALSE),
    Phone_Hampel = ifelse(Phone_Time < phone_hampl | Phone_Time > phone_hampu, TRUE, FALSE)
  ) %>%
    mutate(
        Phone_Outlier_All = Phone_Outlier_Z & Phone_Outlier_IQR & Phone_Outlier_Percentile & Phone_Hampel
    ) %>%
    filter (
        Phone_Outlier_All
    )

lw_phone_outliers <- df_last %>%
  mutate(
    LW_Phone_Z_Score = (as.numeric(Phone_Time) - mean(as.numeric(Phone_Time), na.rm = TRUE)) / sd(as.numeric(Phone_Time), na.rm = TRUE),
    LW_Phone_Outlier_Z = ifelse(abs(LW_Phone_Z_Score) > 3, TRUE, FALSE),
    LW_Phone_Outlier_IQR = ifelse(Phone_Time < lw_phone_iql | Phone_Time > lw_phone_iqu, TRUE, FALSE),
    LW_Phone_Outlier_Percentile = ifelse(Phone_Time < lw_phone_lbq | Phone_Time > lw_phone_ubq, TRUE, FALSE),
    LW_Phone_Hampel = ifelse(Phone_Time < lw_phone_hampl | Phone_Time > lw_phone_hampu, TRUE, FALSE)
  ) %>%
    mutate(
        LW_Phone_Outlier_All = LW_Phone_Outlier_Z & LW_Phone_Outlier_IQR & LW_Phone_Outlier_Percentile & LW_Phone_Hampel
    ) %>%
    filter (
        LW_Phone_Outlier_All
    )

proc_outliers <- df %>%
  mutate(
    Proc_Z_Score = (as.numeric(Processing_Time) - mean(as.numeric(Processing_Time), na.rm = TRUE)) / sd(as.numeric(Processing_Time), na.rm = TRUE),
    Proc_Outlier_Z = ifelse(abs(Proc_Z_Score) > 3, TRUE, FALSE),
    Proc_Outlier_IQR = ifelse(Processing_Time < proc_iql | Processing_Time > proc_iqu, TRUE, FALSE),
    Proc_Outlier_Percentile = ifelse(Processing_Time < proc_lbq | Processing_Time > proc_ubq, TRUE, FALSE),
    Proc_Hampel = ifelse(Processing_Time < proc_hampl | Processing_Time > proc_hampu, TRUE, FALSE)
  ) %>%
  mutate(
    Proc_Outlier_All = Proc_Outlier_Z & Proc_Outlier_IQR & Proc_Outlier_Percentile & Proc_Hampel
  ) %>%
  filter (
    Proc_Outlier_All
  )

lw_proc_outliers <- df_last %>%
mutate(
  LW_Proc_Z_Score = (as.numeric(Processing_Time) - mean(as.numeric(Processing_Time), na.rm = TRUE)) / sd(as.numeric(Processing_Time), na.rm = TRUE),
  LW_Proc_Outlier_Z = ifelse(abs(LW_Proc_Z_Score) > 3, TRUE, FALSE),
  LW_Proc_Outlier_IQR = ifelse(Processing_Time < lw_proc_iql | Processing_Time > lw_proc_iqu, TRUE, FALSE),
  LW_Proc_Outlier_Percentile = ifelse(Processing_Time < lw_proc_lbq | Processing_Time > lw_proc_ubq, TRUE, FALSE),
  LW_Proc_Hampel = ifelse(Processing_Time < lw_proc_hampl | Processing_Time > lw_proc_hampu, TRUE, FALSE)
) %>%
mutate(
  LW_Proc_Outlier_All = LW_Proc_Outlier_Z & LW_Proc_Outlier_IQR & LW_Proc_Outlier_Percentile & LW_Proc_Hampel
) %>%
filter (
  LW_Proc_Outlier_All
)
```

This filter identified `r nrow(outlier_df)` rows that should be investigated as outliers. We can now look at those calls specifically and examine them in detail to see why these have been collected in our outlier set. If we wish to separate out each of the different metrics, we can separate out each of the four elapsed time measures and look at their specific details. This table shows the number of outliers in each category. Please note that the individual categories may total more than the number of outliers reported in the combined dataset. A call may be considered an outlier in more than one category.

| Category | Outlier Count |
| :------: | :-----------: |
| Time To Queue | `r nrow(ttq_outliers)` |
| Time To Dispatch | `r nrow(ttd_outliers)` |
| Phone Time | `r nrow(phone_outliers)` |
| Processing Time | `r nrow(proc_outliers)` |

The volumes for each category are interesting and future analyses may give insights into different factors. Further analyses of these calls wil occur below to enhance our understanding of the information contained in these datasets.

This table will show the ranges that are used to determine which calls are considered outliers and which are not. Please note that calls that appear in the count and analyses have been identified as outliers have been identified as such in four separate tests. The only one that will not be reflected in the table below is the Z-score. That score measures the number of standard deviations from the mean for the value. All calls with a Z-score > 3 are considered possible outliers, since 99% of data should be within 3 standard deviations of the mean.

The three measures included in this table are the Percentile Tests which measures the most extreme values by identifying the calls below the 2.5th percentile or above the 97.5th percentile, the Interquartile Range which uses the 25th and 75th percentiles to calculate the range, then identifies values that are less than 1.5 times that range from the 25th percentile or greater than 1.5 times that range from the 75th percentile, and finally the Hampel Metric. This measure sets bounds at 3 times the median absolute deviation from the median, so it's similar to the Z-score.  

| Category | Percentile Lower | Percentile Upper | IQR Lower | IQR Upper | Hampel Lower | Hampel Upper |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| Time To Queue | `r format(round(ttq_lbq, 0), scientific = FALSE)` | `r format(round(ttq_ubq, 0), scientific = FALSE)` | `r format(round(ttq_iql, 0), scientific = FALSE)` | `r format(round(ttq_iqu, 0), scientific = FALSE)` | `r format(round(ttq_hampl, 0), scientific = FALSE)` | `r format(round(ttq_hampu, 0), scientific = FALSE)` |
| Time To Dispatch | `r format(round(ttd_lbq, 0), scientific = FALSE)` | `r format(round(ttd_ubq, 0), scientific = FALSE)` | `r format(round(ttd_iql, 0), scientific = FALSE)` | `r format(round(ttd_iqu, 0), scientific = FALSE)` | `r format(round(ttd_hampl, 0), scientific = FALSE)` | `r format(round(ttd_hampu, 0), scientific = FALSE)` |
| Phone Time | `r format(round(phone_lbq, 0), scientific = FALSE)` | `r format(round(phone_ubq, 0), scientific = FALSE)` | `r format(round(phone_iql, 0), scientific = FALSE)` | `r format(round(phone_iqu, 0), scientific = FALSE)` | `r format(round(phone_hampl, 0), scientific = FALSE)` | `r format(round(phone_hampu, 0), scientific = FALSE)` |
| Processing Time | `r format(round(proc_lbq, 0), scientific = FALSE)` | `r format(round(proc_ubq, 0), scientific = FALSE)` | `r format(round(proc_iql, 0), scientific = FALSE)` | `r format(round(proc_iqu, 0), scientific = FALSE)` | `r format(round(proc_hampl, 0), scientific = FALSE)` | `r format(round(proc_hampu, 0), scientific = FALSE)` |

Based on the information in the table above, all of the outlier data will come at the upper bounds of values, since the lower bounds for the interquartile range and Hampel filters are both less than zero. The largest upper bound value uses the 97.5th percentile to identify potential outliers. If we want ot change this filter in the future, it may create an increase in the number of outliers reported.

### Detailed Outliers Information

```{r}
#| label: ttq-outlier-graphs
#| echo: false
#| message: false
#| warning: false
#| fig.width: 8
#| fig.height: 5

ttq_out_tc_counts <- ttq_outliers |>
  count(Call_Taker, sort = TRUE) |>
  mutate(color_id = row_number())

ttq_out_max_tc_info <- ttq_out_tc_counts |> filter(n == max(n))
ttq_out_busiest_tc <- ttq_out_max_tc_info |> slice(1) |> pull(Call_Taker)
ttq_out_busiest_tc_count <- ttq_out_max_tc_info |> slice(1) |> pull(n)

# Get number of call takers for proper color scaling
n_takers <- nrow(ttq_out_tc_counts)

ttq_out_barCallTaker <- ttq_out_tc_counts |>
  ggplot(aes(x=reorder(Call_Taker, -n), y=n, fill=factor(color_id))) +
  geom_bar(stat="identity") +
  scale_fill_manual(values = colorRampPalette(
    paletteer::paletteer_d("ggsci::blue_grey_material", n = 10))(n_takers)) +
  labs(title="Time To Queue Outliers by Call Taker",
       x="Call Taker",
       y="Number of Outlier Calls") +
  geom_text(
    aes(label = n),
    vjust = -0.5,
    size = 3.5) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size=8),
        axis.text.y = element_text(size=8),
        axis.title.x = element_text(size=10),
        axis.title.y = element_text(size=10),
        plot.margin = margin(5, 5, 5, 5))

ttq_out_barCallTaker

```

The graph shows that `r ttq_out_busiest_tc` had the most outlier calls with `r ttq_out_busiest_tc_count` calls. Overall there were a small number of call takers that had outlier values in getting a call sent for dispatch.

```{r}
#| label: ttd_outlier-graphs
#| echo: false
#| message: false
#| warning: false
#| fig.width: 8
#| fig.height: 5

ttd_out_disp_counts <- ttd_outliers |>
  count(Dispatcher, sort = TRUE) |>
  mutate(color_id = row_number())

ttd_out_max_disp_info <- ttd_out_disp_counts |> filter(n == max(n))
ttd_out_busiest_disp <- ttd_out_max_disp_info |> slice(1) |> pull(Dispatcher)
ttd_out_busiest_disp_count <- ttd_out_max_disp_info |> slice(1) |> pull(n)

# Get number of dispatchers for proper color scaling
n_dispatchers <- nrow(ttd_out_disp_counts)

ttd_out_barDispatcher <- ttd_out_disp_counts |>
  ggplot(aes(x=reorder(Dispatcher, -n), y=n, fill=factor(color_id))) +
  geom_bar(stat="identity") +
  scale_fill_manual(values = colorRampPalette(
    paletteer::paletteer_d("ggsci::blue_grey_material", n = 10))(n_dispatchers)) +
  labs(title="Time To Dispatch Outliers by Dispatcher",
       x="Dispatcher",
       y="Number of Outlier Calls") +
  geom_text(
    aes(label = n),
    vjust = -0.5,
    size = 3.5) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size=8),
        axis.text.y = element_text(size=8),
        axis.title.x = element_text(size=10),
        axis.title.y = element_text(size=10),
        plot.margin = margin(5, 5, 5, 5))

ttd_out_barDispatcher

```

This graph shows the distribution of calls where the time to dispatch the first unit was considered to be an outlier value. It is interesting to note that `r ttd_out_busiest_disp` has the most outlier values. We should carefully examine those calls to see how this has happened, so those calls do not fall through the cracks.

```{r}
#| label: ttd-outlier-agency-distribution
#| echo: false
#| message: false
#| warning: false
# ggplot2
ttd_out_agency_counts <- ttd_outliers |>
  count(Agency, sort = TRUE)

ttd_out_max_agency_info <- ttd_out_agency_counts |> filter(n == max(n))
ttd_out_busiest_agency <- ttd_out_max_agency_info |> slice(1) |> pull(Agency)
ttd_out_busiest_agency_count <- ttd_out_max_agency_info |> slice(1) |> pull(n)

# Calculate percentage for Police calls
ttd_out_police_percentage <- round((sum(ttd_outliers$Agency == "POLICE", na.rm = TRUE) / nrow(ttd_outliers)) * 100, 1)

ttd_out_barDiscipline <- ttd_outliers |> ggplot(aes(x=Agency, fill=Agency)) +
  geom_bar() +
  scale_fill_manual(
    values = c(POLICE = "#1f77b4", FIRE = "#d62728", EMS = "#2ca02c"),
    name = "Agency"
  ) +
  labs(title="Number of Calls for Service by Agency with Large Dispatch Times",
       x="Discipline",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    position = position_stack(vjust = 0.5),
    size = 3.5,
    color = "white",
    fontface = "bold"
  ) +
  theme_minimal() +
  theme(legend.position="right",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.title = element_text(size = 9),
        legend.text = element_text(size = 9))

ttd_out_barDiscipline

```

As expected, all of the calls that have an outlier in the time to dispatch the calls belonged to `r ttd_out_busiest_agency`. This was expected because AFD FIRE and EMS calls use auto-dispatch and rarely wait for more than a few seconds.

### Outlier Impact

Below is an illustration of the impact that the outliers that we've identified have on the overall numbers we've reported.

```{r}
#| label: outlier-summary
#| echo: false
#| message: false
#| warning: false

# Create row index column for joining
df_indexed <- df %>% mutate(.row_id = row_number())
outlier_df_indexed <- outlier_df %>% mutate(.row_id = row_number())

# Get the original row numbers from df that are in outlier_df
outlier_rows <- which(seq_len(nrow(df)) %in% as.integer(rownames(outlier_df)))

# Create non-outlier dataframe by excluding those rows
nonoutlier_df <- df[-outlier_rows, ]

# Initialize variables outside tryCatch so they're always available
summary_table <- NULL
median_processing_time_nout <- NA_real_
median_phone_time_nout <- NA_real_
median_time_to_queue_nout <- NA_real_
median_time_to_dispatch_nout <- NA_real_
median_ttq_nout <- NA_real_
median_ttd_nout <- NA_real_

# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time",
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(nonoutlier_df)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- nonoutlier_df %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

    # Extract key metrics for use in text (use <<- to assign to parent environment)
    median_time_to_queue_nout <<- summary_table |>
      filter(Variable == "Time To Queue") |>
      pull(Median)

    median_time_to_dispatch_nout <<- summary_table |>
      filter(Variable == "Time To Dispatch") |>
      pull(Median)

    median_processing_time_nout <<- summary_table |>
      filter(Variable == "Processing Time") |>
      pull(Median)

    median_phone_time_nout <<- summary_table |>
      filter(Variable == "Phone Time") |>
      pull(Median)

    median_ttq_nout <<- median_time_to_queue_nout
    median_ttd_nout <<- median_time_to_dispatch_nout

    to_ft(
      summary_table,
      caption = "Non-Outlier Elapsed Time Summary — Statistical summary of call processing times excluding outliers",
      header_map = list(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median",
        Std_Dev = "Std Dev",
        Skewness = "Skew",
        Kurtosis = "Kurt"
      ),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in nonoutlier_df:", paste(names(nonoutlier_df), collapse = ", "), "\n")
})
```

### Comparison: Overall vs. Outlier Statistics

For comparison purposes, here is the overall summary table again:

```{r}
#| label: overall-summary-comparison
#| echo: false
#| message: false
#| warning: false

# Display the overall summary table preserved from earlier
if (exists("summary_table_overall")) {
  to_ft(
    summary_table_overall,
    caption = "Overall Elapsed Time Summary — Statistical summary of all call processing times",
    header_map = list(
      Variable = "Time Metric",
      Minimum = "Min",
      Mean = "Mean",
      Median = "Median",
      Std_Dev = "Std Dev",
      Skewness = "Skew",
      Kurtosis = "Kurt"
    ),
    digits = 2
  )
}
```

It is interesting to see that removing the outliers had the opposite effect and seems to show that they do not have a significant impact on our summary statistics. This is likely because we have only removed `r nrow(outlier_df)` rows from the dataset when recalculating

After this summary table, we can measure the difference between the original medians and the medians after the `r nrow(outlier_df)` outlier rows were removed. This table shows that impact.

| Time Point | Overall Median | Post-Outlier Median | Impact in Seconds |
| :--------: | :------------: | :-----------------: | :---------------: |
| Time To Queue | `r median_time_to_queue_overall` | `r median_ttq_nout` | `r median_time_to_queue_overall - median_ttq_nout` |
| Time To Dispatch | `r median_time_to_dispatch_overall` | `r median_ttd_nout` | `r median_time_to_dispatch_overall - median_ttd_nout` |
| Phone Time | `r median_phone_time_overall` | `r median_phone_time_nout` | `r median_phone_time_overall - median_phone_time_nout` |
| Processing Time | `r median_processing_time_overall` | `r median_processing_time_nout` | `r median_processing_time_overall - median_processing_time_nout` |

There is little impact on the median times because the median value is less sensitive to the impact of outliers.

## Correlation Studies

This section examines associations between elapsed-time performance and key categorical factors using ANOVA-based effect sizes (correlation ratio/eta-squared) and multivariable models:

- TTQ ~ Call Taker (excluding C2C and NULL)
- TTD ~ Dispatcher (excluding Automatic Dispatch, CAD to CAD Service 1, NULL)
- TTQ ~ Call Type (`Problem`)
- TTD ~ Call Type (`Problem`)
- TTQ ~ Call Type + Call Taker (joint model)
- TTD ~ Call Type + Dispatcher (joint model)

We report eta-squared (η²) for one-way models and partial eta-squared (η²p) for multivariable models. The correlation-ratio is reported as r = sqrt(η² or η²p) for interpretability on a 0–1 scale.

```{r}
#| label: correlation-studies
#| echo: false
#| message: false
#| warning: false

suppressWarnings({
  # Helper to compute eta-squared (one-way) and partial eta-squared (multi)
  effect_table <- function(fit, partial = FALSE) {
    tab <- as.data.frame(summary(fit)[[1]])
    tab$Term <- rownames(tab)
    # Guard for missing pieces
    if (!("Sum Sq" %in% names(tab))) return(NULL)
    ss_res <- tryCatch(tab[tab$Term == "Residuals", "Sum Sq"], error = function(e) NA_real_)
    df_res <- tryCatch(tab[tab$Term == "Residuals", "Df"], error = function(e) NA_real_)
    # If Residuals missing, cannot compute; return NULL
    if (is.na(ss_res) || length(ss_res) == 0) return(NULL)
    total_ss <- sum(tab$`Sum Sq`, na.rm = TRUE)
    eff <- tab |>
      dplyr::filter(Term != "Residuals") |>
      dplyr::mutate(
        Eta2 = if (!partial) `Sum Sq` / total_ss else `Sum Sq` / (`Sum Sq` + ss_res),
        P_value = dplyr::if_else(!is.na(`Pr(>F)`), `Pr(>F)`, NA_real_),
        F_value = dplyr::if_else("F value" %in% names(tab), `F value`, NA_real_),
        Df1 = dplyr::if_else("Df" %in% names(tab), as.numeric(Df), NA_real_),
        Df2 = as.numeric(df_res)
      ) |>
      dplyr::select(Term, Df1, Df2, Eta2, F_value, P_value)
    eff
  }

  # Ensure a tibble has required columns even if empty
  ensure_cols <- function(tb, cols) {
    if (is.null(tb)) {
      tb <- tibble::tibble()
    }
    for (nm in cols) {
      if (!nm %in% names(tb)) tb[[nm]] <- NA_real_
    }
    tb[, cols, drop = FALSE]
  }
  # Safety: ensure numeric
  df <- df |> mutate(
    Time_To_Queue = suppressWarnings(as.numeric(Time_To_Queue)),
    Time_To_Dispatch = suppressWarnings(as.numeric(Time_To_Dispatch))
  )

  # Filters for non-person/system entries
  non_person_dispatch <- c("Automatic Dispatch", "CAD to CAD Service 1", "NULL")
  non_person_taker <- c("C2C", "NULL")

  # Minimum group size threshold for ANOVA and rare-level handling
  min_group_n <- 5
  # Keep top-N levels before enforcing minimum size (more aggressive lumping)
  keep_top_n_ct <- 10
  keep_top_n_disp <- 10
  keep_top_n_prob <- 15

  print_diag <- function(title, eligible_rows, pre_levels, post_levels, top_counts) {
    cat("\n— Correlation diagnostics:", title, "—\n")
    cat("Eligible rows:", eligible_rows, "\n")
    cat("Distinct levels (pre → post):", pre_levels, "→", post_levels, " (min group:", min_group_n, ")\n")
    if (!is.null(top_counts) && nrow(top_counts) > 0) {
      cat("Top levels after lumping:\n")
      utils::capture.output(print(dplyr::slice_head(top_counts, n = 5)), file = '')
    }
  }

  # One-way: TTQ ~ Call_Taker (exclude C2C/NULL)
  df_ct <- df |> filter(!is.na(Time_To_Queue), !is.na(Call_Taker)) |> 
    filter(!(Call_Taker %in% non_person_taker))
  pre_levels_ct <- dplyr::n_distinct(df_ct$Call_Taker)
  if (nrow(df_ct) > 0) {
    df_ct$Call_Taker <- df_ct$Call_Taker |>
      forcats::fct_lump_n(n = keep_top_n_ct, other_level = "Other") |>
      forcats::fct_lump_min(min = min_group_n, other_level = "Other")
    df_ct <- droplevels(df_ct)
  }
  post_levels_ct <- if (nrow(df_ct) > 0) nlevels(df_ct$Call_Taker) else 0
  top_ct_counts <- if (nrow(df_ct) > 0) df_ct |> count(Call_Taker, sort = TRUE) else NULL
  es_ttq_ct <- NULL
  if (nrow(df_ct) > 0 && dplyr::n_distinct(df_ct$Call_Taker) > 1) {
    fit_ct <- aov(Time_To_Queue ~ Call_Taker, data = df_ct)
    es_ttq_ct <- effect_table(fit_ct, partial = FALSE)
  } else {
    print_diag("TTQ ~ Call Taker", nrow(df_ct), pre_levels_ct, post_levels_ct, top_ct_counts)
  }

  # One-way: TTD ~ Dispatcher (exclude system/NULL)
  df_disp <- df |> filter(!is.na(Time_To_Dispatch), !is.na(Dispatcher)) |> 
    filter(!(Dispatcher %in% non_person_dispatch))
  pre_levels_disp <- dplyr::n_distinct(df_disp$Dispatcher)
  if (nrow(df_disp) > 0) {
    df_disp$Dispatcher <- df_disp$Dispatcher |>
      forcats::fct_lump_n(n = keep_top_n_disp, other_level = "Other") |>
      forcats::fct_lump_min(min = min_group_n, other_level = "Other")
    df_disp <- droplevels(df_disp)
  }
  post_levels_disp <- if (nrow(df_disp) > 0) nlevels(df_disp$Dispatcher) else 0
  top_disp_counts <- if (nrow(df_disp) > 0) df_disp |> count(Dispatcher, sort = TRUE) else NULL
  es_ttd_disp <- NULL
  if (nrow(df_disp) > 0 && dplyr::n_distinct(df_disp$Dispatcher) > 1) {
    fit_disp <- aov(Time_To_Dispatch ~ Dispatcher, data = df_disp)
    es_ttd_disp <- effect_table(fit_disp, partial = FALSE)
  } else {
    print_diag("TTD ~ Dispatcher", nrow(df_disp), pre_levels_disp, post_levels_disp, top_disp_counts)
  }

  # One-way: TTQ ~ Call Type (Problem)
  df_prob_ttq <- df |> filter(!is.na(Time_To_Queue), !is.na(Problem))
  pre_levels_prob_ttq <- dplyr::n_distinct(df_prob_ttq$Problem)
  if (nrow(df_prob_ttq) > 0) {
    df_prob_ttq$Problem <- df_prob_ttq$Problem |>
      forcats::fct_lump_n(n = keep_top_n_prob, other_level = "Other") |>
      forcats::fct_lump_min(min = min_group_n, other_level = "Other")
    df_prob_ttq <- droplevels(df_prob_ttq)
  }
  post_levels_prob_ttq <- if (nrow(df_prob_ttq) > 0) nlevels(df_prob_ttq$Problem) else 0
  top_prob_ttq_counts <- if (nrow(df_prob_ttq) > 0) df_prob_ttq |> count(Problem, sort = TRUE) else NULL
  es_ttq_type <- NULL
  if (nrow(df_prob_ttq) > 0 && dplyr::n_distinct(df_prob_ttq$Problem) > 1) {
    fit_ttq_type <- aov(Time_To_Queue ~ Problem, data = df_prob_ttq)
    es_ttq_type <- effect_table(fit_ttq_type, partial = FALSE)
  } else {
    print_diag("TTQ ~ Call Type (Problem)", nrow(df_prob_ttq), pre_levels_prob_ttq, post_levels_prob_ttq, top_prob_ttq_counts)
  }

  # One-way: TTD ~ Call Type (Problem)
  df_prob_ttd <- df |> filter(!is.na(Time_To_Dispatch), !is.na(Problem))
  pre_levels_prob_ttd <- dplyr::n_distinct(df_prob_ttd$Problem)
  if (nrow(df_prob_ttd) > 0) {
    df_prob_ttd$Problem <- df_prob_ttd$Problem |>
      forcats::fct_lump_n(n = keep_top_n_prob, other_level = "Other") |>
      forcats::fct_lump_min(min = min_group_n, other_level = "Other")
    df_prob_ttd <- droplevels(df_prob_ttd)
  }
  post_levels_prob_ttd <- if (nrow(df_prob_ttd) > 0) nlevels(df_prob_ttd$Problem) else 0
  top_prob_ttd_counts <- if (nrow(df_prob_ttd) > 0) df_prob_ttd |> count(Problem, sort = TRUE) else NULL
  es_ttd_type <- NULL
  if (nrow(df_prob_ttd) > 0 && dplyr::n_distinct(df_prob_ttd$Problem) > 1) {
    fit_ttd_type <- aov(Time_To_Dispatch ~ Problem, data = df_prob_ttd)
    es_ttd_type <- effect_table(fit_ttd_type, partial = FALSE)
  } else {
    print_diag("TTD ~ Call Type (Problem)", nrow(df_prob_ttd), pre_levels_prob_ttd, post_levels_prob_ttd, top_prob_ttd_counts)
  }

  # Multivariable: TTQ ~ Problem + Call_Taker (exclude non-person takers)
  es_ttq_multi <- NULL; ttq_adj_r2 <- NA_real_
  df_ttq_multi <- df |> filter(!is.na(Time_To_Queue), !is.na(Problem), !is.na(Call_Taker)) |>
    filter(!(Call_Taker %in% non_person_taker))
  pre_prob_levels_ttq <- dplyr::n_distinct(df_ttq_multi$Problem)
  pre_ct_levels_ttq <- dplyr::n_distinct(df_ttq_multi$Call_Taker)
  if (nrow(df_ttq_multi) > 0) {
    df_ttq_multi$Problem <- df_ttq_multi$Problem |>
      forcats::fct_lump_n(n = keep_top_n_prob, other_level = "Other") |>
      forcats::fct_lump_min(min = min_group_n, other_level = "Other")
    df_ttq_multi$Call_Taker <- df_ttq_multi$Call_Taker |>
      forcats::fct_lump_n(n = keep_top_n_ct, other_level = "Other") |>
      forcats::fct_lump_min(min = min_group_n, other_level = "Other")
    df_ttq_multi <- droplevels(df_ttq_multi)
  }
  post_prob_levels_ttq <- if (nrow(df_ttq_multi) > 0) nlevels(df_ttq_multi$Problem) else 0
  post_ct_levels_ttq <- if (nrow(df_ttq_multi) > 0) nlevels(df_ttq_multi$Call_Taker) else 0
  if (nrow(df_ttq_multi) > 0 && post_prob_levels_ttq > 1 && post_ct_levels_ttq > 1) {
    fit_ttq_multi <- aov(Time_To_Queue ~ Problem + Call_Taker, data = df_ttq_multi)
    es_ttq_multi <- effect_table(fit_ttq_multi, partial = TRUE)
    # R^2 from linear model equivalent
    lm_ttq <- lm(Time_To_Queue ~ Problem + Call_Taker, data = df_ttq_multi)
    ttq_adj_r2 <- tryCatch(summary(lm_ttq)$adj.r.squared, error=function(e) NA_real_)
  } else {
    print_diag("TTQ ~ Problem + Call Taker", nrow(df_ttq_multi), paste0(pre_prob_levels_ttq, "/", pre_ct_levels_ttq), paste0(post_prob_levels_ttq, "/", post_ct_levels_ttq), NULL)
  }

  # Multivariable: TTD ~ Problem + Dispatcher (exclude system dispatch)
  es_ttd_multi <- NULL; ttd_adj_r2 <- NA_real_
  df_ttd_multi <- df |> filter(!is.na(Time_To_Dispatch), !is.na(Problem), !is.na(Dispatcher)) |>
    filter(!(Dispatcher %in% non_person_dispatch))
  pre_prob_levels_ttd <- dplyr::n_distinct(df_ttd_multi$Problem)
  pre_disp_levels_ttd <- dplyr::n_distinct(df_ttd_multi$Dispatcher)
  if (nrow(df_ttd_multi) > 0) {
    df_ttd_multi$Problem <- df_ttd_multi$Problem |>
      forcats::fct_lump_n(n = keep_top_n_prob, other_level = "Other") |>
      forcats::fct_lump_min(min = min_group_n, other_level = "Other")
    df_ttd_multi$Dispatcher <- df_ttd_multi$Dispatcher |>
      forcats::fct_lump_n(n = keep_top_n_disp, other_level = "Other") |>
      forcats::fct_lump_min(min = min_group_n, other_level = "Other")
    df_ttd_multi <- droplevels(df_ttd_multi)
  }
  post_prob_levels_ttd <- if (nrow(df_ttd_multi) > 0) nlevels(df_ttd_multi$Problem) else 0
  post_disp_levels_ttd <- if (nrow(df_ttd_multi) > 0) nlevels(df_ttd_multi$Dispatcher) else 0
  if (nrow(df_ttd_multi) > 0 && post_prob_levels_ttd > 1 && post_disp_levels_ttd > 1) {
    fit_ttd_multi <- aov(Time_To_Dispatch ~ Problem + Dispatcher, data = df_ttd_multi)
    es_ttd_multi <- effect_table(fit_ttd_multi, partial = TRUE)
    lm_ttd <- lm(Time_To_Dispatch ~ Problem + Dispatcher, data = df_ttd_multi)
    ttd_adj_r2 <- tryCatch(summary(lm_ttd)$adj.r.squared, error=function(e) NA_real_)
  } else {
    print_diag("TTD ~ Problem + Dispatcher", nrow(df_ttd_multi), paste0(pre_prob_levels_ttd, "/", pre_disp_levels_ttd), paste0(post_prob_levels_ttd, "/", post_disp_levels_ttd), NULL)
  }

  # Formatting helpers
  fmt <- function(x) ifelse(is.na(x), "—", format(round(x, 3), nsmall = 3, trim = TRUE, scientific = FALSE))
  fmt0 <- function(x) ifelse(is.na(x), "—", format(round(x, 0), nsmall = 0, trim = TRUE, scientific = FALSE))
  to_corr_ratio <- function(eta2) { ifelse(is.na(eta2), NA_real_, sqrt(eta2)) }

  # Build display tables
  # Build robustly in pieces to avoid missing-column errors
  ow_cols <- c("Model","Term","Df1","Df2","Eta2","Corr_Ratio","F_value","P_value")
  ow_list <- list()
  if (!is.null(es_ttq_ct) && nrow(es_ttq_ct) > 0) {
    ow_list[[length(ow_list)+1]] <- tibble::tibble(
      Model = "TTQ ~ Call Taker",
      Term = es_ttq_ct$Term,
      Df1 = es_ttq_ct$Df1,
      Df2 = es_ttq_ct$Df2,
      Eta2 = es_ttq_ct$Eta2,
      F_value = es_ttq_ct$F_value,
      Corr_Ratio = to_corr_ratio(es_ttq_ct$Eta2),
      P_value = es_ttq_ct$P_value
    )
  }
  if (!is.null(es_ttd_disp) && nrow(es_ttd_disp) > 0) {
    ow_list[[length(ow_list)+1]] <- tibble::tibble(
      Model = "TTD ~ Dispatcher",
      Term = es_ttd_disp$Term,
      Df1 = es_ttd_disp$Df1,
      Df2 = es_ttd_disp$Df2,
      Eta2 = es_ttd_disp$Eta2,
      F_value = es_ttd_disp$F_value,
      Corr_Ratio = to_corr_ratio(es_ttd_disp$Eta2),
      P_value = es_ttd_disp$P_value
    )
  }
  if (!is.null(es_ttq_type) && nrow(es_ttq_type) > 0) {
    ow_list[[length(ow_list)+1]] <- tibble::tibble(
      Model = "TTQ ~ Call Type",
      Term = es_ttq_type$Term,
      Df1 = es_ttq_type$Df1,
      Df2 = es_ttq_type$Df2,
      Eta2 = es_ttq_type$Eta2,
      F_value = es_ttq_type$F_value,
      Corr_Ratio = to_corr_ratio(es_ttq_type$Eta2),
      P_value = es_ttq_type$P_value
    )
  }
  if (!is.null(es_ttd_type) && nrow(es_ttd_type) > 0) {
    ow_list[[length(ow_list)+1]] <- tibble::tibble(
      Model = "TTD ~ Call Type",
      Term = es_ttd_type$Term,
      Df1 = es_ttd_type$Df1,
      Df2 = es_ttd_type$Df2,
      Eta2 = es_ttd_type$Eta2,
      F_value = es_ttd_type$F_value,
      Corr_Ratio = to_corr_ratio(es_ttd_type$Eta2),
      P_value = es_ttd_type$P_value
    )
  }
  one_way_tbl <- if (length(ow_list)) dplyr::bind_rows(ow_list) else tibble::tibble()
  one_way_tbl <- ensure_cols(one_way_tbl, ow_cols)
  one_way_tbl <- one_way_tbl |>
    dplyr::mutate(
      Eta2 = as.numeric(Eta2),
      Corr_Ratio = as.numeric(Corr_Ratio)
    )

  # Significance stars helper
  p_to_stars <- function(p) {
    if (is.na(p)) return("")
    if (p < 0.001) return("***")
    if (p < 0.01)  return("**")
    if (p < 0.05)  return("*")
    if (p < 0.1)   return(".")
    return("")
  }
  if (nrow(one_way_tbl) > 0) {
    one_way_tbl <- one_way_tbl |>
      dplyr::mutate(Sig = vapply(P_value, p_to_stars, character(1)))
  }

  multi_tbl <- NULL
  if (!is.null(es_ttq_multi)) {
    es_ttq_multi <- es_ttq_multi |> dplyr::filter(Term %in% c("Problem", "Call_Taker")) |>
      dplyr::mutate(Model = "TTQ ~ Call Type + Call Taker",
                    Corr_Ratio = to_corr_ratio(Eta2),
                    Eta2_partial = Eta2,
                    Adj_R2 = ttq_adj_r2)
    multi_tbl <- dplyr::bind_rows(multi_tbl, es_ttq_multi)
  }
  if (!is.null(es_ttd_multi)) {
    es_ttd_multi <- es_ttd_multi |> dplyr::filter(Term %in% c("Problem", "Dispatcher")) |>
      dplyr::mutate(Model = "TTD ~ Call Type + Dispatcher",
                    Corr_Ratio = to_corr_ratio(Eta2),
                    Eta2_partial = Eta2,
                    Adj_R2 = ttd_adj_r2)
    multi_tbl <- dplyr::bind_rows(multi_tbl, es_ttd_multi)
  }

  # Add significance to multivariable terms if available
  if (!is.null(multi_tbl) && nrow(multi_tbl) > 0) {
    if (!("P_value" %in% names(multi_tbl))) multi_tbl$P_value <- NA_real_
    if (!("F_value" %in% names(multi_tbl))) multi_tbl$F_value <- NA_real_
    if (!("Df1" %in% names(multi_tbl))) multi_tbl$Df1 <- NA_real_
    if (!("Df2" %in% names(multi_tbl))) multi_tbl$Df2 <- NA_real_
    multi_tbl <- multi_tbl |>
      dplyr::mutate(Sig = vapply(P_value, p_to_stars, character(1)))
  }
})

# Render tables outside suppressWarnings to ensure output
if (nrow(one_way_tbl) > 0) {
  to_ft(
    one_way_tbl |> dplyr::mutate(
      Eta2 = fmt(Eta2),
      Corr_Ratio = fmt(Corr_Ratio),
      F_value = fmt(F_value),
      P_value = fmt(P_value)
    ),
    caption = "One-Way Effect Sizes (η²) and Correlation Ratios (r)",
    header_map = list(
      Model = "Model",
      Term = "Term",
      Eta2 = "η²",
      Corr_Ratio = "r",
      F_value = "F",
      P_value = "p-value",
      Sig = "Sig"
    ),
    digits = 3
  )
} else {
  cat("No one-way correlation results available.\n")
}

if (!is.null(multi_tbl) && nrow(multi_tbl) > 0) {
  to_ft(
    multi_tbl |> dplyr::transmute(
      Model,
      Term,
      Eta2p = fmt(Eta2_partial),
      r = fmt(Corr_Ratio),
      F_value = fmt(F_value),
      P_value = fmt(P_value),
      Sig,
      Adj_R2 = fmt(Adj_R2)
    ),
    caption = "Multivariable Effect Sizes (partial η²) with Adjusted R²",
    header_map = list(
      Model = "Model",
      Term = "Term",
      Eta2p = "η²p",
      r = "r",
      F_value = "F",
      P_value = "p-value",
      Sig = "Sig",
      Adj_R2 = "Adj. R²"
    ),
    digits = 3
  )
} else {
  cat("No multivariable correlation results available.\n")
}
```

Key takeaways from these results:

- Higher r (closer to 1) indicates a stronger association between the time metric and the categorical factor.
- η² (one-way) and η²p (multivariable) quantify variance explained by the factor(s).
- The multivariable adjusted R² summarizes the joint explanatory power of Call Type and personnel.

```{r}
#| label: correlation-heatmap
#| echo: false
#| message: false
#| warning: false
#| fig.width: 6
#| fig.height: 3.5

hm_data <- dplyr::bind_rows(
  if (exists("one_way_tbl") && nrow(one_way_tbl)>0) one_way_tbl |>
    dplyr::transmute(Model, Term, r = Corr_Ratio, Df1 = Df1, Df2 = Df2, F_value = F_value, P_value = P_value, Type = "One-way"),
  if (exists("multi_tbl") && !is.null(multi_tbl) && nrow(multi_tbl)>0) multi_tbl |>
    dplyr::transmute(Model, Term, r = Corr_Ratio, Df1 = Df1, Df2 = Df2, F_value = F_value, P_value = P_value, Type = "Multivariable")
)

if (!is.null(hm_data) && nrow(hm_data) > 0) {
  # Build hover text with Df, F, p
  hm_data <- hm_data |>
    dplyr::mutate(
      hover_txt = paste0(
        "Model: ", Model,
        "<br>Term: ", Term,
        "<br>Type: ", Type,
        "<br>r: ", ifelse(is.na(r), "—", sprintf("%.3f", r)),
        ifelse(!is.na(Df1) & !is.na(Df2), paste0("<br>Df: ", format(round(Df1), nsmall = 0), ", ", format(round(Df2), nsmall = 0)), ""),
        ifelse(!is.na(F_value), paste0("<br>F: ", sprintf("%.3f", F_value)), ""),
        ifelse(!is.na(P_value), paste0("<br>p: ", sprintf("%.3f", P_value)), "")
      )
    )

  p <- ggplot(hm_data, aes(x = Term, y = Model, fill = r, text = hover_txt)) +
    geom_tile(color = "white") +
    geom_text(aes(label = ifelse(is.na(r), "—", sprintf("%.2f", r))), size = 3) +
    scale_fill_gradient(low = "#e8f1fa", high = "#2c7fb8", na.value = "#f0f0f0") +
    facet_wrap(~Type, nrow = 1, scales = "free_x") +
    labs(title = "Correlation Ratio (r) by Model and Term",
         x = "Term",
         y = NULL,
         fill = "r") +
    theme_minimal(base_size = 11) +
    theme(panel.grid = element_blank(),
          axis.text.x = element_text(angle = 45, hjust = 1))

  plotly::ggplotly(p, tooltip = "text")
}
```

```{r}
#| label: correlation-summary-text
#| echo: false
#| message: false
#| warning: false

top_n <- 3
summarize_top <- function(tbl, lbl) {
  if (is.null(tbl) || nrow(tbl) == 0) return(NULL)
  tbl |> dplyr::arrange(dplyr::desc(Corr_Ratio)) |>
    dplyr::slice_head(n = top_n) |>
    dplyr::mutate(r_txt = sprintf("%.2f", Corr_Ratio)) |>
    dplyr::transmute(txt = paste0("- ", lbl, ": ", Model, " — ", Term, " (r=", r_txt, ifelse(!is.na(Sig) & Sig != "", paste0(", ", Sig), ""), ")")) |>
    dplyr::pull(txt)
}

ow_lines <- if (exists("one_way_tbl") && nrow(one_way_tbl)>0) summarize_top(one_way_tbl, "One-way") else character(0)
mv_lines <- if (exists("multi_tbl") && !is.null(multi_tbl) && nrow(multi_tbl)>0) summarize_top(multi_tbl, "Multivariable") else character(0)

if (length(ow_lines) + length(mv_lines) > 0) {
  cat("\n**Strongest Associations (top r):**\n\n")
  for (ln in c(ow_lines, mv_lines)) cat(ln, "\n")
}
```

## Time to Queue and Dispatch Scatter Analysis

This section examines the relationship between Time to Queue (TTQ) and Time to Dispatch (TTD) through scatter plot visualizations. We analyze this relationship across different dimensions: overall patterns, by discipline (POLICE, FIRE, EMS), by priority levels (1-4), and for 9-1-1 calls specifically.

### Overall TTQ vs TTD Relationship

```{r}
#| label: ttq-ttd-overall-scatter
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Overall relationship between Time to Queue and Time to Dispatch"
#| fig-width: 10
#| fig-height: 6

# Prepare data for scatter plot
scatter_data <- df %>%
  filter(!is.na(Time_To_Queue), !is.na(Time_To_Dispatch)) %>%
  filter(Time_To_Queue >= 0, Time_To_Dispatch >= 0) %>%
  # Cap extreme outliers for visualization clarity
  mutate(
    TTQ_capped = pmin(Time_To_Queue, 300),
    TTD_capped = pmin(Time_To_Dispatch, 600)
  )

# Calculate correlation
overall_cor <- cor(scatter_data$Time_To_Queue, scatter_data$Time_To_Dispatch, 
                   use = "complete.obs", method = "spearman")

# Calculate medians for reference lines
median_ttq <- median(scatter_data$Time_To_Queue, na.rm = TRUE)
median_ttd <- median(scatter_data$Time_To_Dispatch, na.rm = TRUE)

# Create scatter plot with density
p_overall <- ggplot(scatter_data, aes(x = TTQ_capped, y = TTD_capped)) +
  geom_hex(bins = 50) +
  geom_vline(xintercept = median_ttq, color = "red", linetype = "dashed", alpha = 0.7) +
  geom_hline(yintercept = median_ttd, color = "red", linetype = "dashed", alpha = 0.7) +
  geom_smooth(method = "lm", color = "blue", se = TRUE, alpha = 0.3) +
  scale_fill_viridis_c(name = "Count", trans = "log10") +
  labs(
    title = "Time to Queue vs Time to Dispatch - Overall Pattern",
    subtitle = paste0("Spearman correlation: ", round(overall_cor, 3), 
                     " | Medians shown as red dashed lines"),
    x = "Time to Queue (seconds, capped at 300)",
    y = "Time to Dispatch (seconds, capped at 600)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 9)
  )

p_overall
```

**Overall Analysis:**

- **Correlation**: The Spearman correlation between TTQ and TTD is `r round(overall_cor, 3)`, indicating `r if(abs(overall_cor) > 0.5) "a moderate to strong" else if(abs(overall_cor) > 0.3) "a moderate" else "a weak"` relationship.
- **Pattern**: `r if(overall_cor > 0.3) "Calls with longer queue times tend to have longer dispatch times, suggesting systemic delays affect both metrics." else "The weak correlation suggests TTQ and TTD are largely independent, with different factors influencing each."` 
- **Median Performance**: The median TTQ is `r round(median_ttq, 1)` seconds and median TTD is `r round(median_ttd, 1)` seconds.

### TTQ vs TTD by Discipline

```{r}
#| label: ttq-ttd-discipline-scatter
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Time to Queue vs Time to Dispatch by Discipline"
#| fig-width: 12
#| fig-height: 5

# Filter to main disciplines
discipline_data <- scatter_data %>%
  filter(Agency %in% c("POLICE", "FIRE", "EMS"))

# Calculate correlations by discipline
discipline_cors <- discipline_data %>%
  group_by(Agency) %>%
  summarise(
    correlation = cor(Time_To_Queue, Time_To_Dispatch, 
                     use = "complete.obs", method = "spearman"),
    n = n(),
    median_ttq = median(Time_To_Queue, na.rm = TRUE),
    median_ttd = median(Time_To_Dispatch, na.rm = TRUE),
    .groups = 'drop'
  )

# Create faceted scatter plot
p_discipline <- ggplot(discipline_data, aes(x = TTQ_capped, y = TTD_capped, color = Agency)) +
  geom_point(alpha = 0.3, size = 1) +
  geom_smooth(method = "lm", se = TRUE, alpha = 0.2) +
  facet_wrap(~ Agency, ncol = 3) +
  scale_color_manual(
    values = c("POLICE" = "#2c7fb8", "FIRE" = "#e34a33", "EMS" = "#31a354")
  ) +
  labs(
    title = "Time to Queue vs Time to Dispatch by Discipline",
    subtitle = "Linear trend lines shown for each discipline",
    x = "Time to Queue (seconds, capped at 300)",
    y = "Time to Dispatch (seconds, capped at 600)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 9),
    legend.position = "none"
  )

p_discipline

# Create summary table
to_ft(
  discipline_cors %>%
    mutate(
      correlation = round(correlation, 3),
      median_ttq = round(median_ttq, 1),
      median_ttd = round(median_ttd, 1)
    ),
  caption = "TTQ-TTD Correlation and Performance by Discipline",
  header_map = list(
    Agency = "Discipline",
    correlation = "Correlation (ρ)",
    n = "Call Count",
    median_ttq = "Median TTQ (s)",
    median_ttd = "Median TTD (s)"
  ),
  digits = 3
)
```

**Discipline-Specific Insights:**

```{r}
#| label: discipline-insights
#| echo: false
#| results: asis

for (i in 1:nrow(discipline_cors)) {
  agency <- discipline_cors$Agency[i]
  cor_val <- discipline_cors$correlation[i]
  med_ttq <- discipline_cors$median_ttq[i]
  med_ttd <- discipline_cors$median_ttd[i]
  
  strength <- if(abs(cor_val) > 0.5) "strong" else if(abs(cor_val) > 0.3) "moderate" else "weak"
  
  cat(paste0("- **", agency, "**: Shows a ", strength, " correlation (ρ = ", 
             round(cor_val, 3), ") with median TTQ of ", round(med_ttq, 1), 
             "s and median TTD of ", round(med_ttd, 1), "s.\n"))
}
```

### TTQ vs TTD by Priority Level

```{r}
#| label: ttq-ttd-priority-scatter
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Time to Queue vs Time to Dispatch by Priority Level"
#| fig-width: 12
#| fig-height: 8

# Filter to priority levels 1-4
priority_data <- scatter_data %>%
  filter(Priority_Number %in% c("1", "2", "3", "4"))

# Calculate correlations by priority
priority_cors <- priority_data %>%
  group_by(Priority_Number) %>%
  summarise(
    correlation = cor(Time_To_Queue, Time_To_Dispatch, 
                     use = "complete.obs", method = "spearman"),
    n = n(),
    median_ttq = median(Time_To_Queue, na.rm = TRUE),
    median_ttd = median(Time_To_Dispatch, na.rm = TRUE),
    p90_ttq = quantile(Time_To_Queue, 0.9, na.rm = TRUE),
    p90_ttd = quantile(Time_To_Dispatch, 0.9, na.rm = TRUE),
    .groups = 'drop'
  )

# Create faceted scatter plot by priority
p_priority <- ggplot(priority_data, aes(x = TTQ_capped, y = TTD_capped, color = Priority_Number)) +
  geom_point(alpha = 0.3, size = 1) +
  geom_smooth(method = "lm", se = TRUE, alpha = 0.2) +
  facet_wrap(~ Priority_Number, ncol = 2, 
             labeller = labeller(Priority_Number = function(x) paste("Priority", x))) +
  scale_color_viridis_d(option = "plasma") +
  labs(
    title = "Time to Queue vs Time to Dispatch by Priority Level",
    subtitle = "Priority 1 = Highest urgency; Priority 4 = Routine",
    x = "Time to Queue (seconds, capped at 300)",
    y = "Time to Dispatch (seconds, capped at 600)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 9),
    legend.position = "none"
  )

p_priority

# Create summary table
to_ft(
  priority_cors %>%
    mutate(
      correlation = round(correlation, 3),
      median_ttq = round(median_ttq, 1),
      median_ttd = round(median_ttd, 1),
      p90_ttq = round(p90_ttq, 1),
      p90_ttd = round(p90_ttd, 1)
    ),
  caption = "TTQ-TTD Correlation and Performance by Priority Level",
  header_map = list(
    Priority_Number = "Priority",
    correlation = "Correlation (ρ)",
    n = "Call Count",
    median_ttq = "Median TTQ (s)",
    median_ttd = "Median TTD (s)",
    p90_ttq = "P90 TTQ (s)",
    p90_ttd = "P90 TTD (s)"
  ),
  digits = 3
)
```

**Priority-Level Insights:**

```{r}
#| label: priority-insights
#| echo: false
#| results: asis

for (i in 1:nrow(priority_cors)) {
  pri <- priority_cors$Priority_Number[i]
  cor_val <- priority_cors$correlation[i]
  med_ttq <- priority_cors$median_ttq[i]
  med_ttd <- priority_cors$median_ttd[i]
  p90_ttq <- priority_cors$p90_ttq[i]
  p90_ttd <- priority_cors$p90_ttd[i]
  
  strength <- if(abs(cor_val) > 0.5) "strong" else if(abs(cor_val) > 0.3) "moderate" else "weak"
  
  cat(paste0("- **Priority ", pri, "**: ", strength, " correlation (ρ = ", 
             round(cor_val, 3), "). Median performance: TTQ = ", round(med_ttq, 1), 
             "s, TTD = ", round(med_ttd, 1), "s. 90th percentile: TTQ = ", 
             round(p90_ttq, 1), "s, TTD = ", round(p90_ttd, 1), "s.\n"))
}
```

### TTQ vs TTD for 9-1-1 Calls

```{r}
#| label: ttq-ttd-911-scatter
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Time to Queue vs Time to Dispatch for 9-1-1 Calls"
#| fig-width: 10
#| fig-height: 6

# Filter to E-911 calls
e911_data <- scatter_data %>%
  filter(Call_Reception == "E-911")

# Calculate correlation for 9-1-1 calls
e911_cor <- cor(e911_data$Time_To_Queue, e911_data$Time_To_Dispatch, 
                use = "complete.obs", method = "spearman")

# Calculate medians
e911_median_ttq <- median(e911_data$Time_To_Queue, na.rm = TRUE)
e911_median_ttd <- median(e911_data$Time_To_Dispatch, na.rm = TRUE)

# Compare with non-911 calls
non911_data <- scatter_data %>%
  filter(Call_Reception != "E-911")

non911_cor <- cor(non911_data$Time_To_Queue, non911_data$Time_To_Dispatch, 
                  use = "complete.obs", method = "spearman")

# Create scatter plot for 9-1-1 calls
p_911 <- ggplot(e911_data, aes(x = TTQ_capped, y = TTD_capped)) +
  geom_hex(bins = 50) +
  geom_vline(xintercept = e911_median_ttq, color = "red", linetype = "dashed", alpha = 0.7) +
  geom_hline(yintercept = e911_median_ttd, color = "red", linetype = "dashed", alpha = 0.7) +
  geom_smooth(method = "lm", color = "blue", se = TRUE, alpha = 0.3) +
  scale_fill_viridis_c(name = "Count", trans = "log10") +
  labs(
    title = "Time to Queue vs Time to Dispatch for 9-1-1 Calls",
    subtitle = paste0("Spearman correlation: ", round(e911_cor, 3), 
                     " | n = ", format(nrow(e911_data), big.mark = ","), " calls"),
    x = "Time to Queue (seconds, capped at 300)",
    y = "Time to Dispatch (seconds, capped at 600)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 9)
  )

p_911

# Create comparison summary
comparison_summary <- tibble(
  Call_Type = c("9-1-1 Calls", "Non-9-1-1 Calls"),
  n = c(nrow(e911_data), nrow(non911_data)),
  correlation = c(e911_cor, non911_cor),
  median_ttq = c(e911_median_ttq, median(non911_data$Time_To_Queue, na.rm = TRUE)),
  median_ttd = c(e911_median_ttd, median(non911_data$Time_To_Dispatch, na.rm = TRUE))
)

to_ft(
  comparison_summary %>%
    mutate(
      n = format(n, big.mark = ","),
      correlation = round(correlation, 3),
      median_ttq = round(median_ttq, 1),
      median_ttd = round(median_ttd, 1)
    ),
  caption = "TTQ-TTD Performance: 9-1-1 vs Non-9-1-1 Calls",
  header_map = list(
    Call_Type = "Call Type",
    n = "Call Count",
    correlation = "Correlation (ρ)",
    median_ttq = "Median TTQ (s)",
    median_ttd = "Median TTD (s)"
  ),
  digits = 3
)
```

**9-1-1 Call Analysis:**

- **Volume**: 9-1-1 calls represent `r round(nrow(e911_data)/nrow(scatter_data)*100, 1)`% of all calls this week.
- **Correlation**: The TTQ-TTD correlation for 9-1-1 calls is `r round(e911_cor, 3)`, which is `r if(abs(e911_cor) > abs(overall_cor)) "stronger than" else if(abs(e911_cor) < abs(overall_cor)) "weaker than" else "similar to"` the overall correlation.
- **Performance**: 9-1-1 calls show `r if(e911_median_ttq < median_ttq) "faster" else "slower"` median TTQ (`r round(e911_median_ttq, 1)`s vs `r round(median_ttq, 1)`s overall) and `r if(e911_median_ttd < median_ttd) "faster" else "slower"` median TTD (`r round(e911_median_ttd, 1)`s vs `r round(median_ttd, 1)`s overall).

### Key Findings Summary

```{r}
#| label: scatter-summary-insights
#| echo: false
#| results: asis

# Identify discipline with strongest correlation
strongest_discipline <- discipline_cors %>%
  arrange(desc(abs(correlation))) %>%
  slice(1)

# Identify priority with fastest performance
fastest_priority <- priority_cors %>%
  arrange(median_ttd) %>%
  slice(1)

cat("**Key Takeaways from TTQ-TTD Scatter Analysis:**\n\n")
cat(paste0("1. **Strongest Discipline Correlation**: ", strongest_discipline$Agency, 
           " shows the strongest relationship (ρ = ", round(strongest_discipline$correlation, 3), 
           "), suggesting ", if(strongest_discipline$correlation > 0.3) "systemic delays impact both queue and dispatch times." else "queue and dispatch times are largely independent.", "\n\n"))

cat(paste0("2. **Fastest Priority Response**: Priority ", fastest_priority$Priority_Number,
           " demonstrates the fastest median dispatch time (", round(fastest_priority$median_ttd, 1), 
           "s), reflecting appropriate urgency-based processing.\n\n"))

cat(paste0("3. **9-1-1 Performance**: 9-1-1 calls ",
           if(e911_median_ttd < median_ttd) "outperform" else "underperform",
           " the overall median by ",
           abs(round(e911_median_ttd - median_ttd, 1)),
           " seconds in TTD, indicating ",
           if(e911_median_ttd < median_ttd) "effective prioritization of emergency calls." else "potential process improvements needed for emergency response.", "\n\n"))

# Identify any performance concerns
high_p90_priorities <- priority_cors %>%
  filter(p90_ttd > 180) # Flag if 90th percentile > 3 minutes

if(nrow(high_p90_priorities) > 0) {
  cat(paste0("4. **Performance Concern**: ",
             paste(paste0("Priority ", high_p90_priorities$Priority_Number), collapse = ", "),
             " show(s) 90th percentile TTD exceeding 3 minutes, warranting process review.\n\n"))
}
```

## Comparisons

This section is meant to compare this report week's data with the week prior. This will allow us to view some trend information and allow us to generate additional insights. For example there were `r nrow(current_week)` service calls this week and `r nrow(last_week)` service calls the week prior, this represents a change of `r nrow(current_week) - nrow(last_week)` calls from the week prior to last week. When the delta is positive, we had more calls last week, than the week prior. If the delta is negative, we had fewer service calls last week than the week prior.

### General Comparisons

This table will show the differences for some of the collected statistics.

| Statistic | Report Week | Previous Week | Delta | Pct. Change |
| :-------: | :---------: | :-----------: | :---: | :---------: |
| Call Count | `r nrow(current_week)` | `r nrow(last_week)` | `r nrow(current_week) - nrow(last_week)` | `r round((nrow(current_week) - nrow(last_week))/nrow(last_week) * 100, 2)`% |
| APD Calls | `r sum(current_week$Agency == "POLICE")` | `r sum(last_week$Agency == "POLICE")` | `r sum(current_week$Agency == "POLICE") - sum(last_week$Agency == "POLICE")` | `r round((sum(current_week$Agency == "POLICE") - sum(last_week$Agency == "POLICE"))/sum(last_week$Agency == "POLICE") * 100, 2)`% |
| AFD Fire Calls | `r sum(current_week$Agency == "FIRE")` | `r sum(last_week$Agency == "FIRE")` | `r sum(current_week$Agency == "FIRE") - sum(last_week$Agency == "FIRE")` | `r round((sum(current_week$Agency == "FIRE") - sum(last_week$Agency == "FIRE"))/sum(last_week$Agency == "FIRE") * 100, 2)`% |
| AFD EMS Calls | `r sum(current_week$Agency == "EMS")` | `r sum(last_week$Agency == "EMS")` | `r sum(current_week$Agency == "EMS") - sum(last_week$Agency == "EMS")` | `r round((sum(current_week$Agency == "EMS") - sum(last_week$Agency == "EMS"))/sum(last_week$Agency == "EMS") * 100, 2)`% |

As we can see, there were a few more calls last week than the week prior. APD calls were also up, by a higher percentage than the general call volume. Interesting to note, but there was no difference between the two weeks for AFD non-medical calls. Medical calls were down over 7% from the week prior.

### Outlier Comparisons

This table will show the differences in the numbers of likely outlier values from the last two weeks. 

| Statistic | Report Week | Previous Week | Delta | Pct. Change |
| :-------: | :---------: | :-----------: | :---: | :---------: |
| Time To Queue | `r nrow(ttq_outliers)` | `r nrow(lw_ttq_outliers)` | `r nrow(ttq_outliers) - nrow(lw_ttq_outliers)` | `r round((nrow(ttq_outliers) - nrow(lw_ttq_outliers))/nrow(lw_ttq_outliers) * 100, 2)`% |
| Time To Dispatch | `r nrow(ttd_outliers)` | `r nrow(lw_ttd_outliers)` | `r nrow(ttd_outliers) - nrow(lw_ttd_outliers)` | `r round((nrow(ttd_outliers) - nrow(lw_ttd_outliers))/nrow(lw_ttd_outliers) * 100, 2)`% |
| Phone Time | `r nrow(phone_outliers)` | `r nrow(lw_phone_outliers)` | `r nrow(phone_outliers) - nrow(lw_phone_outliers)` | `r round((nrow(phone_outliers) - nrow(lw_phone_outliers))/nrow(lw_phone_outliers) * 100, 2)`% |
| Processing Time | `r nrow(proc_outliers)` | `r nrow(lw_proc_outliers)` | `r nrow(proc_outliers) - nrow(lw_proc_outliers)` | `r round((nrow(proc_outliers) - nrow(lw_proc_outliers))/nrow(lw_proc_outliers) * 100, 2)`% |

From this table, we can see that the number of calls where we've identified a potential outlier value have come down significantly last week from the week prior. The largest decline was in the time taken from phone pickup to dispatch ready. There are, possibly, several reasons behind this and this could be a study in trend analysis in the future.

```{r}
#| label: comparison-narrative-insights
#| echo: false
#| results: asis

# Generate comprehensive week-over-week insights
cat("\n**Week-over-Week Performance Narrative:**\n\n")

# Call volume trend
call_change <- nrow(current_week) - nrow(last_week)
call_change_pct <- round((call_change / nrow(last_week)) * 100, 1)

if (abs(call_change_pct) >= 10) {
  direction <- if(call_change > 0) "surge" else "decline"
  cat(paste0("The ", abs(call_change_pct), "% ", direction, " in call volume (",
             abs(call_change), " calls) represents a significant shift that may reflect seasonal patterns, ",
             "community events, or external factors warranting further investigation. "))
} else if (abs(call_change_pct) >= 5) {
  cat(paste0("Call volume ", if(call_change > 0) "increased" else "decreased", " by ",
             abs(call_change_pct), "%, a moderate shift within normal operational variance. "))
} else {
  cat(paste0("Call volume remained stable with only a ", abs(call_change_pct), 
             "% change, indicating consistent community demand. "))
}

# Discipline-specific trends
apd_change <- sum(current_week$Agency == "POLICE") - sum(last_week$Agency == "POLICE")
afd_fire_change <- sum(current_week$Agency == "FIRE") - sum(last_week$Agency == "FIRE")
afd_ems_change <- sum(current_week$Agency == "EMS") - sum(last_week$Agency == "EMS")

max_change <- max(abs(apd_change), abs(afd_fire_change), abs(afd_ems_change))

if (max_change == abs(apd_change) && abs(apd_change) > 50) {
  cat(paste0("\n\nLaw enforcement calls showed the most significant change (",
             ifelse(apd_change > 0, "+", ""), apd_change, "), which may correlate with ",
             "enforcement campaigns, community events, or crime trends. "))
} else if (max_change == abs(afd_ems_change) && abs(afd_ems_change) > 20) {
  cat(paste0("\n\nEMS call volume shifted notably (",
             ifelse(afd_ems_change > 0, "+", ""), afd_ems_change, "), ",
             "potentially reflecting public health factors, weather conditions, or community health initiatives. "))
}

# Outlier trend analysis
outlier_improvement <- (nrow(lw_ttq_outliers) + nrow(lw_ttd_outliers) + 
                        nrow(lw_phone_outliers) + nrow(lw_proc_outliers)) -
                       (nrow(ttq_outliers) + nrow(ttd_outliers) + 
                        nrow(phone_outliers) + nrow(proc_outliers))

if (outlier_improvement > 10) {
  cat(paste0("\n\nThe ", outlier_improvement, " reduction in outlier calls suggests improved operational ",
             "consistency, possibly reflecting training effectiveness, process refinements, or ",
             "better resource allocation. This positive trend merits continued monitoring."))
} else if (outlier_improvement < -10) {
  cat(paste0("\n\n**Attention**: The increase of ", abs(outlier_improvement), 
             " outlier calls indicates emerging performance challenges that may require ",
             "immediate process review, additional training, or staffing assessment."))
}

cat("\n\n")
```

## Conclusion

```{r}
#| label: executive-conclusion-narrative
#| echo: false
#| results: asis

cat("\n**Executive Summary of Key Findings:**\n\n")

# Highlight top 3 performance successes
cat("**Notable Successes:**\n\n")

# Success 1: SLA compliance
sla_success <- c()
if (!is.na(fire_ems_106_sla_current) && fire_ems_106_sla_current >= 90) {
  sla_success <- c(sla_success, paste0("Fire/EMS 106-second SLA achieved ", 
                                       fire_ems_106_sla_current, "% compliance"))
}
if (!is.na(law_p1_sla_current) && law_p1_sla_current >= 85) {
  sla_success <- c(sla_success, paste0("Law enforcement Priority 1 calls met ",
                                       law_p1_sla_current, "% SLA target"))
}

# Success 2: Outlier reduction
if (exists("outlier_improvement") && outlier_improvement > 5) {
  sla_success <- c(sla_success, paste0("Outlier calls reduced by ", outlier_improvement,
                                       ", demonstrating improved consistency"))
}

# Success 3: Volume management
if (abs(call_change_pct) < 5 && median_ttd_current < 150) {
  sla_success <- c(sla_success, "Maintained stable performance despite consistent call volume")
}

if (length(sla_success) > 0) {
  for (i in 1:min(3, length(sla_success))) {
    cat(paste0(i, ". ", sla_success[i], "\n"))
  }
} else {
  cat("Operational performance met baseline expectations across multiple metrics.\n")
}

cat("\n**Areas Requiring Attention:**\n\n")

# Identify top concerns
concerns <- c()

if (!is.na(law_p1_sla_current) && law_p1_sla_current < 80) {
  concerns <- c(concerns, paste0("Priority 1 law enforcement SLA at ", law_p1_sla_current,
                                 "% requires immediate process review"))
}

if (median_phone_current > 150 && phone_pct > 10) {
  concerns <- c(concerns, paste0("Phone time increased ", phone_pct, 
                                 "%, suggesting more complex calls or training needs"))
}

if (not_captured_count > 50) {
  concerns <- c(concerns, paste0(not_captured_count, " calls lack call reception documentation, ",
                                 "impacting data quality and operational visibility"))
}

if (exists("outlier_improvement") && outlier_improvement < -5) {
  concerns <- c(concerns, paste0("Outlier calls increased, indicating potential process degradation"))
}

if (length(concerns) > 0) {
  for (i in 1:min(3, length(concerns))) {
    cat(paste0(i, ". ", concerns[i], "\n"))
  }
} else {
  cat("No critical performance concerns identified this reporting period.\n")
}

cat("\n**Strategic Recommendations:**\n\n")

# Generate actionable recommendations
if (length(concerns) > 0 || exists("high_p90_priorities") && nrow(high_p90_priorities) > 0) {
  cat("1. Conduct targeted process review for underperforming metrics\n")
  cat("2. Implement focused training on identified performance gaps\n")
  cat("3. Review staffing patterns during peak outlier periods\n")
} else {
  cat("1. Continue current operational protocols and training regimens\n")
  cat("2. Expand hybrid call taker program based on demonstrated effectiveness\n")
  cat("3. Document successful practices for replication and knowledge transfer\n")
}

cat("\n")
```

This report has covered various aspects of the calls for service during week `r WEEK_NUMBER`. We have analyzed the data for completeness and accuracy, explored it for insights into call patterns and trends, and focused on specific areas of interest such as high-priority calls and cardiac arrest incidents. The findings will assist in making informed decisions to improve service delivery and operational efficiency.
