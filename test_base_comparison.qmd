---
title: "DECC Weekly Report"
author: "Tony Dunsworth, PhD"
date: "2025-10-30"
format:
    html:
        toc: true
        toc-depth: 3
        toc-location: left
execute:
  freeze: auto
---

## Week 43 from 19 October through 25 October 2025

## Table of Contents

### Main Sections

- **[Introduction](#introduction)**
- **[Data Cleaning](#data-cleaning)**
- **[Exploratory Analysis](#exploratory-analysis)**
  - [Call Distribution: Hour by Day of Week](#call-distribution-hour-by-day-of-week)
  - [Summary Statistics and Analyses](#summary-statsitcs-and-analyses)
- **[Discipline Analyses](#discipline-analyses)**
  - [APD Analyses](#apd-analyses)
  - [AFD FIRE Analyses](#afd-fire-analyses)
  - [AFD EMS Analyses](#afd-ems-analyses)
- **[Additional Analyses](#additional-analyses)**
  - [Possible Service Delays](#possible-service-delays)
- **[High-Priority and Critical Calls](#high-priority-and-critical-calls)**
  - [High-Priority Call Types](#high-priority-call-types)
  - [High-Priority Response Times](#high-priority-response-times)
- **[E-911 Service Call Analyses](#e-911-service-call-analyses)**
  - [E-911 Call Response Summary](#e-911-call-response-summary)
  - [E-911 Call Breakdowns](#e-911-call-breakdowns)
- **[Cardiac Arrest Calls Analysis](#cardiac-arrest-calls-analysis)**
- **[Mental Health Analyses](#mental-health-analyses)**
- **[Call Source Unrecorded](#call-source-unrecorded)**
- **[Outlier Identification](#outlier-identification)**
- **[Comparisons](#comparisons)**
- **[Conclusion](#conclusion)**

---

```{r libraries}
#| label: setup
#| echo: false
#| message: false
#| warning: false
#| cache: false

# Suppress dbus warnings that can cause preview issues
suppressWarnings({
  suppressPackageStartupMessages({
    library(tidyverse)
    library(tidymodels)
    library(devtools)
    library(remotes)
    library(ggpubr)
    library(ggrepel)
    library(ggraph)
    library(gt)
    library(gtExtras)
    library(GGally)
    library(rstatix)
    library(car)
    library(janitor)
    library(Hmisc)
    library(psych)
    library(corrr)
    library(ggcorrplot)
    library(ggthemes)
    library(ggridges)
    library(multcomp)
    library(emmeans)
    library(RVAideMemoire)
    library(FactoMineR)
    library(DescTools)
    library(nlme)
    library(funModeling)
    library(inspectdf)
    library(dlookr)
    library(viridis)
    library(merTools)
    library(factoextra)
    library(nortest)
    library(MASS)
    library(randtests)
    library(summarytools)
    library(report)
    library(knitr)
    library(kableExtra)
    library(modelbased)
    library(parameters)
    library(performance)
    library(insight)
    library(lubridate)
    library(broom)
    library(GPfit)
    library(survival)
    library(paletteer)
    library(flextable)
    library(officer)
    library(outliers)
  })
})
```

```{r dataframes}
#| label: dataframes
#| include: false
#| cache: false

# ============================================================================
# AUTOMATED WEEK CALCULATION AND DATA LOADING
# ============================================================================
# This chunk automatically determines which weeks to load based on the report date
# Report covers: week PRIOR to the week containing the report date
# Comparison uses: week BEFORE the reporting week

# Report date (today's date or override here if needed)
REPORT_DATE <- Sys.Date()  # Change this to lock to a specific date if needed
# REPORT_DATE <- as.Date("2025-10-30")  # Example: uncomment to use specific date

# Calculate which week the REPORT_DATE falls into (ISO 8601: weeks start Sunday)
report_week_start <- floor_date(REPORT_DATE, "week", week_start = 7)  # 7 = Sunday
report_week_num <- isoweek(report_week_start)
report_year <- year(report_week_start)

# The week we're REPORTING ON is the week PRIOR to the report week
current_week_start <- report_week_start - weeks(1)
current_week_end <- current_week_start + days(6)
# Use the middle of the week (Wednesday) to get accurate ISO week number
current_week_num <- isoweek(current_week_start + days(3))
current_year <- year(current_week_start)

# The COMPARISON week is one week before the current week
last_week_start <- current_week_start - weeks(1)
last_week_end <- last_week_start + days(6)
# Use the middle of the week (Wednesday) to get accurate ISO week number
last_week_num <- isoweek(last_week_start + days(3))
last_year <- year(last_week_start)

# Store week numbers for use throughout the document
WEEK_NUMBER <- current_week_num
WEEK_START_DATE <- current_week_start
WEEK_END_DATE <- current_week_end

LAST_WEEK_NUMBER <- last_week_num
LAST_WEEK_START_DATE <- last_week_start
LAST_WEEK_END_DATE <- last_week_end

# Format dates for display
WEEK_START_FORMATTED <- format(WEEK_START_DATE, "%d %b")
WEEK_END_FORMATTED <- format(WEEK_END_DATE, "%d %b")

LAST_WEEK_START_FORMATTED <- format(LAST_WEEK_START_DATE, "%d %b")
LAST_WEEK_END_FORMATTED <- format(LAST_WEEK_END_DATE, "%d %b")

# ============================================================================
# DATA FILE LOADING HELPER
# ============================================================================
# Function to find and load the correct CSV file for a given week
load_week_data <- function(week_num, week_year) {
  # Determine which folder to look in
  if (week_year == year(REPORT_DATE)) {
    # Current year - look in data/current_year
    base_path <- "data/current_year"
  } else {
    # Prior year - look in data/prior_year
    base_path <- "data/prior_year"
  }
  
  # Try different naming conventions
  possible_files <- c(
    file.path(base_path, paste0("week", week_num, ".csv")),
    file.path(base_path, paste0("Week", week_num, "_", substr(week_year, 3, 4), ".csv")),
    file.path(base_path, paste0("Week", week_num, ".csv")),
    # Fallback: also check root data folder for backwards compatibility
    paste0("data/week", week_num, ".csv"),
    paste0("data/Week", week_num, "_", substr(week_year, 3, 4), ".csv"),
    paste0("data/Week", week_num, ".csv")
  )
  
  # Find the first file that exists
  data_file <- NULL
  for(file in possible_files) {
    if(file.exists(file)) {
      data_file <- file
      break
    }
  }
  
  if(is.null(data_file)) {
    stop("No data file found for week ", week_num, " (", week_year, "). Checked: ", 
         paste(possible_files, collapse = ", "))
  }
  
  cat("Loading week", week_num, "(", week_year, ") data from:", data_file, "\n")
  
  # Load the CSV file
  df <- tryCatch({
    if (requireNamespace("vroom", quietly = TRUE)) {
      as_tibble(vroom::vroom(data_file, col_types = vroom::cols()))
    } else {
      readr::read_csv(data_file, show_col_types = FALSE)
    }
  }, error = function(e) {
    readr::read_csv(data_file, show_col_types = FALSE)
  })
  
  # Validate data loaded
  if(nrow(df) == 0) {
    stop("Data file is empty or failed to load: ", data_file)
  }
  
  cat("  Successfully loaded", nrow(df), "rows\n")
  
  return(df)
}

# ============================================================================
# LOAD DATA FOR CURRENT WEEK AND LAST WEEK
# ============================================================================
cat("\n=== AUTOMATED WEEK CALCULATION ===\n")
cat("Report Date:", format(REPORT_DATE, "%Y-%m-%d"), "\n")
cat("Current Week (reporting on):", WEEK_NUMBER, "—", WEEK_START_FORMATTED, "through", WEEK_END_FORMATTED, current_year, "\n")
cat("Last Week (comparison):", LAST_WEEK_NUMBER, "—", LAST_WEEK_START_FORMATTED, "through", LAST_WEEK_END_FORMATTED, last_year, "\n\n")

# Load current week data
current_week <- load_week_data(current_week_num, current_year)

# Load last week data
last_week <- load_week_data(last_week_num, last_year)

cat("\n=== DATA LOADING COMPLETE ===\n")

# Update title dynamically (since we can't use R in YAML directly)
DYNAMIC_TITLE <- paste0("Weekly Report: Week ", WEEK_NUMBER, " (", 
                        WEEK_START_FORMATTED, " through ", WEEK_END_FORMATTED, " ", current_year, ")")

```

```{r global-options}
#| label: global-options
#| include: false
#| cache: false

# Global knitr options for faster renders and consistent behavior
knitr::opts_chunk$set(
  cache = TRUE,           # enable caching by default
  cache.lazy = FALSE,     # avoid lazy loading cache on Windows
  cache.path = paste0('cache/week-', WEEK_NUMBER, '/'),
  fig.retina = 1,         # smaller figures in HTML for speed
  dpi = 96,               # reasonable default DPI for speed
  dev = 'png',            # faster device for HTML
  message = FALSE,
  warning = FALSE
)

# Reduce noisy dplyr summarise messages
options(dplyr.summarise.inform = FALSE)

# Precompute a few shared palettes to avoid repeated paletteer calls
palette_cyan_base <- tryCatch(as.character(paletteer::paletteer_d("ggsci::cyan_material")),
                              error = function(e) c("#E0F2F1", "#B2DFDB", "#80CBC4", "#4DB6AC", "#26A69A", "#009688", "#00897B", "#00796B", "#00695C", "#004D40"))
palette_blue_base <- tryCatch(as.character(paletteer::paletteer_d("cartography::blue.pal")),
                              error = function(e) c("#08306B", "#08519C", "#2171B5", "#4292C6", "#6BAED6", "#9ECAE1", "#C6DBEF", "#DEEBF7"))
palette_red_base  <- tryCatch(as.character(paletteer::paletteer_d("cartography::red.pal")),
                              error = function(e) c("#67000D", "#A50F15", "#CB181D", "#EF3B2C", "#FB6A4A", "#FC9272", "#FCBBA1", "#FEE0D2"))

palette_cyan_24 <- grDevices::colorRampPalette(palette_cyan_base)(24)
palette_blue_24 <- grDevices::colorRampPalette(palette_blue_base)(24)
palette_red_24  <- grDevices::colorRampPalette(palette_red_base)(24)

# Small helper: safe quantiles/iqr with na.rm
q90 <- function(x) as.numeric(quantile(x, 0.9, na.rm = TRUE))
q99 <- function(x) as.numeric(quantile(x, 0.99, na.rm = TRUE))
iqr_safe <- function(x) IQR(x, na.rm = TRUE)
```

```{r table-helpers}
#| echo: false
#| message: false
#| warning: false
#| cache: false

flextable::set_flextable_defaults(
  theme_fun = flextable::theme_booktabs,
  font.size = 9,
  padding = 3,
  borders = officer::fp_border_default()
)

to_ft <- function(tbl, caption = NULL, header_map = NULL, digits = 2) {
  df <- as.data.frame(tbl)
  ft <- flextable::flextable(df)
  if (!is.null(header_map)) ft <- flextable::set_header_labels(ft, values = header_map)
  if (nrow(df) > 0) ft <- flextable::bg(ft, i = seq(1, nrow(df), by = 2), bg = "#F5F5F5", part = "body")
  ft <- flextable::bold(ft, part = "header")
  ft <- flextable::fontsize(ft, part = "header", size = 9)
  num_cols <- names(df)[vapply(df, is.numeric, logical(1))]
  if (length(num_cols) > 0) {
    ft <- flextable::align(ft, j = num_cols, align = "right", part = "all")
    ft <- flextable::colformat_num(ft, j = num_cols, digits = digits, big.mark = ",")
  }
  if (!is.null(caption)) ft <- flextable::set_caption(ft, caption)
  flextable::autofit(ft)
}
```

## Introduction

This is the weekly report for week `r WEEK_NUMBER` covering the period from `r WEEK_START_FORMATTED` through `r WEEK_END_FORMATTED` 2025. The report will include analyses of the data to emphasize different information that is contained within the data and may be pertinent to both operations and management.

```{r data-load}
#| echo: false
#| output: false
#| cache: true

# Data is now loaded in the dataframes chunk as 'current_week' and 'last_week'
# This chunk processes the current week data for the main report

# Use current_week as the primary dataframe for this report
df <- current_week

cat("Processing current week data:", nrow(df), "rows\n")
cat("Call_Reception column check:", "Call_Reception" %in% names(df), "\n")

# Use lubridate and across() to efficiently parse all date-time columns
df <- df |>
  mutate(across(c(Response_Date,
                   Incident_Start_Time,
                   TimeCallViewed,
                   Incident_Queue_Time,
                   Incident_Dispatch_Time,
                   Incident_Phone_Stop,
                   TimeFirstUnitDispatchAcknowledged,
                   Incident_Enroute_Time,
                   Incident_Arrival_Time,
                   TimeFirstCallCleared,
                   Incident_First_Close_Time,
                   Final_Closed_Time,
                   First_Reopen_Time), ymd_hms))

df$WeekNo <- as.factor(df$WeekNo)
df$Day <- as.factor(df$Day)
df$Hour <- as.factor(df$Hour)

# Convert DOW to an ordered factor to respect the sequence of days
df$DOW <- factor(
    df$DOW,
    levels = c("SUN", "MON", "TUE", "WED", "THU", "FRI", "SAT"),
    ordered = TRUE
)

df$ShiftPart <- factor(
  df$ShiftPart,
  levels = c("EARLY", "MIDS", "LATE"),
  ordered = TRUE
)

# Convert Priority_Number to an ordered factor as well
df$Priority_Number <- ordered(df$Priority_Number)

# Convert numeric variables from 'doubles' to integers
df[c('Time_To_Queue', 'Time_To_Dispatch', 'Phone_Time', 'Processing_Time', 'Rollout_Time', 'Transit_Time', 'Total_Call_Time')] <- sapply(df[c('Time_To_Queue', 'Time_To_Dispatch', 'Phone_Time', 'Processing_Time', 'Rollout_Time', 'Transit_Time', 'Total_Call_Time')], as.numeric)

# Process last week data for comparison (same transformations)
df_last <- last_week |>
  mutate(across(c(Response_Date,
                   Incident_Start_Time,
                   TimeCallViewed,
                   Incident_Queue_Time,
                   Incident_Dispatch_Time,
                   Incident_Phone_Stop,
                   TimeFirstUnitDispatchAcknowledged,
                   Incident_Enroute_Time,
                   Incident_Arrival_Time,
                   TimeFirstCallCleared,
                   Incident_First_Close_Time,
                   Final_Closed_Time,
                   First_Reopen_Time), ymd_hms))

df_last$WeekNo <- as.factor(df_last$WeekNo)
df_last$Day <- as.factor(df_last$Day)
df_last$Hour <- as.factor(df_last$Hour)
df_last$DOW <- factor(df_last$DOW, levels = c("SUN", "MON", "TUE", "WED", "THU", "FRI", "SAT"), ordered = TRUE)
df_last$ShiftPart <- factor(df_last$ShiftPart, levels = c("EARLY", "MIDS", "LATE"), ordered = TRUE)
df_last$Priority_Number <- ordered(df_last$Priority_Number)
df_last[c('Time_To_Queue', 'Time_To_Dispatch', 'Phone_Time', 'Processing_Time', 'Rollout_Time', 'Transit_Time', 'Total_Call_Time')] <- sapply(df_last[c('Time_To_Queue', 'Time_To_Dispatch', 'Phone_Time', 'Processing_Time', 'Rollout_Time', 'Transit_Time', 'Total_Call_Time')], as.numeric)

cat("Processing last week data:", nrow(df_last), "rows\n")
cat("Both weeks ready for analysis and comparison\n")
```

For this week, there were a total of `r nrow(df)` calls for service. The column list is below:

```{r example-data}
#| echo: false
#| tbl-cap: "A sample of the first 10 rows of incident data."
#| cache: true

colnames(df)
```

## Data Cleaning

In order to have a good dataset for analysis, some data cleaning was performed. The first step is to check for missing values in the dataset.

```{r missing-values}
#| echo: false
#| warning: false
#| fig-cap: "Prevalence of missing values. Only columns with missing data are shown."
#| cache: true

# ...existing code...
# Count columns with any missing data
missing_cols_count <- sum(colSums(is.na(df)) > 0)

# Get named vector of missing counts per column
missing_counts <- colSums(is.na(df))

# Optionally, create a tibble for easy use
missing_summary <- tibble(
  column = names(df),
  missing = missing_counts
)

# Find the column with the largest number of missing values
max_missing_col <- names(missing_counts)[which.max(missing_counts)]
max_missing_count <- max(missing_counts)

# Calculate percentage of calls without Incident_Arrival_Time
incident_arrival_missing_pct <- round(missing_counts["Incident_Arrival_Time"] / nrow(df) * 100, 1)

# Identify columns with missing data for inline reporting
incident_arrival_missing_count <- missing_counts["Incident_Arrival_Time"]

# ...existing code...

inspect_na(df) |>
  dplyr::filter(cnt > 0) |> # Explicitly use dplyr's filter
  show_plot(
    text_labels = TRUE,
    label_color = "white"
  ) +
  # Use interpolated palette to handle any number of missing columns
  scale_fill_manual(
    values = colorRampPalette(
      as.character(paletteer::paletteer_d("MexBrewer::Maiz"))
    )(sum(colSums(is.na(df)) > 0))
  ) +
  geom_text(
    aes(label = paste0(round(pcnt, 1), "%")),
    stat = "identity",
    vjust = -0.5,
    size = 3,
    color = "black",
    fontface = "bold"
  ) +
  ggthemes::theme_fivethirtyeight(base_size = 9) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 9), # Rotate x-axis labels
    axis.text.y = element_text(size = 9) # Fine-tune y-axis label size
  )
```

From this plot, we can see that there are only `r missing_cols_count` columns with missing data. Of those, the column with the largest number of missing values is `r max_missing_col`. That is something that we would like to see because that means that most of our calls are closed once and left that way. Later, we will look deeper into those calls to see if there are any patterns to those calls. There were `r incident_arrival_missing_count` calls that did not have a recorded time that the call arrived, representing `r incident_arrival_missing_pct`% of calls for the week. We will have to determine if they were cancelled or how many of those were mutual aid calls where we did not receive a phone call.

## Exploratory Analysis

One of the first analyses is to break down different factor elements to see what we have in the dataset. Starting with the day of the week, the barchart below shows the number of calls for service by day of the week.

```{r day-of-week}
#| echo: false
#| fig-cap: "Number of calls for service by day of the week."
#| cache: true
# ggplot2
dow_counts <- df |>
  count(DOW, sort = TRUE)

max_dow_info <- dow_counts |> filter(n == max(n))
busiest_day_abbr <- max_dow_info |> slice(1) |> pull(DOW)
busiest_day_count <- max_dow_info |> slice(1) |> pull(n)

min_dow_info <- dow_counts |> filter(n == min(n))
slowest_day_abbr <- min_dow_info |> slice(1) |> pull(DOW)
slowest_day_count <- min_dow_info |> slice(1) |> pull(n)

# Create mapping from abbreviations to full day names
day_names <- c(
  "SUN" = "Sunday",
  "MON" = "Monday",
  "TUE" = "Tuesday",
  "WED" = "Wednesday",
  "THU" = "Thursday",
  "FRI" = "Friday",
  "SAT" = "Saturday"
)

# Convert abbreviations to full day names
busiest_day <- day_names[busiest_day_abbr]
slowest_day <- day_names[slowest_day_abbr]

barDOW <- df |> ggplot(aes(x=DOW, fill=DOW)) +
  geom_bar() +
  paletteer::scale_fill_paletteer_d("ggsci::cyan_material") +
  labs(title="Number of Calls for Service by Day of the Week",
       x="Day of the Week",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size = 3.5
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barDOW
```

From this chart, we can see that `r busiest_day` was the busiest day of the week with `r busiest_day_count` service calls, and the slowest day was `r slowest_day` with `r slowest_day_count` service calls. There is some consistency throughout the week, with `r busiest_day_count - slowest_day_count` calls difference between the busiest and slowest days.

```{r hour-of-day}
#| echo: false
#| fig-cap: "Number of calls for service by hour of the day."
#| cache: true
# ggplot2
hour_counts <- df |>
  count(Hour, sort = TRUE)

# Find the actual maximum count and corresponding hour(s)
max_count <- max(hour_counts$n)
max_hour_info <- hour_counts |> filter(n == max_count)

# Convert factor levels back to numeric for proper comparison, then back to formatted string
busiest_hour_numeric <- max_hour_info |>
  slice(1) |>
  pull(Hour) |>
  as.character() |>
  as.numeric()
busiest_hour <- sprintf("%02d", busiest_hour_numeric)
busiest_hour_count <- max_hour_info |> slice(1) |> pull(n)

min_hour_info <- hour_counts |> filter(n == min(n))
# Convert factor levels back to numeric for proper comparison, then back to formatted string
slowest_hour_numeric <- min_hour_info |>
  slice(1) |>
  pull(Hour) |>
  as.character() |>
  as.numeric()
slowest_hour <- sprintf("%02d", slowest_hour_numeric)
slowest_hour_count <- min_hour_info |> slice(1) |> pull(n)

barHour <- df |> ggplot(aes(x=Hour, fill=Hour)) +
  geom_bar() +
  # Use precomputed palette to avoid repeated paletteer calls
  scale_fill_manual(values = palette_cyan_24) +
  labs(title="Number of Calls for Service by Hour of the Day",
       x="Hour of the Day",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size = 3.5
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barHour
```

This week, the busiest hour of the day was `r busiest_hour`00 hours, with `r busiest_hour_count` calls for service. `r slowest_hour`00 hours was the slowest hour of the day with `r slowest_hour_count` calls. Additionally, the pattern shows consistent traffic from late rush hour through the day into the early evening before seeing the volumes start to decline. This appears to confirm assumptions about the busiest parts of the day.

```{r shifts}
#| echo: false
#| fig-cap: "Number of calls for service by shift."
#| cache: true
shift_counts <- df |>
  count(Shift, sort = TRUE)

dn_counts <- df |>
  count(Day_Night, sort = TRUE)

sp_counts <- df |>
  count(ShiftPart, sort = TRUE) # Note: ShiftPart is not used in the plots below

barShift <- df |>
  ggplot(aes(x=Shift, fill=ShiftPart)) +
  geom_bar() +
  paletteer::scale_fill_paletteer_d("ggsci::cyan_material") +
  labs(title="Call Volume per Shift",
    x = "Shift",
    y = "Number of Calls") +
  # Add segment counts centered within each segment (white text)
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    position = position_stack(vjust = 0.5),
    size = 3.5,
    color = "black",
    fontface = "bold"
  ) +
  # Add total counts at the top of each bar
  stat_count(
    aes(label = after_stat(count), fill = NULL),
    geom = "text",
  vjust = -0.5,
    size = 3.5,
    fontface = "bold",
    color = "black"
  ) +
  theme_minimal() +
  theme(legend.position="right",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.title = element_text(size = 9),
        legend.text = element_text(size = 9))

barShift

barDN <- df |>
  ggplot(aes(x=Day_Night, fill=ShiftPart)) +
  geom_bar() +
  paletteer::scale_fill_paletteer_d("ggsci::cyan_material") +
  labs(title="Call Volume by Day/Night",
       x="Day/Night",
       y="Number of Calls") +
  # Add segment counts centered within each segment (white text)
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    position = position_stack(vjust = 0.5),
    size = 3.5,
    color = "black",
    fontface = "bold"
  ) +
  # Add total counts at the top of each bar
  stat_count(
    aes(label = after_stat(count), fill = NULL),
    geom = "text",
  vjust = -0.5,
    size = 3.5,
    fontface = "bold",
    color = "black"
  ) +
  theme_minimal() +
  theme(legend.position="right",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.title = element_text(size = 9),
        legend.text = element_text(size = 9))

barDN
```

As expected, day shifts are busier than night shifts by a considerable margin. As expected, the call volumes throughout the shift for day shift are fairly consistent and on night shift, the longer the shifts go, fewer calls are received. These are both expected from prior work.

```{r priority-level}
#| echo: false
#| fig-cap: "Number of calls for service by priority level."
#| cache: true
# ggplot2
pn_counts <- df |>
  count(Priority_Number, sort = TRUE)

max_pn_info <- pn_counts |> filter(n == max(n))
busiest_pn <- max_pn_info |> slice(1) |> pull(Priority_Number)
busiest_pn_count <- max_pn_info |> slice(1) |> pull(n)

# Calculate percentage statistics for inline use
busiest_pn_percentage <- round(busiest_pn_count / nrow(df) * 100, 1)
priority1_count <- sum(df$Priority_Number == "1", na.rm = TRUE)
priority1_percentage <- round(priority1_count / nrow(df) * 100, 1)

barPriority <- df |> ggplot(aes(x=Priority_Number, fill=Priority_Number)) +
  geom_bar() +
  paletteer::scale_fill_paletteer_d("ggsci::cyan_material") +
  labs(title="Number of Calls for Service by Priority Level",
       x="Priority Level",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size = 3.5
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barPriority
```

The majority of calls received were Priority `r busiest_pn` calls. Priority `r busiest_pn` calls are `r busiest_pn_percentage` percent of the total number of calls, while Priority 1 calls are `r priority1_percentage` percent of the total number of calls. This is a consistent pattern and more detailed analyses of the high priority, and specifically priority 1 calls, can be found below.

```{r discipline}
#| echo: false
#| fig-cap: "Number of calls for service by discipline."
#| cache: true
# ggplot2
agency_counts <- df |>
  count(Agency, sort = TRUE)

max_agency_info <- agency_counts |> filter(n == max(n))
busiest_agency <- max_agency_info |> slice(1) |> pull(Agency)
busiest_agency_count <- max_agency_info |> slice(1) |> pull(n)

# Calculate percentage for Police calls
police_percentage <- round((sum(df$Agency == "POLICE", na.rm = TRUE) / nrow(df)) * 100, 1)

barDiscipline <- df |> ggplot(aes(x=Agency, fill=Agency)) +
  geom_bar() +
  scale_fill_manual(
    values = c(POLICE = "#1f77b4", FIRE = "#d62728", EMS = "#2ca02c"),
    name = "Agency"
  ) +
  labs(title="Number of Calls for Service by Discipline",
       x="Discipline",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    position = position_stack(vjust = 0.5),
    size = 3.5,
    color = "white",
    fontface = "bold"
  ) +
  theme_minimal() +
  theme(legend.position="right",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.title = element_text(size = 9),
        legend.text = element_text(size = 9))

barDiscipline
```

As expected, the majority of calls are for `r busiest_agency`. They represent `r police_percentage`
percent of the total number of calls. This is fairly consistent with previous analyses. We can also examine the way in which we are receiving the calls by looking at the Call_Reception column. That chart is below.

```{r call-reception}
#| echo: false
#| fig-cap: "Number of calls for service by call reception."
#| cache: true

# ggplot2
cr_counts <- df |>
  count(Call_Reception, sort = TRUE)

max_cr_info <- cr_counts |> filter(n == max(n))
busiest_cr <- max_cr_info |> slice(1) |> pull(Call_Reception)
busiest_cr_count <- max_cr_info |> slice(1) |> pull(n)

# Calculate statistics for inline use
e911_count <- sum(df$Call_Reception == "E-911", na.rm = TRUE)
not_captured_count <- sum(df$Call_Reception == "NOT CAPTURED", na.rm = TRUE)
e911_percentage <- round((e911_count / nrow(df)) * 100, 2)
not_captured_percentage <- round((not_captured_count / nrow(df)) * 100, 1)

barReception <- df |> ggplot(aes(x=Call_Reception, fill=Call_Reception)) +
  geom_bar() +
  paletteer::scale_fill_paletteer_d("ggsci::cyan_material") +
  labs(title="Number of Calls for Service by Call Reception",
       x="Call Reception",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size = 3.5
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barReception
```

Most of the calls arrived by `r busiest_cr`. 911 trunk line calls were `r e911_percentage` percent of all calls. There were `r not_captured_count` calls where we did not indicate how the service call was received. This is `r not_captured_percentage` percent of the total number of calls. These calls should be investigated to determine why call origination is not being tracked.

The following is a chart of the top 10 call types. The data is limited to ensure visual clarity and legibility of the information.

```{r call-type}
#| echo: false
#| fig-cap: "Number of calls for service by call type."
#| cache: true
# ggplot2
ct_counts <- df |>
  count(Problem, sort = TRUE)

max_ct_info <- ct_counts |> filter(n == max(n))
busiest_ct <- max_ct_info |> slice(1) |> pull(Problem)
busiest_ct_count <- max_ct_info |> slice(1) |> pull(n)

# Find the most common Problem for each Agency
agency_top_problems <- df |>
  count(Agency, Problem, sort = TRUE) |>
  group_by(Agency) |>
  slice_max(n, n = 1, with_ties = FALSE) |>
  ungroup() |>
  dplyr::select(Agency, Problem, n)

# Create named vectors for easy access in inline code
agency_top_problem_names <- setNames(agency_top_problems$Problem, agency_top_problems$Agency)
agency_top_problem_counts <- setNames(agency_top_problems$n, agency_top_problems$Agency)

problem_counts <- df |>
  count(Problem, sort = TRUE) |>
  slice_head(n = 10)

barProblem <- problem_counts |>
  ggplot(aes(x=reorder(Problem, -n), y=n, fill=Problem)) +
  geom_bar(stat="identity") +
  paletteer::scale_fill_paletteer_d("ggsci::cyan_material") +
  labs(title="Number of Calls for Service by Call Type",
       x="Call Type",
       y="Number of Calls") +
  geom_text(
    aes(label = n),
    vjust = -0.5,
    size = 3.5) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barProblem
```

This week, the most common problem nature was `r busiest_ct`. For AFD, the most common was `r agency_top_problem_names["FIRE"]`. The most common medical call for service was `r agency_top_problem_names["EMS"]`.

We can also look at the number of calls taken by telecommunicators. Again, like the problem types, we will limit the chart to the top 10 telecommunicators to ensure visual clarity and legibility of the information.

```{r telecommunicator}
#| echo: false
#| fig-cap: "Number of calls for service by telecommunicator."
#| cache: true
# ggplot2
tc_counts <- df |>
  count(Call_Taker, sort = TRUE)

max_tc_info <- tc_counts |> filter(n == max(n))
busiest_tc <- max_tc_info |> slice(1) |> pull(Call_Taker)
busiest_tc_count <- max_tc_info |> slice(1) |> pull(n)

tc_counts <- df |>
  count(Call_Taker, sort = TRUE) |>
  slice_head(n = 10)

barCallTaker <- tc_counts |>
  ggplot(aes(x=reorder(Call_Taker, -n), y=n, fill=Call_Taker)) +
  geom_bar(stat="identity") +
  paletteer::scale_fill_paletteer_d("ggsci::cyan_material") +
  labs(title="Number of Calls for Service by Call Taker",
       x="Call Taker",
       y="Number of Calls") +
  geom_text(
    aes(label = n),
    vjust = -0.5,
    size = 3.5) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barCallTaker
```

It is interesting to note that the top "call taker" is `r busiest_tc` again this week with `r busiest_tc_count` calls.

```{r time-on-phone}
#| label: time-on-phone
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Total Time on Phone for Top 10 calltakers"
#| cache: true

# Two tables: top 10 Call_Taker by cumulative Time_To_Queue and Phone_Time

# Ensure numeric seconds for aggregation
df_times <- df |>
  mutate(
    Time_To_Queue = suppressWarnings(as.numeric(Time_To_Queue)),
    Phone_Time    = suppressWarnings(as.numeric(Phone_Time))
  )

# Summarize cumulative seconds by Call_Taker
ttq_by_ct <- df_times |>
  filter(!is.na(Call_Taker)) |>
  group_by(Call_Taker) |>
  dplyr::summarise(
    N_TTQ = sum(!is.na(Time_To_Queue)),
    Total_Time_To_Queue_Sec = sum(Time_To_Queue, na.rm = TRUE),
    .groups = "drop"
  ) |>
  arrange(desc(Total_Time_To_Queue_Sec)) |>
  slice_head(n = 10) |>
  mutate(
    Total_Time_To_Queue_Min = round(Total_Time_To_Queue_Sec / 60, 1),
    Mean_TTQ_Sec = ifelse(N_TTQ > 0, round(Total_Time_To_Queue_Sec / N_TTQ), NA_real_)
  )

  most_time_ct <- ttq_by_ct$Call_Taker[1]
  most_time_ct_total <- ttq_by_ct$Total_Time_To_Queue_Sec[1]
  most_time_ct_mean <- ttq_by_ct$Mean_TTQ_Sec[1]

phone_by_ct <- df_times |>
  filter(!is.na(Call_Taker)) |>
  group_by(Call_Taker) |>
  dplyr::summarise(
    N_Phone = sum(!is.na(Phone_Time)),
    Total_Phone_Time_Sec = sum(Phone_Time, na.rm = TRUE),
    .groups = "drop"
  ) |>
  arrange(desc(Total_Phone_Time_Sec)) |>
  slice_head(n = 10) |>
  mutate(
    Total_Phone_Time_Min = round(Total_Phone_Time_Sec / 60, 1),
    Mean_Phone_Sec = ifelse(N_Phone > 0, round(Total_Phone_Time_Sec / N_Phone), NA_real_)
  )

  longest_phone_ct <- phone_by_ct$Call_Taker[1]
  longest_phone_ct_total <- phone_by_ct$Total_Phone_Time_Sec[1]
  longest_phone_ct_mean <- phone_by_ct$Mean_Phone_Sec[1]

# Render tables with accessible Word-friendly formatting
#to_ft(
#  ttq_by_ct |> dplyr::select(Call_Taker, N_TTQ, Total_Time_To_Queue_Sec, Total_Time_To_Queue_Min, Mean_TTQ_Sec),
#  caption = "Top 10 Call Takers by Cumulative Time To Queue",
#  header_map = list(
#    Call_Taker = "Call Taker",
#    N_TTQ = "Calls",
#    Total_Time_To_Queue_Sec = "Total TTQ (sec)",
#    Total_Time_To_Queue_Min = "Total TTQ (min)",
#    Mean_TTQ_Sec = "Mean TTQ (sec)"
#  ),
#  digits = 0
#)

to_ft(
  phone_by_ct |> dplyr::select(Call_Taker, N_Phone, Total_Phone_Time_Sec, Total_Phone_Time_Min, Mean_Phone_Sec),
  caption = "Top 10 Call Takers by Cumulative Phone Time",
  header_map = list(
    Call_Taker = "Call Taker",
    N_Phone = "Calls",
    Total_Phone_Time_Sec = "Total Phone (sec)",
    Total_Phone_Time_Min = "Total Phone (min)",
    Mean_Phone_Sec = "Mean Phone (sec)"
  ),
  digits = 0
)
```

This table shows the call takers with the highest cumulative time on the phone. This week, `r longest_phone_ct` had the largest average time on the phone with an average of `r longest_phone_ct_mean` seconds per call. This could be due to a number of factors, including the complexity of the calls faced.

### Call Distribution: Hour by Day of Week

The following visualization shows the distribution of calls throughout the day (by hour) for each day of the week. This helps identify patterns in call volume across different days and times.

```{r hour-dow-analysis}
#| label: hour-dow-analysis
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Call Volume by Hour and Day of Week"
#| cache: true

# Create summary data
hourly_dow_summary <- df |>
  group_by(DOW, Hour) |>
  summarise(call_count = n(), .groups = 'drop') |>
  mutate(Hour_numeric = as.numeric(as.character(Hour)))

# Create a heatmap showing call patterns
hour_dow_plot <- ggplot(hourly_dow_summary, aes(x = Hour_numeric, y = DOW, fill = call_count)) +
  geom_tile(color = "white", linewidth = 0.1) +
  scale_x_continuous(name = "Hour of Day",
                     breaks = seq(0, 23, 2),
                     labels = sprintf("%02d:00", seq(0, 23, 2))) +
  scale_y_discrete(name = "Day of Week", limits = rev) +
  scale_fill_gradient2(name = "Calls",
                       low = "lightblue",
                       mid = "yellow",
                       high = "red",
                       midpoint = median(hourly_dow_summary$call_count)) +
  labs(title = "Call Volume Heatmap by Hour and Day of Week",
       subtitle = "Darker colors indicate higher call volumes") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 9, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 9),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
    axis.text.y = element_text(size = 9),
    axis.title = element_text(size = 9),
    legend.position = "right",
    legend.title = element_text(size = 9),
    legend.text = element_text(size = 9),
    panel.grid = element_blank()
  )

hour_dow_plot
```

```{r alternative-ridge-plot}
#| label: alternative-ridge-plot
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Ridge Plot Alternative - Calls per Hour by Day of Week"
#| cache: true

# Try to create actual ridge plot if ggridges is available
tryCatch({
  if (requireNamespace("ggridges", quietly = TRUE)) {
    library(ggridges)

    # Create ridge plot using the raw data for a density estimate
    ridge_plot <- ggplot(df, aes(x = as.numeric(as.character(Hour)), y = DOW, fill = DOW)) +
      ggridges::geom_density_ridges(
  alpha = 0.7,
        scale = 1.2, # Adjust scale for better separation
        rel_min_height = 0.01 # Removes trailing tails
      ) +
      scale_x_continuous(name = "Hour of Day",
                         breaks = seq(0, 23, 4),
                         labels = sprintf("%02d:00", seq(0, 23, 4))) +
      scale_y_discrete(name = "Day of Week", limits = rev) +
      scale_fill_brewer(name = "Day", palette = "Set3") +
      labs(title = "Ridge Plot: Call Volume Distribution by Hour and Day of Week",
           subtitle = "Each ridge shows the hourly distribution for one day") +
      theme_minimal() +
      theme(
        plot.title = element_text(hjust = 0.5, size = 9, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, size = 9),
        axis.text = element_text(size = 9),
        axis.title = element_text(size = 9),
        legend.position = "none"
      )

    print(ridge_plot)
  } else {
    cat("ggridges package not available. Using heatmap above instead.\n")
  }
}, error = function(e) {
  cat("Could not create ridge plot. Error:", e$message, "\n")
  cat("The heatmap above provides similar insights.\n")
})
```

```{r ridge-plot-summary-stats}
#| label: ridge-plot-summary-stats
#| echo: false
#| message: false
#| warning: false
#| cache: true

# Summary statistics for calls by hour and DOW
hourly_summary <- df |>
  group_by(DOW) |>
  summarise(
    total_calls = n(),
    peak_hour = names(sort(table(Hour), decreasing = TRUE))[1],
    avg_calls_per_hour = round(n() / 24, 1),
    .groups = 'drop'
  ) |>
  arrange(desc(total_calls))

# Display summary table
to_ft(
  hourly_summary,
  caption = "Call Volume Summary by Day of Week — Peak hours and average calls per hour",
  header_map = list(
    DOW = "Day of Week",
    total_calls = "Total Calls",
    peak_hour = "Peak Hour",
    avg_calls_per_hour = "Avg Calls/Hour"
  ),
  digits = 0
)
```

The ridgeline plot shows there is a consistency throughout the week for the middle of the day to receive the most calls. The heat map shows one intense spike on Tuesday at 1000 hours. There are spikes at 1200 and 1900 hours. Overall, since `r busiest_hour`00 hours was the busiest hour of the week, the heatmap shows that it was reasonably consistent throughout the week. Equally, the heatmap shows that Friday, the second busiest day of the week, was consistent through the day and shared the same mean calls per hour as found on `r busiest_day`.

### Summary statsitcs and analyses

In this section, we will analyse the continuous variables that represent the elapsed time for various segments of the call process. The variables of interest include: Time_To_Queue, Time_To_Dispatch, Phone_Time, Processing_Time, Rollout_Time, Transit_Time, and Total_Call_Time. They are defined as follows:

- Time_To_Queue
: The time from the start of the call to the time it is released to queue for dispatch.

- Time_To_Dispatch
: The time from the time the call is released for dispatch to the time the first unit is assigned.

- Phone_Time
: The time from the start of the call to the time the phone call ended.

- Processing_Time
: The time from the start of the call until the first unit is assigned.

- Rollout_Time
: The time from the assignment of the first unit to the first unit marking en route to the call.

- Transit_Time
: The time from the first unit marking en route to the call to the first unit arriving on scene.

- Total_Call_Time
: The total time from the start of the call to the time the call was closed. If the call is re-opened, then this clock stops with the first closure.

```{r custom-summary}
#| label: custom-summary
#| echo: false
#| message: false
#| warning: false
#| cache: true

# Initialize variables
summary_table <- NULL
median_time_to_queue <- NA_real_
median_time_to_dispatch <- NA_real_

# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time",
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- df %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

    # Extract key metrics for use in text
  median_time_to_queue <- summary_table |>
  filter(Variable == "Time To Queue") |>
      pull(Median)
    median_time_to_queue_ca <- median_time_to_queue
    median_time_to_queue_ems <- median_time_to_queue
    median_time_to_queue_afd <- median_time_to_queue
    # Preserve overall median for reuse later
    median_time_to_queue_overall <- median_time_to_queue

  median_time_to_dispatch <- summary_table |>
  filter(Variable == "Time To Dispatch") |>
      pull(Median)
    median_time_to_dispatch_ca <- median_time_to_dispatch
    median_time_to_dispatch_ems <- median_time_to_dispatch
    median_time_to_dispatch_afd <- median_time_to_dispatch
    # Preserve overall median for reuse later
    median_time_to_dispatch_overall <- median_time_to_dispatch

    median_processing_time <- summary_table |>
      filter(Variable == "Processing Time") |>
      pull(Median)
    median_processing_time_ca <- median_processing_time
    median_processing_time_ems <- median_processing_time
    median_processing_time_afd <- median_processing_time
    # Preserve overall median for reuse later
    median_processing_time_overall <- median_processing_time

    median_phone_time <- summary_table |>
      filter(Variable == "Phone Time") |>
      pull(Median)
    median_phone_time_ca <- median_phone_time
    median_phone_time_ems <- median_phone_time
    median_phone_time_afd <- median_phone_time
    # Preserve overall median for reuse later
    median_phone_time_overall <- median_phone_time

    # Save overall summary table for later comparison
    summary_table_overall <<- summary_table

    to_ft(
      summary_table,
      caption = "Weekly Elapsed Time Summary Table — Statistical summary of call processing times",
      header_map = list(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median",
        Std_Dev = "Std Dev",
        Skewness = "Skew",
        Kurtosis = "Kurt"
      ),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})
```

The values from this table describe operations for the week being analyzed. In this case, the median time for a call to be placed in queue is `r median_time_to_queue` seconds. This is still in line with what has been seen in the last two weeks. The median time in queue was `r median_time_to_dispatch` seconds. These are comparable numbers with the prior weeks.

```{r ttq-plots}
#| label: ttq-plots
#| echo: false
#| message: false
#| warning: false
#| cache: true

# Helper to format seconds as mm:ss or h:mm:ss
sec_label <- function(x) {
  s <- round(x)
  h <- s %/% 3600
  m <- (s %% 3600) %/% 60
  sec <- s %% 60
  ifelse(h > 0, sprintf("%d:%02d:%02d", h, m, sec), sprintf("%d:%02d", m, sec))
}

# Prepare TTQ values in seconds and basic stats
ttq <- df |>
  transmute(ttq_sec = as.numeric(Time_To_Queue)) |>
  filter(!is.na(ttq_sec), ttq_sec >= 0)

n_ttq <- nrow(ttq)
ttq_med <- median(ttq$ttq_sec, na.rm = TRUE)
ttq_p90 <- as.numeric(quantile(ttq$ttq_sec, 0.90, na.rm = TRUE))
ttq_p99 <- as.numeric(quantile(ttq$ttq_sec, 0.99, na.rm = TRUE))
n_clipped <- sum(ttq$ttq_sec > ttq_p99, na.rm = TRUE)

# Adaptive bin width (Freedman–Diaconis) with nicening
fd <- 2 * IQR(ttq$ttq_sec, na.rm = TRUE) / (n_ttq^(1/3))
binw <- fd
if (!is.finite(binw) || binw <= 0) binw <- 5
binw <- dplyr::case_when(
  binw < 1 ~ 1,
  binw < 2 ~ 2,
  binw < 5 ~ 5,
  binw < 10 ~ 10,
  binw < 15 ~ 15,
  TRUE ~ round(binw, -1)
)

# Clip extreme tail for readability (ensure at least 20s to show reference lines)
x_max <- max(20, ttq_p99)

# Single plot: histogram (normalized to density) with density overlay and markers
ttq_hist_dens <- ggplot(ttq, aes(x = ttq_sec)) +
  # Histogram scaled to density for alignment with density curve
  geom_histogram(aes(y = after_stat(density)),
                 binwidth = binw, boundary = 0, closed = "left",
                 fill = "#1c5789", color = "white", alpha = 0.6) +
  # Density overlay
  geom_density(color = "#1c5789", fill = "#1c5789", alpha = 0.2, adjust = 1.1, linewidth = 1) +
  # Median and P90 reference lines
  geom_vline(xintercept = ttq_med, linetype = "dashed", color = "#d62728", linewidth = 0.8) +
  geom_vline(xintercept = ttq_p90, linetype = "dotted", color = "#ff7f0e", linewidth = 0.8) +
  # NENA/NFPA reference lines at 0:15 and 0:20
  geom_vline(xintercept = 15, linetype = "longdash", color = "#2ca02c", linewidth = 0.7) +
  geom_vline(xintercept = 20, linetype = "longdash", color = "#9467bd", linewidth = 0.7) +
  # Labels for markers (placed above the density peak area)
  annotate("label", x = ttq_med, y = Inf, vjust = 1.2,
           label = paste0("Median: ", round(ttq_med), "s"), size = 4, fill = "white") +
  annotate("label", x = ttq_p90, y = Inf, vjust = 1.2,
           label = paste0("P90: ", round(ttq_p90), "s"), size = 4, fill = "white") +
  annotate("label", x = 15, y = Inf, vjust = 1.2,
           label = "NENA 0:15", size = 4, fill = "white") +
  annotate("label", x = 20, y = Inf, vjust = 1.2,
           label = "NFPA 0:20", size = 4, fill = "white") +
  scale_x_continuous(name = "Time to Queue (mm:ss)",
                     limits = c(0, x_max),
                     breaks = scales::pretty_breaks(8),
                     labels = sec_label) +
  scale_y_continuous(name = "Density") +
  labs(title = "Time to Queue — Histogram with Density",
    subtitle = paste0("Histogram normalized to density; clipped at 99th percentile (", scales::comma(n_clipped), " removed).\n",
          "Median (dashed red), 90th percentile (dotted orange). Reference lines at 0:15 (NENA) and 0:20 (NFPA 1225)."),
       caption = "Data: CAD") +
  theme_minimal(base_size = 9) +
  theme(
    plot.title = element_text(face = "bold", size = 9),
    plot.subtitle = element_text(size = 9),
    axis.title.x = element_text(size = 11),
    axis.title.y = element_text(size = 11),
    axis.text.x = element_text(size = 9),
    axis.text.y = element_text(size = 9)
  )

ttq_hist_dens

```

These combined histogram and density plots are designed to show the distribution of the elapsed time between events in the call's lifecycle. Using the 90^th^ percentile, we can state that 90% of all service calls are ready to be dispatched within `r ttq_p90` seconds. The same 90^th^ lines are reflected in the remaining plots below.

```{r elapsed-time-plots}
#| label: elapsed-time-plots
#| echo: false
#| message: false
#| warning: false
#| cache: true

# Reusable plot helper: histogram normalized to density with density overlay and markers
plot_time_hist_dens <- function(data, var, title_text) {
  vals <- as.numeric(data[[var]])
  vals <- vals[is.finite(vals) & vals >= 0]
  if (length(vals) < 2) return(NULL)

  # Stats
  n <- length(vals)
  med <- median(vals)
  p90 <- as.numeric(quantile(vals, 0.90))
  p99 <- as.numeric(quantile(vals, 0.99))
  n_clipped <- sum(vals > p99)

  # Freedman–Diaconis bin width with nicening
  fd <- 2 * IQR(vals) / (n^(1/3))
  binw <- fd
  if (!is.finite(binw) || binw <= 0) binw <- 5
  binw <- dplyr::case_when(
    binw < 1 ~ 1,
    binw < 2 ~ 2,
    binw < 5 ~ 5,
    binw < 10 ~ 10,
    binw < 15 ~ 15,
    TRUE ~ round(binw, -1)
  )

  x_max <- max(20, p99)

  # mm:ss (or h:mm:ss) labels
  sec_label <- function(x) {
    s <- round(x)
    h <- s %/% 3600
    m <- (s %% 3600) %/% 60
    sec <- s %% 60
    ifelse(h > 0, sprintf("%d:%02d:%02d", h, m, sec), sprintf("%d:%02d", m, sec))
  }

  ggplot(data.frame(x = vals), aes(x = x)) +
    geom_histogram(aes(y = after_stat(density)),
                   binwidth = binw, boundary = 0, closed = "left",
                   fill = "#1c5789", color = "white", alpha = 0.6) +
    geom_density(color = "#1c5789", fill = "#1c5789", alpha = 0.2, adjust = 1.1, linewidth = 1) +
    geom_vline(xintercept = med, linetype = "dashed", color = "#d62728", linewidth = 0.8) +
    geom_vline(xintercept = p90, linetype = "dotted", color = "#ff7f0e", linewidth = 0.8) +
    geom_vline(xintercept = 15, linetype = "longdash", color = "#2ca02c", linewidth = 0.7) +
    geom_vline(xintercept = 20, linetype = "longdash", color = "#9467bd", linewidth = 0.7) +
  annotate("label", x = med, y = Inf, vjust = 1.2,
       label = "Median", size = 3.5, fill = "white") +
  annotate("label", x = p90, y = Inf, vjust = 1.2,
       label = "P90", size = 3.5, fill = "white") +
  annotate("label", x = 15, y = Inf, vjust = 1.2,
       label = "NENA", size = 3.5, fill = "white") +
  annotate("label", x = 20, y = Inf, vjust = 1.2,
       label = "NFPA", size = 3.5, fill = "white") +
    scale_x_continuous(name = "Time (mm:ss)",
                       limits = c(0, x_max),
                       breaks = scales::pretty_breaks(8),
                       labels = sec_label) +
    scale_y_continuous(name = "Density") +
    labs(title = title_text,
      subtitle = NULL,
      caption = "Data: CAD") +
    theme_minimal(base_size = 9) +
    theme(
      plot.title = element_text(face = "bold", size = 9),
      axis.title.x = element_text(size = 11),
      axis.title.y = element_text(size = 11),
      axis.text.x = element_text(size = 9),
      axis.text.y = element_text(size = 9)
    )
}

# Build and print plots for other elapsed-time metrics
p_dispatch   <- plot_time_hist_dens(df, "Time_To_Dispatch",  "Time to Dispatch — Histogram with Density")
p_phone      <- plot_time_hist_dens(df, "Phone_Time",       "Phone Time — Histogram with Density")
p_processing <- plot_time_hist_dens(df, "Processing_Time",  "Processing Time — Histogram with Density")
#p_rollout    <- plot_time_hist_dens(df, "Rollout_Time",     "Rollout Time — Histogram with Density")
#p_transit    <- plot_time_hist_dens(df, "Transit_Time",     "Transit Time — Histogram with Density")
#p_total      <- plot_time_hist_dens(df, "Total_Call_Time",  "Total Call Time — Histogram with Density")

if (!is.null(p_dispatch))   print(p_dispatch)
if (!is.null(p_phone))      print(p_phone)
if (!is.null(p_processing)) print(p_processing)
#if (!is.null(p_rollout))    print(p_rollout)
#if (!is.null(p_transit))    print(p_transit)
#if (!is.null(p_total))      print(p_total)
```

```{r elapsed-time-grid}
#| label: elapsed-time-grid
#| echo: false
#| message: false
#| warning: false
#| cache: true

# Short-title versions for grid
p_dispatch_grid   <- plot_time_hist_dens(df, "Time_To_Dispatch",  "Time to Dispatch")
p_phone_grid      <- plot_time_hist_dens(df, "Phone_Time",       "Phone Time")
p_processing_grid <- plot_time_hist_dens(df, "Processing_Time",  "Processing Time")
#p_rollout_grid    <- plot_time_hist_dens(df, "Rollout_Time",     "Rollout Time")
#p_transit_grid    <- plot_time_hist_dens(df, "Transit_Time",     "Transit Time")
#p_total_grid      <- plot_time_hist_dens(df, "Total_Call_Time",  "Total Call Time")

# Arrange elapsed-time plots in an adaptive grid (skip NULL plots)
plots_list <- list(
  p_dispatch_grid,  # Time To Dispatch
  p_phone_grid,     # Phone Time
  p_processing_grid # Processing Time
  # p_rollout_grid,   # Rollout Time
  # p_transit_grid,   # Transit Time
  # p_total_grid      # Total Call Time
)
plots_list <- Filter(Negate(is.null), plots_list)

if (length(plots_list) > 0) {
  n_plots <- length(plots_list)
  # Prefer 2x2 when exactly 4 plots; otherwise use up to 3 columns
  ncol <- if (n_plots == 4) 2 else min(3, n_plots)
  nrow <- ceiling(n_plots / ncol)
  grid <- ggpubr::ggarrange(plotlist = plots_list, ncol = ncol, nrow = nrow, align = "hv")
  print(grid)
}
```

**Plot Key:**

| Line Type/Color      | Meaning                |
|---------------------|------------------------|
| **Dashed Red**    | Median                 |
| **Dotted Orange** | 90th Percentile (P90)  |
| **Longdash Green** | NENA 0:15 Standard     |
| **Longdash Purple** | NFPA 0:20 Standard     |

These show that the processing times for DECC are well within the NENA and NFPA guidelines. This is good operational data to show how well we are performing with respect to those guidelines. Over time, we can track these metrics to ensure that we continue to meet or exceed those standards.

## Discipline Analyses

As discussed earlier, we can create additional subsets from this data to look at specific areas of interest. We will create several new datasets from this weekly set for further analysis. The first will be a dataset that combines APD Priority 1 calls with AFD Priority 1 and 2 calls and evaluates those as emergency calls. We will also create specific datasets for law, fire, and EMS for specific analyses of the disciplines. We will also create datasets that identify calls that exceed certain parameters that have been defined from other reports. Finally, because we have been evaluating Cardiac Arrest calls for some time, we'll create and analyze that dataset.

```{r new-datasets}
#| label: new-datasets
#| echo: false
#| message: false
#| warning: false
#| cache: true

df_hp <- df |>
  filter((Agency == "POLICE" & as.numeric(Priority_Number) < 2) |
         (Agency %in% c("FIRE", "EMS") & as.numeric(Priority_Number) < 3))

lw_hp <- df_last |>
  filter((Agency == "POLICE" & as.numeric(Priority_Number) < 2) |
         (Agency %in% c("FIRE", "EMS") & as.numeric(Priority_Number) < 3))

df_lp <- df |>
  filter((Agency == "POLICE" & as.numeric(Priority_Number) >= 2) |
         (Agency %in% c("FIRE", "EMS") & as.numeric(Priority_Number) >= 3))

lw_lp <- df_last |>
  filter((Agency == "POLICE" & as.numeric(Priority_Number) >= 2) |
         (Agency %in% c("FIRE", "EMS") & as.numeric(Priority_Number) >= 3))

df_law <- df |> filter(Agency == "POLICE")
df_fire <- df |> filter(Agency == "FIRE")
df_ems <- df |> filter(Agency == "EMS")
df_ttq_delay <- df_hp |> filter(Time_To_Queue > 60)
df_ttd_delay <- df_hp |> filter(Time_To_Dispatch > 60)
df_ca <- df |> filter(Problem == "CARDIAC ARREST ALS 2- SUPV")

lw_law <- df_last |> filter(Agency == "POLICE")
lw_fire <- df_last |> filter(Agency == "FIRE")
lw_ems <- df_last |> filter(Agency == "EMS")
lw_ttq_delay <- last_week |> filter(Time_To_Queue > 60)
lw_ttd_delay <- last_week |> filter(Time_To_Dispatch > 60)
lw_cs <- df_last |> filter(Problem == "CARDIAC ARREST ALS 2- SUPV")

mental_health <- c("MUTUAL PSYCHOLOGICAL EMERGENCY", "PSYCHIATRIC EMERGENCY ALS 1", "PSYCHIATRIC EMERGENCY VIOLENT", "WELFARE CHECK", "JUMPER FROM WWB", "MENTAL HEALTH CASE", "SUICIDE DELAY", "SUICIDE IN PROG NO INJ", "SUICIDE IN PROG INJ/PILLS", "SUICIDE IN PROG TRAUMA")

# Mental health related calls subset
# - Filters rows where `Problem` is one of the values in `mental_health`
# - Uses `%in%` and handles potential NA values safely with `is.na()` check
# - Creates a new dataset `df_mh` for downstream analysis
df_mh <- df |> dplyr::filter(!is.na(Problem) & Problem %in% mental_health)
lw_mh <- df_last |> dplyr::filter(!is.na(Problem) & Problem %in% mental_health)

# E-911 Calls subset
# - Filters rows where `Call_Reception` is `E-911`
# - Creates a new dataset `df_911` for downstream analysis
df_911 <- df |> dplyr::filter(!is.na(Call_Reception) & Call_Reception == "E-911")
df_non_e <- df |> dplyr::filter(!is.na(Call_Reception) & Call_Reception != "E-911")
lw_911 <- df_last |> dplyr::filter(!is.na(Call_Reception) & Call_Reception == "E-911")
lw_non_e <- df_last |> dplyr::filter(!is.na(Call_Reception) & Call_Reception != "E-911")

# Call Reception Not Recorded subset
# - Filters rows where `Call_Reception` was not recorded/captured or is NA
# - Normalize values for robust matching
df_nrr <- df |>
  dplyr::filter(
    is.na(Call_Reception) |
      toupper(trimws(Call_Reception)) %in% c("NOT RECORDED", "NOT CAPTURED")
  )

lw_nrr <- df_last |>
  dplyr::filter(
    is.na(Call_Reception) |
    toupper(trimws(Call_Reception)) %in% c("NOT RECORDED", "NOT CAPTURED")
  )
```

```{r precomputed-summaries}
#| label: precomputed-summaries
#| echo: false
#| message: false
#| warning: false
#| cache: true

# Overall
dow_counts <- df |> count(DOW, sort = TRUE)
hour_counts <- df |> count(Hour, sort = TRUE)
cr_counts <- df |> count(Call_Reception, sort = TRUE)
pri_counts <- df |> count(Priority_Number, sort = TRUE)

lw_dow_counts <- df_last |> count(DOW, sort = TRUE)
lw_hour_counts <- df_last |> count(Hour, sort = TRUE)
lw_cr_counts <- df_last |> count(Call_Reception, sort = TRUE)
lw_pri_counts <- df_last |> count(Priority_Number, sort = TRUE)

# APD / Law
dow_counts_law <- df_law |> count(DOW, sort = TRUE)
hour_counts_law <- df_law |> count(Hour, sort = TRUE)
cr_counts_law <- df_law |> count(Call_Reception, sort = TRUE)
pri_counts_law <- df_law |> count(Priority_Number, sort = TRUE)

lw_dow_counts_law <- lw_law |> count(DOW, sort = TRUE)
lw_hour_counts_law <- lw_law |> count(Hour, sort = TRUE)
lw_cr_counts_law <- lw_law |> count(Call_Reception, sort = TRUE)
lw_pri_counts_law <- lw_law |> count(Priority_Number, sort = TRUE)

# AFD / Fire
dow_counts_fire <- df_fire |> count(DOW, sort = TRUE)
hour_counts_fire <- df_fire |> count(Hour, sort = TRUE)
cr_counts_fire <- df_fire |> count(Call_Reception, sort = TRUE)
pri_counts_fire <- df_fire |> count(Priority_Number, sort = TRUE)

lw_dow_counts_fire <- lw_fire |> count(DOW, sort = TRUE)
lw_hour_counts_fire <- lw_fire |> count(Hour, sort = TRUE)
lw_cr_counts_fire <- lw_fire |> count(Call_Reception, sort = TRUE)
lw_pri_counts_fire <- lw_fire |> count(Priority_Number, sort = TRUE)

# EMS
dow_counts_ems <- df_ems |> count(DOW, sort = TRUE)
hour_counts_ems <- df_ems |> count(Hour, sort = TRUE)
cr_counts_ems <- df_ems |> count(Call_Reception, sort = TRUE)
pri_counts_ems <- df_ems |> count(Priority_Number, sort = TRUE)

lw_dow_counts_ems <- lw_ems |> count(DOW, sort = TRUE)
lw_hour_counts_ems <- lw_ems |> count(Hour, sort = TRUE)
lw_cr_counts_ems <- lw_ems |> count(Call_Reception, sort = TRUE)
lw_pri_counts_ems <- lw_ems |> count(Priority_Number, sort = TRUE)

# E-911 / Non-911
dow_counts_911 <- df_911 |> count(DOW, sort = TRUE)
hour_counts_911 <- df_911 |> count(Hour, sort = TRUE)
cr_counts_911 <- df_911 |> count(Call_Reception, sort = TRUE)

lw_dow_counts_911 <- lw_911 |> count(DOW, sort = TRUE)
lw_hour_counts_911 <- lw_911 |> count(Hour, sort = TRUE)
lw_cr_counts_911 <- lw_911 |> count(Call_Reception, sort = TRUE)

dow_counts_non911 <- df_non_e |> count(DOW, sort = TRUE)
hour_counts_non911 <- df_non_e |> count(Hour, sort = TRUE)
cr_counts_non911 <- df_non_e |> count(Call_Reception, sort = TRUE)

lw_dow_counts_non911 <- lw_non_e |> count(DOW, sort = TRUE)
lw_hour_counts_non911 <- lw_non_e |> count(Hour, sort = TRUE)
lw_cr_counts_non911 <- lw_non_e |> count(Call_Reception, sort = TRUE)

# Mental Health subset
dow_counts_mh <- df_mh |> count(DOW, sort = TRUE)
hour_counts_mh <- df_mh |> count(Hour, sort = TRUE)
cr_counts_mh <- df_mh |> count(Call_Reception, sort = TRUE)
pri_counts_mh <- df_mh |> count(Priority_Number, sort = TRUE)

lw_dow_counts_mh <- lw_mh |> count(DOW, sort = TRUE)
lw_hour_counts_mh <- lw_mh |> count(Hour, sort = TRUE)
lw_cr_counts_mh <- lw_mh |> count(Call_Reception, sort = TRUE)
lw_pri_counts_mh <- lw_mh |> count(Priority_Number, sort = TRUE)

# High-priority subset
pri_counts_hp <- df_hp |> count(Priority_Number, sort = TRUE)

lw_pri_counts_hp <- lw_hp |> count(Priority_Number, sort = TRUE)
```

By defining these datasets, we can now add to our analyses. For example, we can reuse the same information from above to drill down into APD and AFD calls. Starting with APD calls for service, we can examine everything as we did above.

### APD Analyses

```{r apd-day-of-week}
#| echo: false
#| fig-cap: "Number of calls for service by day of the week."
# ggplot2 (precomputed in precomputed-summaries)
dow_counts_law <- dow_counts_law

max_dow_info_law <- dow_counts_law |> filter(n == max(n))
busiest_day_abbr_law <- max_dow_info_law |> slice(1) |> pull(DOW)
busiest_day_count_law <- max_dow_info_law |> slice(1) |> pull(n)

min_dow_info_law <- dow_counts_law |> filter(n == min(n))
slowest_day_abbr_law <- min_dow_info_law |> slice(1) |> pull(DOW)
slowest_day_count_law <- min_dow_info_law |> slice(1) |> pull(n)

# Create mapping from abbreviations to full day names
day_names <- c(
  "SUN" = "Sunday",
  "MON" = "Monday",
  "TUE" = "Tuesday",
  "WED" = "Wednesday",
  "THU" = "Thursday",
  "FRI" = "Friday",
  "SAT" = "Saturday"
)

# Convert abbreviations to full day names
busiest_day_law <- day_names[busiest_day_abbr_law]
slowest_day_law <- day_names[slowest_day_abbr_law]

barDOW_APD <- df_law |> ggplot(aes(x = DOW, fill = DOW)) +
    geom_bar() +
    paletteer::scale_fill_paletteer_d("ggsci::blue_material") +
    labs(
        title = "Number of Calls for Service for APD by Day of the Week",
        x = "Day of the Week",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barDOW_APD
```

This week, `r busiest_day_law` was the busiest day of the week for APD service calls. `r slowest_day_law`, being the lightest day of the week overall, was the lightest day for the APD as well.

```{r apd-hour-of-day}
#| echo: false
#| fig-cap: "Number of calls for service by hour of the day."
#| cache: true
# ggplot2 (precomputed in precomputed-summaries)
hour_counts_law <- hour_counts_law

max_hour_info_law <- hour_counts_law |> filter(n == max(n))
busiest_hour_law <- sprintf("%02d", max_hour_info_law |> slice(1) |> pull(Hour))
busiest_hour_count_law <- max_hour_info_law |> slice(1) |> pull(n)

min_hour_info_law <- hour_counts_law |> filter(n == min(n))
slowest_hour_law <- sprintf("%02d", min_hour_info_law |> slice(1) |> pull(Hour))
slowest_hour_count_law <- min_hour_info_law |> slice(1) |> pull(n)

barHour_APD <- df_law |> ggplot(aes(x = Hour, fill = Hour)) +
    geom_bar() +
  # Use precomputed blue palette to avoid repeated paletteer calls
  scale_fill_manual(values = palette_blue_24) +
    labs(
        title = "Number of Calls for Service for APD by Hour of the Day",
        x = "Hour of the Day",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barHour_APD
```

The busiest hours for the week were 1000 and 1700 hours. There is still a consistent pattern to the morning and afternoon rush hours being the busiest times of the week.

```{r apd-call-reception}
#| echo: false
#| fig-cap: "Number of calls for service by call reception."
# ggplot2 (precomputed in precomputed-summaries)
cr_counts_law <- cr_counts_law

max_cr_info_law <- cr_counts_law |> filter(n == max(n))
busiest_cr_law <- max_cr_info_law |> slice(1) |> pull(Call_Reception)
busiest_cr_count_law <- max_cr_info_law |> slice(1) |> pull(n)

barReception_APD <- df_law |> ggplot(aes(x = Call_Reception, fill = Call_Reception)) +
    geom_bar() +
    paletteer::scale_fill_paletteer_d("ggsci::blue_material") +
    labs(
        title = "Number of Calls for Service for by Call Reception",
        x = "Call Reception",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barReception_APD
```

As can be seen, the majority of calls came through `r busiest_cr_law`. This stands in contrast to `r busiest_cr` being the vehicle for the most calls overall for the week.

```{r apd-call-type}
#| echo: false
#| fig-cap: "Number of calls for service by call type."
# ggplot2
prob_counts_law <- df_law |>
  count(Problem, sort = TRUE)

max_prob_info_law <- prob_counts_law |> filter(n == max(n))
busiest_prob_law <- max_prob_info_law |> slice(1) |> pull(Problem)
busiest_prob_count_law <- max_prob_info_law |> slice(1) |> pull(n)

problem_counts_APD <- df_law |>
  count(Problem, sort = TRUE) |>
  slice_head(n = 10)

barProblem_APD <- problem_counts_APD |>
    ggplot(aes(x = reorder(Problem, -n), y = n, fill = Problem)) +
    geom_bar(stat = "identity") +
    paletteer::scale_fill_paletteer_d("ggsci::blue_material") +
    labs(
        title = "Number of Calls for Service by Call Type",
        x = "Call Type",
        y = "Number of Calls"
    ) +
    geom_text(
        aes(label = n),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barProblem_APD
```

The largest call type was for `r busiest_prob_law`, which was also the largest call type for the week overall. This could be something to monitor over time to see how the trend changes over time.

```{r apd-priority-level}
#| echo: false
#| fig-cap: "Number of calls for service by priority level."
# ggplot2
pri_counts_law <- df_law |>
  count(Priority_Number, sort = TRUE)

max_pri_info_law <- pri_counts_law |> filter(n == max(n))
busiest_pri_law <- max_pri_info_law |> slice(1) |> pull(Priority_Number)
busiest_pri_count_law <- max_pri_info_law |> slice(1) |> pull(n)

# Calculate percentage for APD priority calls
busiest_pri_law_percentage <- round((sum(df_law$Priority_Number == busiest_pri_law, na.rm = TRUE) / nrow(df_law)) * 100, 1)

# Calculate percentage for P1 calls
p1_law_percentage <- round((sum(df_law$Priority_Number == 1, na.rm = TRUE) / nrow(df_law)) * 100, 1)

barPriority_APD <- df_law |> ggplot(aes(x = Priority_Number, fill = Priority_Number)) +
    geom_bar() +
    paletteer::scale_fill_paletteer_d("ggsci::blue_material") +
    labs(
        title = "Number of Calls for Service by Priority Level",
        x = "Priority Level",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barPriority_APD
```

As expected, the largest number of calls were Priority `r busiest_pri_law` calls which represent `r busiest_pri_law_percentage` percent of all APD calls. Again, this comports with the overall weekly trends.

```{r apd-custom-summary}
#| label: apd-custom-summary
#| echo: false
#| message: false
#| warning: false

# Initialize variables
summary_table <- NULL
median_phone_time <- NA_real_
median_processing_time <- NA_real_

# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time",
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_law)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- df_law %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

    # Extract key metrics for use in text
    median_time_to_queue <- summary_table |>
      filter(Variable == "Time To Queue") |>
      pull(Median)
  # Keep a namespaced copy for APD section
  median_time_to_queue_apd <- median_time_to_queue

    median_time_to_dispatch <- summary_table |>
      filter(Variable == "Time To Dispatch") |>
      pull(Median)
  # Keep a namespaced copy for APD section
  median_time_to_dispatch_apd <- median_time_to_dispatch

    median_processing_time <- summary_table |>
      filter(Variable == "Processing_Time") |>
      pull(Median)
  # Keep a namespaced copy for APD section
  median_processing_time_apd <- median_processing_time

    median_phone_time <- summary_table |>
      filter(Variable == "Phone_Time") |>
      pull(Median)
  # Keep a namespaced copy for APD section
  median_phone_time_apd <- median_phone_time

  apd_p1_calls <- df_law |>
    dplyr::filter(Priority_Number == 1)

  apd_p1_spec <- df_law |>
    dplyr::filter(Priority_Number == 1 & Time_To_Dispatch <= 120)

  apd_p2_calls <- df_law |>
    dplyr::filter(Priority_Number == 2)

  apd_p2_spec <- df_law |>
    dplyr::filter(Priority_Number == 2 & Time_To_Dispatch <= 600)

  apd_p3_calls <- df_law |>
    dplyr::filter(Priority_Number == 3)

  apd_p3_spec <- df_law |>
    dplyr::filter(Priority_Number == 3 & Time_To_Dispatch <= 1200)

  apd_p4_calls <- df_law |>
    dplyr::filter(Priority_Number == 4)

  apd_p4_spec <- df_law |>
    dplyr::filter(Priority_Number == 4 & Time_To_Dispatch <= 3600)

  apd_p1_compliance_pct <- round((nrow(apd_p1_spec) / nrow(apd_p1_calls)) * 100, 2)
  apd_p2_compliance_pct <- round((nrow(apd_p2_spec) / nrow(apd_p2_calls)) * 100, 2)
  apd_p3_compliance_pct <- round((nrow(apd_p3_spec) / nrow(apd_p3_calls)) * 100, 2)
  apd_p4_compliance_pct <- round((nrow(apd_p4_spec) / nrow(apd_p4_calls)) * 100, 2)

    to_ft(
      summary_table,
      caption = "Weekly Elapsed Time Summary Table — Statistical summary of call processing times",
      header_map = list(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median",
        Std_Dev = "Std Dev",
        Skewness = "Skew",
        Kurtosis = "Kurt"
      ),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})

# Extract key metrics for inline use (outside tryCatch to ensure they're available)
median_phone_time <- tryCatch({
  if (exists("summary_table")) {
    summary_table |> filter(Variable == "Phone Time") |> pull(Median)
  } else {
    NA_real_
  }
}, error = function(e) NA_real_)

median_processing_time <- tryCatch({
  if (exists("summary_table")) {
  summary_table |> filter(Variable == "Processing Time") |> pull(Median)
  } else {
    NA_real_
  }
}, error = function(e) NA_real_)

# Calculate P4 percentage for inline use
p4_percentage_apd <- round((sum(df_law$Priority_Number == "4", na.rm = TRUE) / nrow(df_law)) * 100, 1)
```

This table shows that overall, we have a median time on the phones of about `r median_phone_time` seconds and it takes about double that for a call to start and be dispatched, `r median_processing_time` seconds. Some of that difference is going to be due to having to hold Priority 4 and above calls until there is a unit available. Since the P4 calls are `r p4_percentage_apd` percent of APD calls, this could have a measureable impact on service times for DECC staff.

| Priority | Dispatched in SLA | Total Calls | Pct in SLA |
|:--------:|:--------:|:--------:|:--------:|
| 1 | `r nrow(apd_p1_spec)` | `r nrow(apd_p1_calls)` | `r apd_p1_compliance_pct`% |
| 2 | `r nrow(apd_p2_spec)` | `r nrow(apd_p2_calls)` | `r apd_p2_compliance_pct`% |
| 3 | `r nrow(apd_p3_spec)` | `r nrow(apd_p3_calls)` | `r apd_p3_compliance_pct`% |
| 4 | `r nrow(apd_p4_spec)` | `r nrow(apd_p4_calls)` | `r apd_p4_compliance_pct`% |

: SLA Compliance for APD Calls (P1 & P2) {.striped .hover}

As you will note, these numbers are significantly lower than those found in the other weekend reports. In those reports, there was an additional filter which removed all calls where the time in queue was longer than 30 minutes. This report reflects a more accurate view of actual compliance with our partners' SLAs. This table will be repeated for the fire and medical calls later in the report.

### AFD FIRE Analyses

Because AFD calls for service can be split into two distinct disciplines, fire-related calls and medical-related calls, DECC has, historically, separated the two disciplines for analytical purposes. So this section will look at fire-related calls for service for the week.

```{r afd-day-of-week}
#| echo: false
#| fig-cap: "Number of calls for service by day of the week."
# ggplot2 (precomputed in precomputed-summaries)
dow_counts_fire <- dow_counts_fire

max_dow_info_fire <- dow_counts_fire |> filter(n == max(n))
busiest_day_abbr_fire <- max_dow_info_fire |> slice(1) |> pull(DOW)
busiest_day_count_fire <- max_dow_info_fire |> slice(1) |> pull(n)

min_dow_info_fire <- dow_counts_fire |> filter(n == min(n))
slowest_day_abbr_fire <- min_dow_info_fire |> slice(1) |> pull(DOW)
slowest_day_count_fire <- min_dow_info_fire |> slice(1) |> pull(n)

# Create mapping from abbreviations to full day names
day_names <- c(
  "SUN" = "Sunday",
  "MON" = "Monday",
  "TUE" = "Tuesday",
  "WED" = "Wednesday",
  "THU" = "Thursday",
  "FRI" = "Friday",
  "SAT" = "Saturday"
)

# Convert abbreviations to full day names
busiest_day_fire <- day_names[busiest_day_abbr_fire]
slowest_day_fire <- day_names[slowest_day_abbr_fire]

barDOW_AFD <- df_fire |> ggplot(aes(x = DOW, fill = DOW)) +
    geom_bar() +
    paletteer::scale_fill_paletteer_d("ggsci::red_material") +
    labs(
        title = "Number of Calls for Service for AFD by Day of the Week",
        x = "Day of the Week",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barDOW_AFD
```

This week, the busiest day for fire-related calls was `r busiest_day_fire` with `r busiest_day_count_fire` calls for service. `r slowest_day_fire` was the lightest day for fire-related calls with `r slowest_day_count_fire` calls for service.

```{r afd-hour-of-day}
#| echo: false
#| fig-cap: "Number of calls for service by hour of the day."
#| cache: true
# ggplot2 (precomputed in precomputed-summaries)
hour_counts_fire <- hour_counts_fire

max_hour_info_fire <- hour_counts_fire |> filter(n == max(n))
busiest_hour_fire <- sprintf("%02d", max_hour_info_fire |> slice(1) |> pull(Hour))
busiest_hour_count_fire <- max_hour_info_fire |> slice(1) |> pull(n)

min_hour_info_fire <- hour_counts_fire |> filter(n == min(n))
slowest_hour_fire <- sprintf("%02d", min_hour_info_fire |> slice(1) |> pull(Hour))
slowest_hour_count_fire <- min_hour_info_fire |> slice(1) |> pull(n)

barHour_AFD <- df_fire |> ggplot(aes(x = Hour, fill = Hour)) +
    geom_bar() +
  # Use precomputed red palette to avoid repeated paletteer calls
  scale_fill_manual(values = palette_red_24) +
    labs(
        title = "Number of Calls for Service for AFD by Hour of the Day",
        x = "Hour of the Day",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barHour_AFD
```

Fire-related calls are much more spread out through the day as can be seen in the graph above. However, `r busiest_hour_fire`00 hours was the busiest hour for the week There hasn't been an overall trend identified in the four weeks of this report. However, we will continue to observe the patterns to see if any trends emerge in fire related calls.

```{r afd-call-reception}
#| echo: false
#| fig-cap: "Number of calls for service by call reception."
# ggplot2 (precomputed in precomputed-summaries)
cr_counts_fire <- cr_counts_fire

max_cr_info_fire <- cr_counts_fire |> filter(n == max(n))
busiest_cr_fire <- max_cr_info_fire |> slice(1) |> pull(Call_Reception)
busiest_cr_count_fire <- max_cr_info_fire |> slice(1) |> pull(n)

busiest_cr_pct_fire <- round((sum(df_fire$Call_Reception == busiest_cr_fire, na.rm = TRUE) / nrow(df_fire)) * 100, 2)
cr_phone_pct_fire <- round((sum(df_fire$Call_Reception == "Phone", na.rm = TRUE) / nrow(df_fire)) * 100, 2)

barReception_AFD <- df_fire |> ggplot(aes(x = Call_Reception, fill = Call_Reception)) +
    geom_bar() +
    paletteer::scale_fill_paletteer_d("ggsci::red_material") +
    labs(
        title = "Number of Calls for Service for AFD by Call Reception",
        x = "Call Reception",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barReception_AFD
```

The majority of fire-related calls came in via `r busiest_cr_fire`. That accounts for `r busiest_cr_pct_fire` percent of all fire-related calls. However the numbers for Mutual Aid and E-911 were larger percentages of the overall volume. In this case, Phone, not necessarily E-911 represented `r cr_phone_pct_fire` percent of all fire-related service calls received.

```{r afd-call-type}
#| echo: false
#| fig-cap: "Number of calls for service by call type."
# ggplot2
prob_counts_fire <- df_fire |>
  count(Problem, sort = TRUE)

max_prob_info_fire <- prob_counts_fire |> filter(n == max(n))
busiest_prob_fire <- max_prob_info_fire |> slice(1) |> pull(Problem)
busiest_prob_count_fire <- max_prob_info_fire |> slice(1) |> pull(n)

problem_counts_AFD <- df_fire |>
  count(Problem, sort = TRUE) |>
  slice_head(n = 10)

barProblem_AFD <- problem_counts_AFD |>
    ggplot(aes(x = reorder(Problem, -n), y = n, fill = Problem)) +
    geom_bar(stat = "identity") +
    paletteer::scale_fill_paletteer_d("ggsci::red_material") +
    labs(
        title = "Number of Calls for Service for AFD by Call Type",
        x = "Call Type",
        y = "Number of Calls"
    ) +
    geom_text(
        aes(label = n),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barProblem_AFD
```

The greatest number of fire-related service calls were for `r busiest_prob_fire`. That is an interesting observation and should be watched through the future.

```{r afd-priority-level}
#| echo: false
#| fig-cap: "Number of calls for service by priority level."
# ggplot2
pri_counts_fire <- df_fire |>
  count(Priority_Number, sort = TRUE)

max_pri_info_fire <- pri_counts_fire |> filter(n == max(n))
busiest_pri_fire <- max_pri_info_fire |> slice(1) |> pull(Priority_Number)
busiest_pri_count_fire <- max_pri_info_fire |> slice(1) |> pull(n)

# Calculate percentage for APD priority calls
busiest_pri_fire_percentage <- round((sum(df_fire$Priority_Number == busiest_pri_fire, na.rm = TRUE) / nrow(df_fire)) * 100, 1)

# Calculate percentage for P1 calls
p1_fire_percentage <- round((sum(df_fire$Priority_Number == 1, na.rm = TRUE) / nrow(df_fire)) * 100, 1)

barPriority_AFD <- df_fire |> ggplot(aes(x = Priority_Number, fill = Priority_Number)) +
    geom_bar() +
    paletteer::scale_fill_paletteer_d("ggsci::red_material") +
    labs(
        title = "Number of Calls for Service by Priority Level",
        x = "Priority Level",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barPriority_AFD
```

The most-used priority for fire-related calls was P`r busiest_pri_fire`. P1 calls account for `r p1_fire_percentage` percent of all fire-related calls this week.

```{r afd-custom-summary}
#| label: afd-custom-summary
#| echo: false
#| message: false
#| warning: false

# Initialize variables
summary_table <- NULL
median_processing_time <- NA_real_
median_phone_time <- NA_real_
mean_phone_time <- NA_real_

# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time",
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_fire)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- df_fire %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

    # Extract key metrics for use in text
    median_time_to_queue <- summary_table |>
      filter(Variable == "Time To Queue") |>
      pull(Median)

    median_time_to_dispatch <- summary_table |>
      filter(Variable == "Time To Dispatch") |>
      pull(Median)
    median_time_to_dispatch_afd <- median_time_to_dispatch

    median_processing_time <- summary_table |>
      filter(Variable == "Processing_Time") |>
      pull(Median)
    median_processing_time_afd <- median_processing_time

    median_phone_time <- summary_table |>
      filter(Variable == "Phone_Time") |>
      pull(Median)
    median_phone_time_afd <- median_phone_time

    mean_phone_time <- summary_table |>
      filter(Variable == "Phone_Time") |>
      pull(Mean)
    mean_phone_time_afd <- mean_phone_time

    fire_hp_calls <- df_fire |>
      dplyr::filter(Priority_Number <= 2)

    fire_64_calls <- df_fire |>
      dplyr::filter(Priority_Number <= 2 & Time_To_Dispatch <= 64)

    fire_106_calls <- df_fire |>
      dplyr::filter(Priority_Number <= 2 & Time_To_Dispatch <= 106)

    fire_64_compliance_pct <- round((nrow(fire_64_calls) / nrow(fire_hp_calls)) * 100, 3)
    fire_106_compliance_pct <- round((nrow(fire_106_calls) / nrow(fire_hp_calls)) * 100, 3)

    to_ft(
      summary_table,
      caption = "Weekly Elapsed Time Summary Table — Statistical summary of call processing times",
      header_map = list(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median",
        Std_Dev = "Std Dev",
        Skewness = "Skew",
        Kurtosis = "Kurt"
      ),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})
```

Overall, DECC operations appear to be very efficient at getting fire-related service calls out to the field. The median processing time was only `r median_processing_time` seconds. This shows that we can easily be in compliance with all necessary NENA and NFPA guidelines. The median time on the phone was `r median_phone_time` seconds. The mean time was `r mean_phone_time` seconds, which is still amazing.

| Calls dispatched in 64 seconds | Calls dispatched in 106 seconds | P1 & P2 Calls | Pct in 64 sec. | Pct in 106 sec. |
|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|
| `r nrow(fire_64_calls)` | `r nrow(fire_106_calls)` | `r nrow(fire_hp_calls)` | `r fire_64_compliance_pct`% | `r fire_106_compliance_pct`% |

: SLA Compliance for AFD Fire Calls (P1 & P2) {.striped .hover}

These numbers show that we are meeting and exceeding our SLAs with the AFD.

### AFD EMS Analyses

Because AFD calls for service can be split into two distinct disciplines, fire-related calls and medical-related calls, DECC has, historically, separated the two disciplines for analytical purposes. So this section will look at medical-related calls for service for the week.

```{r ems-day-of-week}
#| echo: false
#| fig-cap: "Number of calls for service by day of the week."
# ggplot2 (precomputed in precomputed-summaries)
dow_counts_ems <- dow_counts_ems

max_dow_info_ems <- dow_counts_ems |> filter(n == max(n))
busiest_day_abbr_ems <- max_dow_info_ems |> slice(1) |> pull(DOW)
busiest_day_count_ems <- max_dow_info_ems |> slice(1) |> pull(n)

min_dow_info_ems <- dow_counts_ems |> filter(n == min(n))
slowest_day_abbr_ems <- min_dow_info_ems |> slice(1) |> pull(DOW)
slowest_day_count_ems <- min_dow_info_ems |> slice(1) |> pull(n)

# Create mapping from abbreviations to full day names
day_names <- c(
  "SUN" = "Sunday",
  "MON" = "Monday",
  "TUE" = "Tuesday",
  "WED" = "Wednesday",
  "THU" = "Thursday",
  "FRI" = "Friday",
  "SAT" = "Saturday"
)

# Convert abbreviations to full day names
busiest_day_ems <- day_names[busiest_day_abbr_ems]
slowest_day_ems <- day_names[slowest_day_abbr_ems]

barDOW_EMS <- df_ems |> ggplot(aes(x = DOW, fill = DOW)) +
    geom_bar() +
    paletteer::scale_fill_paletteer_d("ggsci::green_material") +
    labs(
        title = "Number of Calls for Service for EMS by Day of the Week",
        x = "Day of the Week",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barDOW_EMS
```

 This week, there is a spike in medical calls on `r busiest_day_ems`. This appears to correlate to the information that we saw earlier in the report. Outside of `r busiest_day_ems`, the remainder of the week appears to be consistent for the number of medical calls handled.

```{r ems-hour-of-day}
#| echo: false
#| fig-cap: "Number of calls for service by hour of the day."
# ggplot2 (precomputed in precomputed-summaries)
hour_counts_ems <- hour_counts_ems

max_hour_info_ems <- hour_counts_ems |> filter(n == max(n))
busiest_hour_ems <- sprintf("%02d", max_hour_info_ems |> slice(1) |> pull(Hour))
busiest_hour_count_ems <- max_hour_info_ems |> slice(1) |> pull(n)

min_hour_info_ems <- hour_counts_ems |> filter(n == min(n))
slowest_hour_ems <- sprintf("%02d", min_hour_info_ems |> slice(1) |> pull(Hour))
slowest_hour_count_ems <- min_hour_info_ems |> slice(1) |> pull(n)

barHour_EMS <- df_ems |> ggplot(aes(x = Hour, fill = Hour)) +
    geom_bar() +
    {
        ems_base_cols <- tryCatch({
            as.character(paletteer::paletteer_d("cartography::green.pal"))
        }, error = function(e) {
            c("#00441B", "#006D2C", "#238B45", "#41AE76", "#66C2A4", "#99D8C9", "#C7E9C0", "#E5F5E0")
        })
        scale_fill_manual(values = grDevices::colorRampPalette(ems_base_cols)(nlevels(df_ems$Hour)))
    } +
    labs(
        title = "Number of Calls for Service for EMS by Hour of the Day",
        x = "Hour of the Day",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barHour_EMS
```

This week, the busiest hour was `r busiest_hour_ems` hours. The afternoon to evening hours, this week, stayed consistently busy which appears to continue the trends previously observed.

```{r ems-call-reception}
#| echo: false
#| fig-cap: "Number of calls for service by call reception."
# ggplot2 (precomputed in precomputed-summaries)
cr_counts_ems <- cr_counts_ems

max_cr_info_ems <- cr_counts_ems |> filter(n == max(n))
busiest_cr_ems <- max_cr_info_ems |> slice(1) |> pull(Call_Reception)
busiest_cr_count_ems <- max_cr_info_ems |> slice(1) |> pull(n)

cr_nr_pct_ems <- round((sum(df_ems$Call_Reception == "NOT CAPTURED") / nrow(df_ems)) * 100, 1)

barReception_EMS <- df_ems |> ggplot(aes(x = Call_Reception, fill = Call_Reception)) +
    geom_bar() +
    paletteer::scale_fill_paletteer_d("ggsci::green_material") +
    labs(
        title = "Number of Calls for Service for EMS by Call Reception",
        x = "Call Reception",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barReception_EMS
```

As expected, the vast majority of medical calls arrived via `r busiest_cr_ems`. However, `r cr_nr_pct_ems` percent of medical calls arrived without a method by which we recevied the call. We should continue to monitor and investigate why these are occurring.

```{r ems-call-type}
#| echo: false
#| fig-cap: "Number of calls for service by call type."
# ggplot2
prob_counts_med <- df_ems |>
  count(Problem, sort = TRUE)

max_prob_info_med <- prob_counts_med |> filter(n == max(n))
busiest_prob_med <- max_prob_info_med |> slice(1) |> pull(Problem)
busiest_prob_count_med <- max_prob_info_med |> slice(1) |> pull(n)

ems_ma_call <- sum(startsWith(df_ems$Problem, "MUTUAL"), na.rm = TRUE)

problem_counts_EMS <- df_ems |>
  count(Problem, sort = TRUE) |>
  slice_head(n = 10)

barProblem_EMS <- problem_counts_EMS |>
    ggplot(aes(x = reorder(Problem, -n), y = n, fill = Problem)) +
    geom_bar(stat = "identity") +
    paletteer::scale_fill_paletteer_d("ggsci::green_material") +
    labs(
        title = "Number of Calls for Service for EMS by Call Type",
        x = "Call Type",
        y = "Number of Calls"
    ) +
    geom_text(
        aes(label = n),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barProblem_EMS
```

The most prevalent medical service type this week was `r busiest_prob_med`. Further we had `r ems_ma_call` mutual aid medical calls this week.

```{r ems-priority-level}
#| echo: false
#| fig-cap: "Number of calls for service by priority level."
# ggplot2
pri_counts_ems <- df_ems |>
  count(Priority_Number, sort = TRUE)

max_pri_info_ems <- pri_counts_ems |> filter(n == max(n))
busiest_pri_ems <- max_pri_info_ems |> slice(1) |> pull(Priority_Number)
busiest_pri_count_ems <- max_pri_info_ems |> slice(1) |> pull(n)

# Calculate percentage for APD priority calls
busiest_pri_ems_percentage <- round((sum(df_ems$Priority_Number == busiest_pri_ems, na.rm = TRUE) / nrow(df_ems)) * 100, 1)

# Calculate percentage for P1 calls
p1_ems_percentage <- round((sum(df_ems$Priority_Number == 1, na.rm = TRUE) / nrow(df_ems)) * 100, 1)

barPriority_EMS <- df_ems |> ggplot(aes(x = Priority_Number, fill = Priority_Number)) +
    geom_bar() +
    paletteer::scale_fill_paletteer_d("ggsci::green_material") +
    labs(
        title = "Number of Calls for Service by Priority Level",
        x = "Priority Level",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )


barPriority_EMS
```

 The majority of medical service calls are P`r busiest_pri_ems`, which is to be expected.

```{r ems-custom-summary}
#| label: ems-custom-summary
#| echo: false
#| message: false
#| warning: false

# Initialize variables
summary_table <- NULL
median_processing_time <- NA_real_
median_phone_time <- NA_real_

# Create a summary table of elapsed time variables
# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time",
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_ems)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- df_ems %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

    # Extract key metrics for use in text
    median_time_to_queue <- summary_table |>
      filter(Variable == "Time To Queue") |>
      pull(Median)
    median_time_to_queue_ems <- median_time_to_queue

    median_time_to_dispatch <- summary_table |>
      filter(Variable == "Time To Dispatch") |>
      pull(Median)
    median_time_to_dispatch_ems <- median_time_to_dispatch

    median_processing_time <- summary_table |>
      filter(Variable == "Processing_Time") |>
      pull(Median)
    median_processing_time_ems <- median_processing_time

    median_phone_time <- summary_table |>
      filter(Variable == "Phone_Time") |>
      pull(Median)
    median_phone_time_ems <- median_phone_time

    ems_hp_calls <- df_ems |>
      dplyr::filter(Priority_Number <= 2)

    ems_64_calls <- df_ems |>
      dplyr::filter(Priority_Number <= 2 & Processing_Time <= 64)

    ems_106_calls <- df_ems |>
      dplyr::filter(Priority_Number <= 2 & Processing_Time <= 106)

    ems_64_compliance_pct <- round((nrow(ems_64_calls) / nrow(ems_hp_calls)) * 100, 1)
    ems_106_compliance_pct <- round((nrow(ems_106_calls) / nrow(ems_hp_calls)) * 100, 1)

    to_ft(
      summary_table,
      caption = "Weekly Elapsed Time Summary Table — Statistical summary of call processing times",
      header_map = list(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median",
        Std_Dev = "Std Dev",
        Skewness = "Skew",
        Kurtosis = "Kurt"
      ),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})
```

The median time to process medical calls was `r median_processing_time_ems` seconds. Again, this puts us in good form when examening our operational efficiency. The median time on phones, `r median_phone_time_ems` seconds, is longer than the overall median. That is to be expected with these calls taking longer to triage.

| Calls dispatched in 64 seconds | Calls dispatched in 106 seconds | P1 & P2 Calls | Pct in 64 sec. | Pct in 106 sec. |
|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|
| `r nrow(ems_64_calls)` | `r nrow(ems_106_calls)` | `r nrow(ems_hp_calls)` | `r ems_64_compliance_pct`% | `r ems_106_compliance_pct`% |
: SLA Compliance for AFD Medical Calls (P1 & P2) {.striped .hover}

These numbers show that there is some room for improvement in meeting the SLAs for AFD medical calls. Like the APD SLA analysis, the change in numbers can be explained by the removal of the filter for calls that remained in queue over 30 minutes. Originally, this filter was implemented to address calls that appeared to be defective, but this may not be the case and further investigation is merited.

## Additional Analyses

Earlier, for this report, we created some additional datasets that we can investigate in the course of our analysis. The first two are lists of calls where the elapsed time prior to release to queue or the time spent in dispatch is greater than 60 seconds for *emergency* calls. For the first, there are `r nrow(df_ttq_delay)` emergency service calls where the elapsed time from call start to the call entering the queue for dispatch was over 60 seconds. There are also `r nrow(df_ttd_delay)` emergency service calls where the elapsed time from entering queue to the first unit assigned was over 60 seconds.

### Possible Service Delays

We can look at the datasets and see if there are telecommunicators who may experience more challenging calls during the week. First will be a table of telecommunicators who worked emergency calls that took longer than 60 seconds to go from start to queue. The second will be a table of dispatchers who assigned an emergency call that waited in queue longer than 60 seconds.

```{r queue-too-long}
#| echo: false
#| message: false
#| warning: false

# Table: Call_Taker frequency in df_ttq_delay (descending)
library(dplyr)
library(knitr)

call_taker_counts <- df_ttq_delay %>%
  count(Call_Taker, sort = TRUE)

to_ft(
  call_taker_counts,
  caption = "Frequency of Call Taker in Delayed TTQ Calls (Descending)",
  header_map = list(Call_Taker = "Call Taker", n = "Count"),
  digits = 0
)
```

From this, since there are a small number of telecommunicators who have more than one call in the table above, there may not be any need for amerlioration. This, however, could be something that is included in the report template in order to monitor. Should a telecommunicator appear multiple times in this table over a period of time, additional training or mentoring may be called for.

```{r ttd-delay-table}
#| echo: false
#| message: false
#| warning: false

# Table: Dispatcher frequency in df_ttd_delay (descending)
dispatcher_counts <- df_ttd_delay %>%
  count(Dispatcher, sort = TRUE)

to_ft(
  dispatcher_counts,
  caption = "Frequency of Dispatcher in Delayed TTD Calls (Descending)",
  header_map = list(Dispatcher = "Dispatcher", n = "Count"),
  digits = 0
)
```

This list is fairly short and could simply be monitored in future should the need arise.

## High-Priority and Critical Calls

In this section, we will focus on the calls that are deemed high-priority or critical. This includes APD Priority 1 calls and AFD Priority 1 and 2 calls. We have identified these calls in the `df_hp` dataset created earlier.

### High-Priority Call Types

```{r hp-call-types}
#| echo: false
#| fig-cap: "Top High-Priority Call Types"
# ggplot2
hp_call_types <- df_hp |>
  count(Problem, sort = TRUE) |>
  slice_head(n = 10)

ggplot(hp_call_types, aes(x = reorder(Problem, n), y = n, fill = Problem)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  paletteer::scale_fill_paletteer_d("ggsci::deep_orange_material") +
  labs(title = "Top High-Priority Call Types",
       x = "Call Type",
       y = "Number of Calls") +
  geom_text(
    aes(label = n),
    hjust = -0.2,
    size = 3
  ) +
  theme_minimal() +
  theme(
    legend.position = "none",
    plot.title = element_text(hjust = 0.5, size = 12),
    axis.text.x = element_text(size = 9),
    axis.text.y = element_text(size = 9),
    axis.title.x = element_text(size = 11),
    axis.title.y = element_text(size = 11)
  )
```

Almost all of the problem types in this graph belong to AFD and are medical calls. Based on the information above, this is to be expected.

### High-Priority Response Times

```{r hp-call-summary}
#| label: hp-call-summary
#| echo: false
#| message: false
#| warning: false

# Initialize variables
summary_table <- NULL
median_processing_time <- NA_real_
median_phone_time <- NA_real_

# Create a summary table of elapsed time variables
# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time",
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_hp)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- df_hp %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

    # Extract key metrics for use in text
  median_time_to_queue <- summary_table |>
  filter(Variable == "Time To Queue") |>
      pull(Median)

  median_time_to_dispatch <- summary_table |>
  filter(Variable == "Time To Dispatch") |>
      pull(Median)

    median_processing_time <- summary_table |>
      filter(Variable == "Processing_Time") |>
      pull(Median)

    median_phone_time <- summary_table |>
      filter(Variable == "Phone_Time") |>
      pull(Median)


    # Always assign these for inline use
    median_phone_time_hp <- tryCatch({
      summary_table |> filter(Variable == "Phone Time") |> pull(Median)
    }, error = function(e) NA_real_)

    median_processing_time_hp <- tryCatch({
      summary_table |> filter(Variable == "Processing Time") |> pull(Median)
    }, error = function(e) NA_real_)

    to_ft(
      summary_table,
      caption = "Weekly Elapsed Time Summary Table — Statistical summary of high priority call processing times",
      header_map = list(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median",
        Std_Dev = "Std Dev",
        Skewness = "Skew",
        Kurtosis = "Kurt"
      ),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
  median_phone_time_hp <- NA_real_
  median_processing_time_hp <- NA_real_
})

```

## E-911 Service Call Analyses

This section will analyze calls that arrived to DECC via E-911 trunk lines. Last week, the median processing time for high-priority calls was `r median_processing_time_hp` seconds.

### E-911 Call Response Summary

```{r e911-call-summary}
#| label: e911-calls
#| echo: false
#| message: false
#| warning: false

# Initialize variables
summary_table <- NULL
median_processing_time <- NA_real_
median_phone_time <- NA_real_

# Create a summary table of elapsed time variables
# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time",
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_911)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- df_911 %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

    # Extract key metrics for use in text
  median_time_to_queue <- summary_table |>
  filter(Variable == "Time To Queue") |>
      pull(Median)

  median_time_to_dispatch <- summary_table |>
  filter(Variable == "Time To Dispatch") |>
      pull(Median)

    median_processing_time <- summary_table |>
      filter(Variable == "Processing_Time") |>
      pull(Median)

    median_phone_time <- summary_table |>
      filter(Variable == "Phone_Time") |>
      pull(Median)

    to_ft(
      summary_table,
      caption = "Weekly Elapsed Time Summary Table — Statistical summary of 9-1-1 call processing times",
      header_map = list(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median",
        Std_Dev = "Std Dev",
        Skewness = "Skew",
        Kurtosis = "Kurt"
      ),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})

    # Always assign these for inline use
    median_phone_time_911 <- tryCatch({
      summary_table |> filter(Variable == "Phone Time") |> pull(Median)
    }, error = function(e) NA_real_)

    median_processing_time_911 <- tryCatch({
      summary_table |> filter(Variable == "Processing Time") |> pull(Median)
    }, error = function(e) NA_real_)
```

The table above shows the information about the handling of calls that arrived by 9-1-1 trunk lines. The median time to process those calls for dispatch was `r median_processing_time_911` seconds.

```{r restore-overall-medians}
#| echo: false
# If overall aliases exist, restore the generic median variables to overall values
if (exists("median_time_to_queue_overall")) median_time_to_queue <- median_time_to_queue_overall
if (exists("median_time_to_dispatch_overall")) median_time_to_dispatch <- median_time_to_dispatch_overall
if (exists("median_processing_time_overall")) median_processing_time <- median_processing_time_overall
if (exists("median_phone_time_overall")) median_phone_time <- median_phone_time_overall
```

### E-911 Call Breakdowns

```{r 911-call-day}
#| label: 911-call-day
#| echo: false
#| fig-cap: "E-911 Call Volume by Day of the Week"
# ggplot2 (precomputed in precomputed-summaries)

max_dow_info_911 <- dow_counts_911 |> filter(n == max(n))
busiest_day_abbr_911 <- max_dow_info_911 |> slice(1) |> pull(DOW)
busiest_day_count_911 <- max_dow_info_911 |> slice(1) |> pull(n)

min_dow_info_911 <- dow_counts_911 |> filter(n == min(n))
slowest_day_abbr_911 <- min_dow_info_911 |> slice(1) |> pull(DOW)
slowest_day_count_911 <- min_dow_info_911 |> slice(1) |> pull(n)

# Create mapping from abbreviations to full day names
day_names <- c(
  "SUN" = "Sunday",
  "MON" = "Monday",
  "TUE" = "Tuesday",
  "WED" = "Wednesday",
  "THU" = "Thursday",
  "FRI" = "Friday",
  "SAT" = "Saturday"
)

# Convert abbreviations to full day names
busiest_day_911 <- day_names[busiest_day_abbr_911]
slowest_day_911 <- day_names[slowest_day_abbr_911]

barDOW_911 <- df_911 |> ggplot(aes(x = DOW, fill = DOW)) +
    geom_bar() +
    paletteer::scale_fill_paletteer_d("ggsci::indigo_material") +
    labs(
        title = "Number of Calls for Service for 911 cals by Day of the Week",
        x = "Day of the Week",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barDOW_911
```

As can be seen `r busiest_day_911` was the busiest day for E-911 calls this week. This is inline with the busiest day of the week for all calls to the center.

```{r 911-call-hour}
#| label: 911-call-hour
#| echo: false
#| fig-cap: "E-911 Call Volume by Hour of the Day"
#| warning: false
# ggplot2 (precomputed in precomputed-summaries)

hour_counts_911 <- hour_counts_911

max_hour_info_911 <- hour_counts_911 |> filter(n == max(n))
busiest_hour_numeric_911 <- max_hour_info_911 |>
  slice(1) |>
  pull(Hour) |>
  as.character() |>
  as.numeric()
busiest_hour_911 <- sprintf("%02d", busiest_hour_numeric_911)
busiest_hour_count_911 <- max_hour_info_911 |> slice(1) |> pull(n)

min_hour_info_911 <- hour_counts_911 |> filter(n == min(n))
slowest_hour_numeric_911 <- min_hour_info_911 |>
  slice(1) |>
  pull(Hour) |>
  as.character() |>
  as.numeric()
slowest_hour_911 <- sprintf("%02d", slowest_hour_numeric_911)
slowest_hour_count_911 <- min_hour_info_911 |> slice(1) |> pull(n)

barHour <- df_911 |> ggplot(aes(x=Hour, fill=Hour)) +
  geom_bar() +
  # Use ggsci::indigo_material palette with robust fallback
  {
    indigo_material_cols <- tryCatch(
      {
        as.character(paletteer::paletteer_d("ggsci::indigo_material"))
      },
      error = function(e) {
        # Fallback indigo colors if palette not available
        c("#1A237E", "#283593", "#3949AB", "#3F51B5", "#5C6BC0", "#7986CB", "#9FA8DA", "#C5CAE9")
      }
    )
    scale_fill_manual(values = grDevices::colorRampPalette(indigo_material_cols)(nlevels(df_911$Hour)))
  } +
  labs(title="Number of Calls for Service by Hour of the Day",
       x="Hour of the Day",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size = 3.5
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barHour
```

This shows that while the overall pattern for 9-1-1 calls is in keeping with the general weekly trend. However, the spike at `r busiest_hour_911`00 hours stands out. Was there a significant call for service during that hour that generated a lot of calls on the 9-1-1 lines for the floor?

### Non 9-1-1 Call Breakdowns

Additionally, we've been asked to report on calls that were received by other means than the 9-1-1 trunk lines. These will include calls arriving on the administrative trunk lines, officer generated call, walk-in calls at 3600 Wheeler, etc. There is an interest in paying attention to these calls to identify trends and determine if there are operational; improvement opportunities present in handling these non-9-1-1 calls.

```{r non-e911-call-summary}
#| label: non-e911-calls
#| echo: false
#| message: false
#| warning: false

# Initialize variables
summary_table <- NULL
median_processing_time_non911 <- NA_real_
median_phone_time_non911 <- NA_real_

# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time",
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_non_e)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <<- df_non_e %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

    # Extract key metrics for use in text (use <<- to assign to parent environment)
    median_time_to_queue_non911 <<- summary_table |>
      filter(Variable == "Time To Queue") |>
      pull(Median)

    median_time_to_dispatch_non911 <<- summary_table |>
      filter(Variable == "Time To Dispatch") |>
      pull(Median)

    median_processing_time_non911 <<- summary_table |>
      filter(Variable == "Processing Time") |>
      pull(Median)

    median_phone_time_non911 <<- summary_table |>
      filter(Variable == "Phone Time") |>
      pull(Median)

    to_ft(
      summary_table,
      caption = "Weekly Elapsed Time Summary Table — Statistical summary of non E-911 call processing times",
      header_map = list(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median",
        Std_Dev = "Std Dev",
        Skewness = "Skew",
        Kurtosis = "Kurt"
      ),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df_non_e:", paste(names(df_non_e), collapse = ", "), "\n")
})
```

It is interesting to note that the median processing time, `r median_processing_time_non911` seconds, is not that much longer than the 9-1-1 median processing time. Also interesting to note is that the median time on the phones, `r median_phone_time_non911` seconds is less than the same for E-911 calls. This is likely because the calls coming in on the E-911 lines will require more triage and support during the call's lifespan.

```{r non-911-call-day}
#| label: non-911-call-day
#| echo: false
#| fig-cap: "Non E-911 Call Volume by Day of the Week"
# ggplot2 (precomputed in precomputed-summaries)

max_dow_info_non911 <- dow_counts_non911 |> filter(n == max(n))
busiest_day_abbr_non911 <- max_dow_info_non911 |> slice(1) |> pull(DOW)
busiest_day_count_non911 <- max_dow_info_non911 |> slice(1) |> pull(n)

min_dow_info_non911 <- dow_counts_non911 |> filter(n == min(n))
slowest_day_abbr_non911 <- min_dow_info_non911 |> slice(1) |> pull(DOW)
slowest_day_count_non911 <- min_dow_info_non911 |> slice(1) |> pull(n)

# Create mapping from abbreviations to full day names
day_names <- c(
  "SUN" = "Sunday",
  "MON" = "Monday",
  "TUE" = "Tuesday",
  "WED" = "Wednesday",
  "THU" = "Thursday",
  "FRI" = "Friday",
  "SAT" = "Saturday"
)

# Convert abbreviations to full day names
busiest_day_non911 <- day_names[busiest_day_abbr_non911]
slowest_day_non911 <- day_names[slowest_day_abbr_non911]

barDOW_non911 <- df_non_e |> ggplot(aes(x = DOW, fill = DOW)) +
    geom_bar() +
    paletteer::scale_fill_paletteer_d("ggsci::indigo_material") +
    labs(
        title = "Number of Calls for Service for Non-911 cals by Day of the Week",
        x = "Day of the Week",
        y = "Number of Calls"
    ) +
    geom_text(
        stat = "count",
        aes(label = after_stat(count)),
        vjust = -0.5,
        size = 3.5
    ) +
    theme_minimal() +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11)
    )

barDOW_911
```

As can be seen `r busiest_day_non911` was the busiest day for Non-E-911 calls this week. This is inline with the busiest day of the week for all calls to the center.

```{r non-911-call-hour}
#| label: non-911-call-hour
#| echo: false
#| fig-cap: "Non-E-911 Call Volume by Hour of the Day"
#| warning: false
# ggplot2 (precomputed in precomputed-summaries)

hour_counts_non911 <- hour_counts_non911

max_hour_info_non911 <- hour_counts_non911 |> filter(n == max(n))
busiest_hour_numeric_non911 <- max_hour_info_non911 |>
  slice(1) |>
  pull(Hour) |>
  as.character() |>
  as.numeric()
busiest_hour_non911 <- sprintf("%02d", busiest_hour_numeric_non911)
busiest_hour_count_non911 <- max_hour_info_non911 |> slice(1) |> pull(n)

min_hour_info_non911 <- hour_counts_non911 |> filter(n == min(n))
slowest_hour_numeric_non911 <- min_hour_info_non911 |>
  slice(1) |>
  pull(Hour) |>
  as.character() |>
  as.numeric()
slowest_hour_non911 <- sprintf("%02d", slowest_hour_numeric_non911)
slowest_hour_count_non911 <- min_hour_info_non911 |> slice(1) |> pull(n)

barHour <- df_non_e |> ggplot(aes(x=Hour, fill=Hour)) +
  geom_bar() +
  # Use ggsci::indigo_material palette with robust fallback
  {
    indigo_material_cols <- tryCatch(
      {
        as.character(paletteer::paletteer_d("ggsci::indigo_material"))
      },
      error = function(e) {
        # Fallback indigo colors if palette not available
        c("#1A237E", "#283593", "#3949AB", "#3F51B5", "#5C6BC0", "#7986CB", "#9FA8DA", "#C5CAE9")
      }
    )
    scale_fill_manual(values = grDevices::colorRampPalette(indigo_material_cols)(nlevels(df_non_e$Hour)))
  } +
  labs(title="Number of Calls for Service by Hour of the Day",
       x="Hour of the Day",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size = 3.5
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barHour
```

## Topical Analyses

These specific analyses have been included regardless of the emergency status of the call. They include types of calls that we have been requested to monitor and looking at outlier calls for any existing patterns.

### Cardiac Arrest Calls Analysis

Finally, we will look into the specific subset of calls that are related to cardiac arrests. These calls have been identified in the `df_ca` dataset.

```{r ca-call-volume}
#| echo: false
#| fig-cap: "Cardiac Arrest Call Volume by Day and Hour"
# ggplot2
barDOW_CA <- df_ca |> ggplot(aes(x=DOW, fill=DOW)) +
  geom_bar() +
  paletteer::scale_fill_paletteer_d("ggsci::light_green_material") +
  labs(title="Number of Calls for Service for Cardiac Arrest by Day of the Week",
       x="Day of the Week",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size = 3.5
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barDOW_CA
```

As we can see, with a very limited number of cardiac arrest calls for the week.

```{r ca-response-times}
#| echo: false
#| fig-cap: "Cardiac Arrest Call Response Times"
#| warning: false
#| message: false
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time",
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_ca)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- df_ca %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

     #Extract key metrics for use in text
  median_time_to_queue <- summary_table |>
  filter(Variable == "Time To Queue") |>
      pull(Median)

  median_time_to_dispatch <- summary_table |>
  filter(Variable == "Time To Dispatch") |>
      pull(Median)

  median_processing_time <- summary_table |>
      filter(Variable == "Processing_Time") |>
      pull(Median)

  median_phone_time <- summary_table |>
      filter(Variable == "Phone_Time") |>
      pull(Median)

    to_ft(
      summary_table,
      caption = "Weekly Elapsed Time Summary Table — Statistical summary of call processing times",
      header_map = list(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median",
        Std_Dev = "Std Dev",
        Skewness = "Skew",
        Kurtosis = "Kurtosis"
      ),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})

    # Always assign these for inline use
    median_phone_time_ca <- tryCatch({
      summary_table |> filter(Variable == "Phone Time") |> pull(Median)
    }, error = function(e) NA_real_)

    median_processing_time_ca <- tryCatch({
      summary_table |> filter(Variable == "Processing Time") |> pull(Median)
    }, error = function(e) NA_real_)
```

However, we can see that the median time to process a cardiac arrest and get the units rolling is about `r median_processing_time_ca` seconds. The median time that we are on the phone is significantly longer, `r median_phone_time_ca` seconds. That is to be expected since the calltaker is likely giving T-CPR instructions while the units are en route.

### Mental Health Analyses

With the advent of Marcus' Law in Virginia, there has been an emphasis on how mental health calls are processed and serviced. The following analyses will focus on the mental health calls that have been defined as such after consultation with DCHS.

```{r mh-day-of-week}
#| echo: false
#| fig-cap: "Number of calls for service by day of the week."
# ggplot2 (precomputed in precomputed-summaries)
dow_counts_mh <- dow_counts_mh

max_dow_info_mh <- dow_counts_mh |> filter(n == max(n))
busiest_day_abbr_mh <- max_dow_info_mh |> slice(1) |> pull(DOW)
busiest_day_count_mh <- max_dow_info_mh |> slice(1) |> pull(n)

min_dow_info_mh <- dow_counts_mh |> filter(n == min(n))
slowest_day_abbr_mh <- min_dow_info_mh |> slice(1) |> pull(DOW)
slowest_day_count_mh <- min_dow_info_mh |> slice(1) |> pull(n)

# Create mapping from abbreviations to full day names
day_names <- c(
  "SUN" = "Sunday",
  "MON" = "Monday",
  "TUE" = "Tuesday",
  "WED" = "Wednesday",
  "THU" = "Thursday",
  "FRI" = "Friday",
  "SAT" = "Saturday"
)

# Convert abbreviations to full day names
busiest_day_mh <- day_names[busiest_day_abbr_mh]
slowest_day_mh <- day_names[slowest_day_abbr_mh]

barDOW_MH <- df_mh |> ggplot(aes(x=DOW, fill=DOW)) +
  geom_bar() +
  paletteer::scale_fill_paletteer_d("ggsci::purple_material") +
  labs(title="Number of Mental Health related calls by Day of the Week",
       x="Day of the Week",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size = 3.5
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barDOW_MH
```

The busiest day of the week for mental health calls was `r busiest_day_mh` with `r busiest_day_count_mh` service calls.

```{r mh-hour-of-day}
#| echo: false
#| fig-cap: "Number of calls for service by hour of the day."
# ggplot2 (precomputed in precomputed-summaries)
hour_counts_mh <- hour_counts_mh

max_hour_info_mh <- hour_counts_mh |> filter(n == max(n))
busiest_day_mh <- max_hour_info_mh |> slice(1) |> pull(Hour)
busiest_day_count_mh <- max_hour_info_mh |> slice(1) |> pull(n)

min_hour_info_mh <- hour_counts_mh |> filter(n == min(n))
slowest_day_mh <- min_hour_info_mh |> slice(1) |> pull(Hour)
slowest_day_count_mh <- min_hour_info_mh |> slice(1) |> pull(n)

barHour_MH <- df_mh |> ggplot(aes(x=Hour, fill=Hour)) +
  geom_bar() +
  # Use ggsci::purple_material palette scaled to 24 levels for hours
  {
    purple_material_cols <- tryCatch(
      {
        as.character(paletteer::paletteer_d("ggsci::purple_material"))
      },
      error = function(e) {
        # Fallback purple colors if palette not available
        c("#F3E5F5", "#E1BEE7", "#CE93D8", "#BA68C8", "#AB47BC", "#9C27B0", "#8E24AA", "#7B1FA2", "#6A1B9A", "#4A148C")
      }
    )
    scale_fill_manual(values = grDevices::colorRampPalette(purple_material_cols)(24))
  } +
  labs(title="Number of Calls for Service for APD by Hour of the Day",
       x="Hour of the Day",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size = 3.5
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barHour_MH
```

Most of these calls arrived, for this past week, in the late mornings through evenings. Again, should this data prove to be part of a trend, then we should adjust the availability of repsonders to address the community's needs.

```{r mh-call-reception}
#| echo: false
#| fig-cap: "Number of calls for service by call reception."
# ggplot2
cr_counts_mh <- df_mh |>
  count(Call_Reception, sort = TRUE)

max_cr_info_mh <- cr_counts_mh |> filter(n == max(n))
busiest_cr_mh <- max_cr_info_mh |> slice(1) |> pull(Call_Reception)
busiest_cr_count_mh <- max_cr_info_mh |> slice(1) |> pull(n)

min_cr_info_mh <- cr_counts_mh |> filter(n == min(n))
slowest_cr_mh <- min_cr_info_mh |> slice(1) |> pull(Call_Reception)
slowest_cr_count_mh <- min_cr_info_mh |> slice(1) |> pull(n)

barReception_MH <- df_mh |> ggplot(aes(x=Call_Reception, fill=Call_Reception)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Calls for Service for by Call Reception",
       x="Call Reception",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size = 3.5
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barReception_MH
```

This week, most of our mental health calls `r busiest_cr_mh` Further analysis could be understaken to determine if any of these are transfer calls from our local 988 provider partner.

```{r mh-call-type}
#| echo: false
#| fig-cap: "Number of calls for service by call type."
# ggplot2
ct_counts_mh <- df_mh |>
  count(Problem, sort = TRUE)

max_ct_info_mh <- ct_counts_mh |> filter(n == max(n))
busiest_ct_mh <- max_ct_info_mh |> slice(1) |> pull(Problem)
busiest_ct_count_mh <- max_ct_info_mh |> slice(1) |> pull(n)

min_ct_info_mh <- ct_counts_mh |> filter(n == min(n))
slowest_ct_mh <- min_ct_info_mh |> slice(1) |> pull(Problem)
slowest_ct_count_mh <- min_ct_info_mh |> slice(1) |> pull(n)

problem_counts_MH <- df_mh |>
  count(Problem, sort = TRUE) |>
  slice_head(n = 10)

barProblem_MH <- problem_counts_MH |>
  ggplot(aes(x=reorder(Problem, -n), y=n, fill=Problem)) +
  geom_bar(stat="identity") +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Calls for Service by Call Type",
       x="Call Type",
       y="Number of Calls") +
    geom_text(
        aes(label = n),
        vjust = -0.7,
        size = 3) +
    theme_minimal(base_size = 9) +
    theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9)
    )

barProblem_MH
```

The most used call type was `r busiest_ct_mh` which is expected.

```{r mh-priority-level}
#| echo: false
#| fig-cap: "Number of calls for service by priority level."
# ggplot2
pri_counts_mh <- df_mh |>
  count(Priority_Number, sort = TRUE)

max_pri_info_mh <- pri_counts_mh |> filter(n == max(n))
busiest_pri_mh <- max_pri_info_mh |> slice(1) |> pull(Priority_Number)
busiest_pri_count_mh <- max_pri_info_mh |> slice(1) |> pull(n)

min_pri_info_mh <- pri_counts_mh |> filter(n == min(n))
slowest_pri_mh <- min_pri_info_mh |> slice(1) |> pull(Priority_Number)
slowest_pri_count_mh <- min_pri_info_mh |> slice(1) |> pull(n)

barPriority_MH <- df_mh |> ggplot(aes(x=Priority_Number, fill=Priority_Number)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Calls for Service by Priority Level",
       x="Priority Level",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size = 3.5
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

barPriority_MH
```

Since `r busiest_ct_mh` was the most used call type and is a P2 call, Priority `r busiest_pri_mh` is the most used priority. The question, in the future, will be does these calls need to changed to a higher priority?

```{r mh-custom-summary}
#| label: mh-custom-summary
#| echo: false
#| message: false
#| warning: false

# Initialize variables
summary_table <- NULL
median_processing_time <- NA_real_

# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time",
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_mh)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- df_mh %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

    #Extract key metrics for use in text
  median_time_to_queue <- summary_table |>
  filter(Variable == "Time To Queue") |>
      pull(Median)

  median_time_to_dispatch <- summary_table |>
  filter(Variable == "Time To Dispatch") |>
      pull(Median)

    median_processing_time <- summary_table |>
      filter(Variable == "Processing_Time") |>
      pull(Median)

    median_phone_time <- summary_table |>
      filter(Variable == "Phone_Time") |>
      pull(Median)

    to_ft(
      summary_table,
      caption = "Weekly Elapsed Time Summary Table — Statistical summary of call processing times",
      header_map = list(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median",
        Std_Dev = "Std Dev",
        Skewness = "Skew",
        Kurtosis = "Kurt"
      ),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})

    # Always assign these for inline use
    median_phone_time_mh <- tryCatch({
      summary_table |> filter(Variable == "Phone Time") |> pull(Median)
    }, error = function(e) NA_real_)

    median_processing_time_mh <- tryCatch({
      summary_table |> filter(Variable == "Processing Time") |> pull(Median)
    }, error = function(e) NA_real_)
```

Processing times for these calls are longer, somewhere around `r median_processing_time_mh` seconds. There are several factors that can impact this. The time to make it dispatchable was longer, implying that with these types of calls, it take calltakers longer to get the information necessary in the initial triage to accurately locate and classify the call. Another possible issue, in reviewing the dispatch times is that these calls require specialized training and skill sets on the part of the field responders. If those responders are already assigned to other calls, this could create the delay as seen here. As these values change over time, we should be able to build better pictures and determine the delay points and create strategies to ameliorate them.

### Call Source Unrecorded

As discussed earlier in the document, there are `r nrow(df_nrr)` calls where the call reception method was not recorded. In order to determine if there are any operational issues behind this, we can deep dive into these calls and see what information we can obtain from them.

```{r source-unknown}
#| label: source-unknown
#| echo: false
#| message: false
#| warning: false

#| fig.width: 8
#| fig.height: 5

if (nrow(df_nrr) == 0) {
  cat("No calls with unrecorded or uncaptured call source this week.\n")
} else {
  nrr_tc_counts <- df_nrr |>
    count(Call_Taker, sort = TRUE) |>
    mutate(color_id = dplyr::row_number())

  # Guard in case Call_Taker is all NA
  if (nrow(nrr_tc_counts) == 0 || all(is.na(nrr_tc_counts$Call_Taker))) {
    cat("No call taker information available for unrecorded call sources.\n")
  } else {
    nrr_max_tc_info <- nrr_tc_counts |> dplyr::filter(n == max(n))
    nrr_busiest_tc <- nrr_max_tc_info |> dplyr::slice(1) |> dplyr::pull(Call_Taker)
    nrr_busiest_tc_count <- nrr_max_tc_info |> dplyr::slice(1) |> dplyr::pull(n)

    # Get number of call takers for proper color scaling
    n_takers <- nrow(nrr_tc_counts)

    nrr_barCallTaker <- nrr_tc_counts |>
      ggplot(aes(x=reorder(Call_Taker, -n), y=n, fill=factor(color_id))) +
      geom_bar(stat="identity") +
      scale_fill_manual(values = grDevices::colorRampPalette(
        paletteer::paletteer_d("ggsci::teal_material", n = 10))(n_takers)) +
      labs(title="Call Source Not Recorded by Call Taker",
           x="Call Taker",
           y="Number of Call Source Not Recorded Calls") +
      geom_text(
        aes(label = n),
        vjust = -0.5,
        size = 3.5) +
      theme_minimal() +
      theme(legend.position="none",
            plot.title = element_text(hjust = 0.5, size = 12),
            axis.text.x = element_text(angle=45, hjust=1, size=8),
            axis.text.y = element_text(size=8),
            axis.title.x = element_text(size=10),
            axis.title.y = element_text(size=10),
            plot.margin = margin(5, 5, 5, 5))

    nrr_barCallTaker
  }
}
```

There are a few calltakers with more than one call where the call source went unrecorded. `r nrr_busiest_tc` had the most calls without recording the source with `r nrr_busiest_tc_count` calls. We should look into those calls to see what additional factors exist that caused this to occur and work to address those.

```{r source-unknown-discipline}
#| label: source-unknown-discipline
#| echo: false
#| message: false
#| warning: false
# ggplot2
nrr_agency_counts <- df_nrr |>
  count(Agency, sort = TRUE)

nrr_max_agency_info <- nrr_agency_counts |> filter(n == max(n))
nrr_busiest_agency <- nrr_max_agency_info |> slice(1) |> pull(Agency)
nrr_busiest_agency_count <- nrr_max_agency_info |> slice(1) |> pull(n)

# Calculate percentage for Police calls
nrr_police_percentage <- round((sum(df_nrr$Agency == "POLICE", na.rm = TRUE) / nrow(df_nrr)) * 100, 1)

nrr_barDiscipline <- df_nrr |> ggplot(aes(x=Agency, fill=Agency)) +
  geom_bar() +
  scale_fill_manual(
    values = c(POLICE = "#1f77b4", FIRE = "#d62728", EMS = "#2ca02c"),
    name = "Agency"
  ) +
  labs(title="Number of Calls for Service by Discipline with no source",
       x="Discipline",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    position = position_stack(vjust = 0.5),
    size = 3.5,
    color = "white",
    fontface = "bold"
  ) +
  theme_minimal() +
  theme(legend.position="right",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.title = element_text(size = 9),
        legend.text = element_text(size = 9))

nrr_barDiscipline
```

The majority of calls with no source were for `r nrr_busiest_agency` with `r nrr_busiest_agency_count` calls for service. This week, that is substatially higher than the other two agencies combined.

```{r source-unknown-problem}
#| label: source-unknown-problem
#| echo: false
#| message: false
#| warning: false
# ggplot2

nrr_problem_counts <- df_nrr |>
  count(Problem, sort = TRUE) |>
  mutate(color_id = row_number())

# Get number of problem types for proper color scaling
n_problems <- nrow(nrr_problem_counts)

nrr_barProblem <- nrr_problem_counts |> 
  ggplot(aes(x=reorder(Problem, -n), y=n, fill=factor(color_id))) +
  geom_bar(stat="identity") +
  scale_fill_manual(values = grDevices::colorRampPalette(
    paletteer::paletteer_d("ggsci::teal_material", n = 10))(n_problems)) +
  labs(title="Number of Unrecorded calls by Call Type",
       x="Call Type",
       y="Number of Calls") +
  geom_text(
    aes(label = n),
    vjust = -0.5,
    size = 3.5) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

nrr_barProblem
```

## Outlier Identification

The calls in this set have been identified as outliers using four methods. The first is called the 'Z-score' method. This method calculates the number of standard deviations the observation is from the mean. When that number is greater than 3 or less than -3, the observation will be flagged as an outlier. The second method uses the Interquartile range (IQR), or the distance between the 25th and 75th percentiles. Each row is evaluated to see if the column in question contains a value that is greater than 1.5 times the IQR above the 75th percentile or less than 1.5 times the IQR below the 25th percentile.
The third is called the Hampel identifier. This method uses the median and the median absolute deviation (MAD) to identify outliers. The MAD is multiplied by 3 and added to and subtracted from the median to create upper and lower bounds for the data. Any value outside of these bounds is flagged as an outlier. This method is more robust than the Z-score method as it is less affected by extreme values in the data. This method was included because this data is non-parametric and we are relying on the median values in our reports. A final test that is used occasionally is to identify the rows that are in the 1st and 99th percentiles for the column in question. Each of these methods has advantages and disadvantages in use. Additionally, the IQR can pre-suppose a normal distribution of data, which as can be seen above, does not exist in our datasets. The output of this test should identify rows with values larger than the target values and very few, if any, that are lower than the target values. This is also due to the distribution of the data.

```{r outliers}
#| label: outliers
#| echo: false
#| message: false
#| warning: false

ttq_lbq <- quantile(df$Time_To_Queue, 0.025, na.rm = TRUE)
ttq_ubq <- quantile(df$Time_To_Queue, 0.975, na.rm = TRUE)
ttd_lbq <- quantile(df$Time_To_Dispatch, 0.025, na.rm = TRUE)
ttd_ubq <- quantile(df$Time_To_Dispatch, 0.975, na.rm = TRUE)
phone_lbq <- quantile(df$Phone_Time, 0.025, na.rm = TRUE)
phone_ubq <- quantile(df$Phone_Time, 0.975, na.rm = TRUE)
proc_lbq <- quantile(df$Processing_Time, 0.025, na.rm = TRUE)
proc_ubq <- quantile(df$Processing_Time, 0.975, na.rm = TRUE)

lw_ttq_lbq <- quantile(df_last$Time_To_Queue, 0.025, na.rm = TRUE)
lw_ttq_ubq <- quantile(df_last$Time_To_Queue, 0.975, na.rm = TRUE)
lw_ttd_lbq <- quantile(df_last$Time_To_Dispatch, 0.025, na.rm = TRUE)
lw_ttd_ubq <- quantile(df_last$Time_To_Dispatch, 0.975, na.rm = TRUE)
lw_phone_lbq <- quantile(df_last$Phone_Time, 0.025, na.rm = TRUE)
lw_phone_ubq <- quantile(df_last$Phone_Time, 0.975, na.rm = TRUE)
lw_proc_lbq <- quantile(df_last$Processing_Time, 0.025, na.rm = TRUE)
lw_proc_ubq <- quantile(df_last$Processing_Time, 0.975, na.rm = TRUE)

ttq_q1 <- quantile(df$Time_To_Queue, 0.25, na.rm = TRUE)
ttq_q3 <- quantile(df$Time_To_Queue, 0.75, na.rm = TRUE)
ttd_q1 <- quantile(df$Time_To_Dispatch, 0.25, na.rm = TRUE)
ttd_q3 <- quantile(df$Time_To_Dispatch, 0.75, na.rm = TRUE)
phone_q1 <- quantile(df$Phone_Time, 0.25, na.rm = TRUE)
phone_q3 <- quantile(df$Phone_Time, 0.75, na.rm = TRUE)
proc_q1 <- quantile(df$Processing_Time, 0.25, na.rm = TRUE)
proc_q3 <- quantile(df$Processing_Time, 0.75, na.rm = TRUE)

lw_ttq_q1 <- quantile(df_last$Time_To_Queue, 0.25, na.rm = TRUE)
lw_ttq_q3 <- quantile(df_last$Time_To_Queue, 0.75, na.rm = TRUE)
lw_ttd_q1 <- quantile(df_last$Time_To_Dispatch, 0.25, na.rm = TRUE)
lw_ttd_q3 <- quantile(df_last$Time_To_Dispatch, 0.75, na.rm = TRUE)
lw_phone_q1 <- quantile(df_last$Phone_Time, 0.25, na.rm = TRUE)
lw_phone_q3 <- quantile(df_last$Phone_Time, 0.75, na.rm = TRUE)
lw_proc_q1 <- quantile(df_last$Processing_Time, 0.25, na.rm = TRUE)
lw_proc_q3 <- quantile(df_last$Processing_Time, 0.75, na.rm = TRUE)

ttq_iqr <- IQR(df$Time_To_Queue, na.rm = TRUE)
ttd_iqr <- IQR(df$Time_To_Dispatch, na.rm = TRUE)
phone_iqr <- IQR(df$Phone_Time, na.rm = TRUE)
proc_iqr <- IQR(df$Processing_Time, na.rm = TRUE)

lw_ttq_iqr <- IQR(df_last$Time_To_Queue, na.rm = TRUE)
lw_ttd_iqr <- IQR(df_last$Time_To_Dispatch, na.rm = TRUE)
lw_phone_iqr <- IQR(df_last$Phone_Time, na.rm = TRUE)
lw_proc_iqr <- IQR(df_last$Processing_Time, na.rm = TRUE)

ttq_iql <- ttq_q1 - 1.5 * ttq_iqr
ttq_iqu <- ttq_q3 + 1.5 * ttq_iqr
ttd_iql <- ttd_q1 - 1.5 * ttd_iqr
ttd_iqu <- ttd_q3 + 1.5 * ttd_iqr
phone_iql <- phone_q1 - 1.5 * phone_iqr
phone_iqu <- phone_q3 + 1.5 * phone_iqr
proc_iql <- proc_q1 - 1.5 * proc_iqr
proc_iqu <- proc_q3 + 1.5 * proc_iqr

lw_ttq_iql <- lw_ttq_q1 - 1.5 * lw_ttq_iqr
lw_ttq_iqu <- lw_ttq_q3 + 1.5 * lw_ttq_iqr
lw_ttd_iql <- lw_ttd_q1 - 1.5 * lw_ttd_iqr
lw_ttd_iqu <- lw_ttd_q3 + 1.5 * lw_ttd_iqr
lw_phone_iql <- lw_phone_q1 - 1.5 * lw_phone_iqr
lw_phone_iqu <- lw_phone_q3 + 1.5 * lw_phone_iqr
lw_proc_iql <- lw_proc_q1 - 1.5 * lw_proc_iqr
lw_proc_iqu <- lw_proc_q3 + 1.5 * lw_proc_iqr

ttq_hampl <- median(df$Time_To_Queue, na.rm = TRUE) - 3*mad(df$Time_To_Queue, constant = 1, na.rm = TRUE)
ttq_hampu <- median(df$Time_To_Queue, na.rm = TRUE) + 3*mad(df$Time_To_Queue, constant = 1, na.rm = TRUE)
ttd_hampl <- median(df$Time_To_Dispatch, na.rm = TRUE) - 3*mad(df$Time_To_Dispatch, constant = 1, na.rm = TRUE)
ttd_hampu <- median(df$Time_To_Dispatch, na.rm = TRUE) + 3*mad(df$Time_To_Dispatch, constant = 1, na.rm = TRUE)
phone_hampl <- median(df$Phone_Time, na.rm = TRUE) - 3*mad(df$Phone_Time, constant = 1, na.rm = TRUE)
phone_hampu <- median(df$Phone_Time, na.rm = TRUE) + 3*mad(df$Phone_Time, constant = 1, na.rm = TRUE)
proc_hampl <- median(df$Processing_Time, na.rm = TRUE) - 3*mad(df$Processing_Time, constant = 1, na.rm = TRUE)
proc_hampu <- median(df$Processing_Time, na.rm = TRUE) + 3*mad(df$Processing_Time, constant = 1, na.rm = TRUE)

lw_ttq_hampl <- median(df_last$Time_To_Queue, na.rm = TRUE) - 3*mad(df_last$Time_To_Queue, constant = 1, na.rm = TRUE)
lw_ttq_hampu <- median(df_last$Time_To_Queue, na.rm = TRUE) + 3*mad(df_last$Time_To_Queue, constant = 1, na.rm = TRUE)
lw_ttd_hampl <- median(df_last$Time_To_Dispatch, na.rm = TRUE) - 3*mad(df_last$Time_To_Dispatch, constant = 1, na.rm = TRUE)
lw_ttd_hampu <- median(df_last$Time_To_Dispatch, na.rm = TRUE) + 3*mad(df_last$Time_To_Dispatch, constant = 1, na.rm = TRUE)
lw_phone_hampl <- median(df_last$Phone_Time, na.rm = TRUE) - 3*mad(df_last$Phone_Time, constant = 1, na.rm = TRUE)
lw_phone_hampu <- median(df_last$Phone_Time, na.rm = TRUE) + 3*mad(df_last$Phone_Time, constant = 1, na.rm = TRUE)
lw_proc_hampl <- median(df_last$Processing_Time, na.rm = TRUE) - 3*mad(df_last$Processing_Time, constant = 1, na.rm = TRUE)
lw_proc_hampu <- median(df_last$Processing_Time, na.rm = TRUE) + 3*mad(df_last$Processing_Time, constant = 1, na.rm = TRUE)

outlier_df <- df %>%
  mutate(
    TTQ_Z_Score = (as.numeric(Time_To_Queue) - mean(as.numeric(Time_To_Queue), na.rm = TRUE)) / sd(as.numeric(Time_To_Queue), na.rm = TRUE),
    TTD_Z_Score = (as.numeric(Time_To_Dispatch) - mean(as.numeric(Time_To_Dispatch), na.rm = TRUE)) / sd(as.numeric(Time_To_Dispatch), na.rm = TRUE),
    Phone_Z_Score = (as.numeric(Phone_Time) - mean(as.numeric(Phone_Time), na.rm = TRUE)) / sd(as.numeric(Phone_Time), na.rm = TRUE),
    Proc_Z_Score = (as.numeric(Processing_Time) - mean(as.numeric(Processing_Time), na.rm = TRUE)) / sd(as.numeric(Processing_Time), na.rm = TRUE),
    TTQ_Outlier_Z = ifelse(abs(TTQ_Z_Score) > 3, TRUE, FALSE),
    TTD_Outlier_Z = ifelse(abs(TTD_Z_Score) > 3, TRUE, FALSE),
    Phone_Outlier_Z = ifelse(abs(Phone_Z_Score) > 3, TRUE, FALSE),
    Proc_Outlier_Z = ifelse(abs(Proc_Z_Score) > 3, TRUE, FALSE),
    TTQ_Outlier_IQR = ifelse(Time_To_Queue < ttq_iql | Time_To_Queue > ttq_iqu, TRUE, FALSE),
    TTD_Outlier_IQR = ifelse(Time_To_Dispatch < ttd_iql | Time_To_Dispatch > ttd_iqu, TRUE, FALSE),
    Phone_Outlier_IQR = ifelse(Phone_Time < phone_iql | Phone_Time > phone_iqu, TRUE, FALSE),
    Proc_Outlier_IQR = ifelse(Processing_Time < proc_iql | Processing_Time > proc_iqu, TRUE, FALSE),
    TTQ_Outlier_Percentile = ifelse(Time_To_Queue < ttq_lbq | Time_To_Queue > ttq_ubq, TRUE, FALSE),
    TTD_Outlier_Percentile = ifelse(Time_To_Dispatch < ttd_lbq | Time_To_Dispatch > ttd_ubq, TRUE, FALSE),
    Phone_Outlier_Percentile = ifelse(Phone_Time < phone_lbq | Phone_Time > phone_ubq, TRUE, FALSE),
    Proc_Outlier_Percentile = ifelse(Processing_Time < proc_lbq | Processing_Time > proc_ubq, TRUE, FALSE),
    TTQ_Hampel = ifelse(Time_To_Queue < ttq_hampl | Time_To_Queue > ttq_hampu, TRUE, FALSE),
    TTD_Hampel = ifelse(Time_To_Dispatch < ttd_hampl | Time_To_Dispatch > ttd_hampu, TRUE, FALSE),
    Phone_Hampel = ifelse(Phone_Time < phone_hampl | Phone_Time > phone_hampu, TRUE, FALSE),
    Proc_Hampel = ifelse(Processing_Time < proc_hampl | Processing_Time > proc_hampu, TRUE, FALSE)
  ) %>%
  # Create composite flags: a row is an outlier for a variable if flagged by all 4 methods
  mutate(
    TTQ_Outlier_All = TTQ_Outlier_Z & TTQ_Outlier_IQR & TTQ_Outlier_Percentile & TTQ_Hampel,
    TTD_Outlier_All = TTD_Outlier_Z & TTD_Outlier_IQR & TTD_Outlier_Percentile & TTD_Hampel,
    Phone_Outlier_All = Phone_Outlier_Z & Phone_Outlier_IQR & Phone_Outlier_Percentile & Phone_Hampel,
    Proc_Outlier_All = Proc_Outlier_Z & Proc_Outlier_IQR & Proc_Outlier_Percentile & Proc_Hampel
  ) %>%
  # Filter to keep only rows where at least one variable is flagged by all 4 methods
  filter(
    TTQ_Outlier_All | TTD_Outlier_All | Phone_Outlier_All | Proc_Outlier_All
  )

outlier_lw <- df_last %>%
  mutate(
    LW_TTQ_Z_Score = (as.numeric(Time_To_Queue) - mean(as.numeric(Time_To_Queue), na.rm = TRUE)) / sd(as.numeric(Time_To_Queue), na.rm = TRUE),
    LW_TTD_Z_Score = (as.numeric(Time_To_Dispatch) - mean(as.numeric(Time_To_Dispatch), na.rm = TRUE)) / sd(as.numeric(Time_To_Dispatch), na.rm = TRUE),
    LW_Phone_Z_Score = (as.numeric(Phone_Time) - mean(as.numeric(Phone_Time), na.rm = TRUE)) / sd(as.numeric(Phone_Time), na.rm = TRUE),
    LW_Proc_Z_Score = (as.numeric(Processing_Time) - mean(as.numeric(Processing_Time), na.rm = TRUE)) / sd(as.numeric(Processing_Time), na.rm = TRUE),
    LW_TTQ_Outlier_Z = ifelse(abs(LW_TTQ_Z_Score) > 3, TRUE, FALSE),
    LW_TTD_Outlier_Z = ifelse(abs(LW_TTD_Z_Score) > 3, TRUE, FALSE),
    LW_Phone_Outlier_Z = ifelse(abs(LW_Phone_Z_Score) > 3, TRUE, FALSE),
    LW_Proc_Outlier_Z = ifelse(abs(LW_Proc_Z_Score) > 3, TRUE, FALSE),
    LW_TTQ_Outlier_IQR = ifelse(Time_To_Queue < lw_ttq_iql | Time_To_Queue > lw_ttq_iqu, TRUE, FALSE),
    LW_TTD_Outlier_IQR = ifelse(Time_To_Dispatch < lw_ttd_iql | Time_To_Dispatch > lw_ttd_iqu, TRUE, FALSE),
    LW_Phone_Outlier_IQR = ifelse(Phone_Time < lw_phone_iql | Phone_Time > lw_phone_iqu, TRUE, FALSE),
    LW_Proc_Outlier_IQR = ifelse(Processing_Time < lw_proc_iql | Processing_Time > lw_proc_iqu, TRUE, FALSE),
    LW_TTQ_Outlier_Percentile = ifelse(Time_To_Queue < lw_ttq_lbq | Time_To_Queue > lw_ttq_ubq, TRUE, FALSE),
    LW_TTD_Outlier_Percentile = ifelse(Time_To_Dispatch < lw_ttd_lbq | Time_To_Dispatch > lw_ttd_ubq, TRUE, FALSE),
    LW_Phone_Outlier_Percentile = ifelse(Phone_Time < lw_phone_lbq | Phone_Time > lw_phone_ubq, TRUE, FALSE),
    LW_Proc_Outlier_Percentile = ifelse(Processing_Time < lw_proc_lbq | Processing_Time > lw_proc_ubq, TRUE, FALSE),
    LW_TTQ_Hampel = ifelse(Time_To_Queue < lw_ttq_hampl | Time_To_Queue > lw_ttq_hampu, TRUE, FALSE),
    LW_TTD_Hampel = ifelse(Time_To_Dispatch < lw_ttd_hampl | Time_To_Dispatch > lw_ttd_hampu, TRUE, FALSE),
    LW_Phone_Hampel = ifelse(Phone_Time < lw_phone_hampl | Phone_Time > lw_phone_hampu, TRUE, FALSE),
    LW_Proc_Hampel = ifelse(Processing_Time < lw_proc_hampl | Processing_Time > lw_proc_hampu, TRUE, FALSE)
  ) %>%
  # Create composite flags: a row is an outlier for a variable if flagged by all 4 methods
  mutate(
    LW_TTQ_Outlier_All = LW_TTQ_Outlier_Z & LW_TTQ_Outlier_IQR & LW_TTQ_Outlier_Percentile & LW_TTQ_Hampel,
    LW_TTD_Outlier_All = LW_TTD_Outlier_Z & LW_TTD_Outlier_IQR & LW_TTD_Outlier_Percentile & LW_TTD_Hampel,
    LW_Phone_Outlier_All = LW_Phone_Outlier_Z & LW_Phone_Outlier_IQR & LW_Phone_Outlier_Percentile & LW_Phone_Hampel,
    LW_Proc_Outlier_All = LW_Proc_Outlier_Z & LW_Proc_Outlier_IQR & LW_Proc_Outlier_Percentile & LW_Proc_Hampel
  ) %>%
  # Filter to keep only rows where at least one variable is flagged by all 4 methods
  filter(
    LW_TTQ_Outlier_All | LW_TTD_Outlier_All | LW_Phone_Outlier_All | LW_Proc_Outlier_All
  )

  ttq_outliers <- df %>%
  mutate(
    TTQ_Z_Score = (as.numeric(Time_To_Queue) - mean(as.numeric(Time_To_Queue), na.rm = TRUE)) / sd(as.numeric(Time_To_Queue), na.rm = TRUE),
    TTQ_Outlier_Z = ifelse(abs(TTQ_Z_Score) > 3, TRUE, FALSE),
    TTQ_Outlier_IQR = ifelse(Time_To_Queue < ttq_iql | Time_To_Queue > ttq_iqu, TRUE, FALSE),
    TTQ_Outlier_Percentile = ifelse(Time_To_Queue < ttq_lbq | Time_To_Queue > ttq_ubq, TRUE, FALSE),
    TTQ_Hampel = ifelse(Time_To_Queue < ttq_hampl | Time_To_Queue > ttq_hampu, TRUE, FALSE)
  ) %>%
  mutate(
    TTQ_Outlier_All = TTQ_Outlier_Z & TTQ_Outlier_IQR & TTQ_Outlier_Percentile & TTQ_Hampel
  ) %>%
  filter (
    TTQ_Outlier_All
  )

lw_ttq_outliers <- df_last %>%
  mutate(
    LW_TTQ_Z_Score = (as.numeric(Time_To_Queue) - mean(as.numeric(Time_To_Queue), na.rm = TRUE)) / sd(as.numeric(Time_To_Queue), na.rm = TRUE),
    LW_TTQ_Outlier_Z = ifelse(abs(LW_TTQ_Z_Score) > 3, TRUE, FALSE),
    LW_TTQ_Outlier_IQR = ifelse(Time_To_Queue < lw_ttq_iql | Time_To_Queue > lw_ttq_iqu, TRUE, FALSE),
    LW_TTQ_Outlier_Percentile = ifelse(Time_To_Queue < lw_ttq_lbq | Time_To_Queue > lw_ttq_ubq, TRUE, FALSE),
    LW_TTQ_Hampel = ifelse(Time_To_Queue < lw_ttq_hampl | Time_To_Queue > lw_ttq_hampu, TRUE, FALSE)
  ) %>%
  mutate(
    LW_TTQ_Outlier_All = LW_TTQ_Outlier_Z & LW_TTQ_Outlier_IQR & LW_TTQ_Outlier_Percentile & LW_TTQ_Hampel
  ) %>%
  filter (
    LW_TTQ_Outlier_All
  )

ttd_outliers <- df %>%
  mutate(
    TTD_Z_Score = (as.numeric(Time_To_Dispatch) - mean(as.numeric(Time_To_Dispatch), na.rm = TRUE)) / sd(as.numeric(Time_To_Dispatch), na.rm = TRUE),
    TTD_Outlier_Z = ifelse(abs(TTD_Z_Score) > 3, TRUE, FALSE),
    TTD_Outlier_IQR = ifelse(Time_To_Dispatch < ttd_iql | Time_To_Dispatch > ttd_iqu, TRUE, FALSE),
    TTD_Outlier_Percentile = ifelse(Time_To_Dispatch < ttd_lbq | Time_To_Dispatch > ttd_ubq, TRUE, FALSE),
    TTD_Hampel = ifelse(Time_To_Dispatch < ttd_hampl | Time_To_Dispatch > ttd_hampu, TRUE, FALSE)
  ) %>%
  mutate(
    TTD_Outlier_All = TTD_Outlier_Z & TTD_Outlier_IQR & TTD_Outlier_Percentile & TTD_Hampel
  ) %>%
  filter (
    TTD_Outlier_All
  )

lw_ttd_outliers <- df_last %>%
  mutate(
    LW_TTD_Z_Score = (as.numeric(Time_To_Dispatch) - mean(as.numeric(Time_To_Dispatch), na.rm = TRUE)) / sd(as.numeric(Time_To_Dispatch), na.rm = TRUE),
    LW_TTD_Outlier_Z = ifelse(abs(LW_TTD_Z_Score) > 3, TRUE, FALSE),
    LW_TTD_Outlier_IQR = ifelse(Time_To_Dispatch < lw_ttd_iql | Time_To_Dispatch > lw_ttd_iqu, TRUE, FALSE),
    LW_TTD_Outlier_Percentile = ifelse(Time_To_Dispatch < lw_ttd_lbq | Time_To_Dispatch > lw_ttd_ubq, TRUE, FALSE),
    LW_TTD_Hampel = ifelse(Time_To_Dispatch < lw_ttd_hampl | Time_To_Dispatch > lw_ttd_hampu, TRUE, FALSE)
  ) %>%
  mutate(
    LW_TTD_Outlier_All = LW_TTD_Outlier_Z & LW_TTD_Outlier_IQR & LW_TTD_Outlier_Percentile & LW_TTD_Hampel
  ) %>%
  filter (
    LW_TTD_Outlier_All
  )

phone_outliers <- df %>%
  mutate(
    Phone_Z_Score = (as.numeric(Phone_Time) - mean(as.numeric(Phone_Time), na.rm = TRUE)) / sd(as.numeric(Phone_Time), na.rm = TRUE),
    Phone_Outlier_Z = ifelse(abs(Phone_Z_Score) > 3, TRUE, FALSE),
    Phone_Outlier_IQR = ifelse(Phone_Time < phone_iql | Phone_Time > phone_iqu, TRUE, FALSE),
    Phone_Outlier_Percentile = ifelse(Phone_Time < phone_lbq | Phone_Time > phone_ubq, TRUE, FALSE),
    Phone_Hampel = ifelse(Phone_Time < phone_hampl | Phone_Time > phone_hampu, TRUE, FALSE)
  ) %>%
    mutate(
        Phone_Outlier_All = Phone_Outlier_Z & Phone_Outlier_IQR & Phone_Outlier_Percentile & Phone_Hampel
    ) %>%
    filter (
        Phone_Outlier_All
    )

lw_phone_outliers <- df_last %>%
  mutate(
    LW_Phone_Z_Score = (as.numeric(Phone_Time) - mean(as.numeric(Phone_Time), na.rm = TRUE)) / sd(as.numeric(Phone_Time), na.rm = TRUE),
    LW_Phone_Outlier_Z = ifelse(abs(LW_Phone_Z_Score) > 3, TRUE, FALSE),
    LW_Phone_Outlier_IQR = ifelse(Phone_Time < lw_phone_iql | Phone_Time > lw_phone_iqu, TRUE, FALSE),
    LW_Phone_Outlier_Percentile = ifelse(Phone_Time < lw_phone_lbq | Phone_Time > lw_phone_ubq, TRUE, FALSE),
    LW_Phone_Hampel = ifelse(Phone_Time < lw_phone_hampl | Phone_Time > lw_phone_hampu, TRUE, FALSE)
  ) %>%
    mutate(
        LW_Phone_Outlier_All = LW_Phone_Outlier_Z & LW_Phone_Outlier_IQR & LW_Phone_Outlier_Percentile & LW_Phone_Hampel
    ) %>%
    filter (
        LW_Phone_Outlier_All
    )

proc_outliers <- df %>%
  mutate(
    Proc_Z_Score = (as.numeric(Processing_Time) - mean(as.numeric(Processing_Time), na.rm = TRUE)) / sd(as.numeric(Processing_Time), na.rm = TRUE),
    Proc_Outlier_Z = ifelse(abs(Proc_Z_Score) > 3, TRUE, FALSE),
    Proc_Outlier_IQR = ifelse(Processing_Time < proc_iql | Processing_Time > proc_iqu, TRUE, FALSE),
    Proc_Outlier_Percentile = ifelse(Processing_Time < proc_lbq | Processing_Time > proc_ubq, TRUE, FALSE),
    Proc_Hampel = ifelse(Processing_Time < proc_hampl | Processing_Time > proc_hampu, TRUE, FALSE)
  ) %>%
  mutate(
    Proc_Outlier_All = Proc_Outlier_Z & Proc_Outlier_IQR & Proc_Outlier_Percentile & Proc_Hampel
  ) %>%
  filter (
    Proc_Outlier_All
  )

lw_proc_outliers <- df_last %>%
mutate(
  LW_Proc_Z_Score = (as.numeric(Processing_Time) - mean(as.numeric(Processing_Time), na.rm = TRUE)) / sd(as.numeric(Processing_Time), na.rm = TRUE),
  LW_Proc_Outlier_Z = ifelse(abs(LW_Proc_Z_Score) > 3, TRUE, FALSE),
  LW_Proc_Outlier_IQR = ifelse(Processing_Time < lw_proc_iql | Processing_Time > lw_proc_iqu, TRUE, FALSE),
  LW_Proc_Outlier_Percentile = ifelse(Processing_Time < lw_proc_lbq | Processing_Time > lw_proc_ubq, TRUE, FALSE),
  LW_Proc_Hampel = ifelse(Processing_Time < lw_proc_hampl | Processing_Time > lw_proc_hampu, TRUE, FALSE)
) %>%
mutate(
  LW_Proc_Outlier_All = LW_Proc_Outlier_Z & LW_Proc_Outlier_IQR & LW_Proc_Outlier_Percentile & LW_Proc_Hampel
) %>%
filter (
  LW_Proc_Outlier_All
)
```

This filter identified `r nrow(outlier_df)` rows that should be investigated as outliers. We can now look at those calls specifically and examine them in detail to see why these have been collected in our outlier set. If we wish to separate out each of the different metrics, we can separate out each of the four elapsed time measures and look at their specific details. This table shows the number of outliers in each category. Please note that the individual categories may total more than the number of outliers reported in the combined dataset. A call may be considered an outlier in more than one category.

| Category | Outlier Count |
| :------: | :-----------: |
| Time To Queue | `r nrow(ttq_outliers)` |
| Time To Dispatch | `r nrow(ttd_outliers)` |
| Phone Time | `r nrow(phone_outliers)` |
| Processing Time | `r nrow(proc_outliers)` |

The volumes for each category are interesting and future analyses may give insights into different factors. Further analyses of these calls wil occur below to enhance our understanding of the information contained in these datasets.

This table will show the ranges that are used to determine which calls are considered outliers and which are not. Please note that calls that appear in the count and analyses have been identified as outliers have been identified as such in four separate tests. The only one that will not be reflected in the table below is the Z-score. That score measures the number of standard deviations from the mean for the value. All calls with a Z-score > 3 are considered possible outliers, since 99% of data should be within 3 standard deviations of the mean.

The three measures included in this table are the Percentile Tests which measures the most extreme values by identifying the calls below the 2.5th percentile or above the 97.5th percentile, the Interquartile Range which uses the 25th and 75th percentiles to calculate the range, then identifies values that are less than 1.5 times that range from the 25th percentile or greater than 1.5 times that range from the 75th percentile, and finally the Hampel Metric. This measure sets bounds at 3 times the median absolute deviation from the median, so it's similar to the Z-score.  

| Category | Percentile Lower | Percentile Upper | IQR Lower | IQR Upper | Hampel Lower | Hampel Upper |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| Time To Queue | `r format(round(ttq_lbq, 0), scientific = FALSE)` | `r format(round(ttq_ubq, 0), scientific = FALSE)` | `r format(round(ttq_iql, 0), scientific = FALSE)` | `r format(round(ttq_iqu, 0), scientific = FALSE)` | `r format(round(ttq_hampl, 0), scientific = FALSE)` | `r format(round(ttq_hampu, 0), scientific = FALSE)` |
| Time To Dispatch | `r format(round(ttd_lbq, 0), scientific = FALSE)` | `r format(round(ttd_ubq, 0), scientific = FALSE)` | `r format(round(ttd_iql, 0), scientific = FALSE)` | `r format(round(ttd_iqu, 0), scientific = FALSE)` | `r format(round(ttd_hampl, 0), scientific = FALSE)` | `r format(round(ttd_hampu, 0), scientific = FALSE)` |
| Phone Time | `r format(round(phone_lbq, 0), scientific = FALSE)` | `r format(round(phone_ubq, 0), scientific = FALSE)` | `r format(round(phone_iql, 0), scientific = FALSE)` | `r format(round(phone_iqu, 0), scientific = FALSE)` | `r format(round(phone_hampl, 0), scientific = FALSE)` | `r format(round(phone_hampu, 0), scientific = FALSE)` |
| Processing Time | `r format(round(proc_lbq, 0), scientific = FALSE)` | `r format(round(proc_ubq, 0), scientific = FALSE)` | `r format(round(proc_iql, 0), scientific = FALSE)` | `r format(round(proc_iqu, 0), scientific = FALSE)` | `r format(round(proc_hampl, 0), scientific = FALSE)` | `r format(round(proc_hampu, 0), scientific = FALSE)` |

Based on the information in the table above, all of the outlier data will come at the upper bounds of values, since the lower bounds for the interquartile range and Hampel filters are both less than zero. The largest upper bound value uses the 97.5th percentile to identify potential outliers. If we want ot change this filter in the future, it may create an increase in the number of outliers reported.

### Detailed Outliers Information

```{r ttq-outlier-graphs}
#| label: ttq-outlier-graphs
#| echo: false
#| message: false
#| warning: false
#| fig.width: 8
#| fig.height: 5

ttq_out_tc_counts <- ttq_outliers |>
  count(Call_Taker, sort = TRUE) |>
  mutate(color_id = row_number())

ttq_out_max_tc_info <- ttq_out_tc_counts |> filter(n == max(n))
ttq_out_busiest_tc <- ttq_out_max_tc_info |> slice(1) |> pull(Call_Taker)
ttq_out_busiest_tc_count <- ttq_out_max_tc_info |> slice(1) |> pull(n)

# Get number of call takers for proper color scaling
n_takers <- nrow(ttq_out_tc_counts)

ttq_out_barCallTaker <- ttq_out_tc_counts |>
  ggplot(aes(x=reorder(Call_Taker, -n), y=n, fill=factor(color_id))) +
  geom_bar(stat="identity") +
  scale_fill_manual(values = colorRampPalette(
    paletteer::paletteer_d("ggsci::blue_grey_material", n = 10))(n_takers)) +
  labs(title="Time To Queue Outliers by Call Taker",
       x="Call Taker",
       y="Number of Outlier Calls") +
  geom_text(
    aes(label = n),
    vjust = -0.5,
    size = 3.5) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size=8),
        axis.text.y = element_text(size=8),
        axis.title.x = element_text(size=10),
        axis.title.y = element_text(size=10),
        plot.margin = margin(5, 5, 5, 5))

ttq_out_barCallTaker

```

The graph shows that `r ttq_out_busiest_tc` had the most outlier calls with `r ttq_out_busiest_tc_count` calls. Overall there were a small number of call takers that had outlier values in getting a call sent for dispatch.

```{r ttd-outlier-graphs}
#| label: ttd_outlier-graphs
#| echo: false
#| message: false
#| warning: false
#| fig.width: 8
#| fig.height: 5

ttd_out_disp_counts <- ttd_outliers |>
  count(Dispatcher, sort = TRUE) |>
  mutate(color_id = row_number())

ttd_out_max_disp_info <- ttd_out_disp_counts |> filter(n == max(n))
ttd_out_busiest_disp <- ttd_out_max_disp_info |> slice(1) |> pull(Dispatcher)
ttd_out_busiest_disp_count <- ttd_out_max_disp_info |> slice(1) |> pull(n)

# Get number of dispatchers for proper color scaling
n_dispatchers <- nrow(ttd_out_disp_counts)

ttd_out_barDispatcher <- ttd_out_disp_counts |>
  ggplot(aes(x=reorder(Dispatcher, -n), y=n, fill=factor(color_id))) +
  geom_bar(stat="identity") +
  scale_fill_manual(values = colorRampPalette(
    paletteer::paletteer_d("ggsci::blue_grey_material", n = 10))(n_dispatchers)) +
  labs(title="Time To Dispatch Outliers by Dispatcher",
       x="Dispatcher",
       y="Number of Outlier Calls") +
  geom_text(
    aes(label = n),
    vjust = -0.5,
    size = 3.5) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size=8),
        axis.text.y = element_text(size=8),
        axis.title.x = element_text(size=10),
        axis.title.y = element_text(size=10),
        plot.margin = margin(5, 5, 5, 5))

ttd_out_barDispatcher

```

This graph shows the distribution of calls where the time to dispatch the first unit was considered to be an outlier value. It is interesting to note that `r ttd_out_busiest_disp` has the most outlier values. We should carefully examine those calls to see how this has happened, so those calls do not fall through the cracks.

```{r ttd-outlier-agency}
#| label: ttd_outlier_agency
#| echo: false
#| message: false
#| warning: false
# ggplot2
ttd_out_agency_counts <- ttd_outliers |>
  count(Agency, sort = TRUE)

ttd_out_max_agency_info <- ttd_out_agency_counts |> filter(n == max(n))
ttd_out_busiest_agency <- ttd_out_max_agency_info |> slice(1) |> pull(Agency)
ttd_out_busiest_agency_count <- ttd_out_max_agency_info |> slice(1) |> pull(n)

# Calculate percentage for Police calls
ttd_out_police_percentage <- round((sum(ttd_outliers$Agency == "POLICE", na.rm = TRUE) / nrow(ttd_outliers)) * 100, 1)

ttd_out_barDiscipline <- ttd_outliers |> ggplot(aes(x=Agency, fill=Agency)) +
  geom_bar() +
  scale_fill_manual(
    values = c(POLICE = "#1f77b4", FIRE = "#d62728", EMS = "#2ca02c"),
    name = "Agency"
  ) +
  labs(title="Number of Calls for Service by Agency with Large Dispatch Times",
       x="Discipline",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    position = position_stack(vjust = 0.5),
    size = 3.5,
    color = "white",
    fontface = "bold"
  ) +
  theme_minimal() +
  theme(legend.position="right",
        plot.title = element_text(hjust = 0.5, size = 12),
        axis.text.x = element_text(angle=45, hjust=1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.title = element_text(size = 9),
        legend.text = element_text(size = 9))

ttd_out_barDiscipline

```

As expected, all of the calls that have an outlier in the time to dispatch the calls belonged to `r ttd_out_busiest_agency`. This was expected because AFD FIRE and EMS calls use auto-dispatch and rarely wait for more than a few seconds.

### Outlier Impact

Below is an illustration of the impact that the outliers that we've identified have on the overall numbers we've reported.

```{r outlier-summary}
#| label: outlier-summary
#| echo: false
#| message: false
#| warning: false

# Create row index column for joining
df_indexed <- df %>% mutate(.row_id = row_number())
outlier_df_indexed <- outlier_df %>% mutate(.row_id = row_number())

# Get the original row numbers from df that are in outlier_df
outlier_rows <- which(seq_len(nrow(df)) %in% as.integer(rownames(outlier_df)))

# Create non-outlier dataframe by excluding those rows
nonoutlier_df <- df[-outlier_rows, ]

# Initialize variables outside tryCatch so they're always available
summary_table <- NULL
median_processing_time_nout <- NA_real_
median_phone_time_nout <- NA_real_
median_time_to_queue_nout <- NA_real_
median_time_to_dispatch_nout <- NA_real_
median_ttq_nout <- NA_real_
median_ttd_nout <- NA_real_

# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time",
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(nonoutlier_df)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- nonoutlier_df %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

    # Extract key metrics for use in text (use <<- to assign to parent environment)
    median_time_to_queue_nout <<- summary_table |>
      filter(Variable == "Time To Queue") |>
      pull(Median)

    median_time_to_dispatch_nout <<- summary_table |>
      filter(Variable == "Time To Dispatch") |>
      pull(Median)

    median_processing_time_nout <<- summary_table |>
      filter(Variable == "Processing Time") |>
      pull(Median)

    median_phone_time_nout <<- summary_table |>
      filter(Variable == "Phone Time") |>
      pull(Median)

    median_ttq_nout <<- median_time_to_queue_nout
    median_ttd_nout <<- median_time_to_dispatch_nout

    to_ft(
      summary_table,
      caption = "Non-Outlier Elapsed Time Summary — Statistical summary of call processing times excluding outliers",
      header_map = list(
        Variable = "Time Metric",
        Minimum = "Min",
        Mean = "Mean",
        Median = "Median",
        Std_Dev = "Std Dev",
        Skewness = "Skew",
        Kurtosis = "Kurt"
      ),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in nonoutlier_df:", paste(names(nonoutlier_df), collapse = ", "), "\n")
})
```

### Comparison: Overall vs. Outlier Statistics

For comparison purposes, here is the overall summary table again:

```{r overall-summary-comparison}
#| label: overall-summary-comparison
#| echo: false
#| message: false
#| warning: false

# Display the overall summary table preserved from earlier
if (exists("summary_table_overall")) {
  to_ft(
    summary_table_overall,
    caption = "Overall Elapsed Time Summary — Statistical summary of all call processing times",
    header_map = list(
      Variable = "Time Metric",
      Minimum = "Min",
      Mean = "Mean",
      Median = "Median",
      Std_Dev = "Std Dev",
      Skewness = "Skew",
      Kurtosis = "Kurt"
    ),
    digits = 2
  )
}
```

It is interesting to see that removing the outliers had the opposite effect and seems to show that they do not have a significant impact on our summary statistics. This is likely because we have only removed `r nrow(outlier_df)` rows from the dataset when recalculating

After this summary table, we can measure the difference between the original medians and the medians after the `r nrow(outlier_df)` outlier rows were removed. This table shows that impact.

| Time Point | Overall Median | Post-Outlier Median | Impact in Seconds |
| :--------: | :------------: | :-----------------: | :---------------: |
| Time To Queue | `r median_time_to_queue_overall` | `r median_ttq_nout` | `r median_time_to_queue_overall - median_ttq_nout` |
| Time To Dispatch | `r median_time_to_dispatch_overall` | `r median_ttd_nout` | `r median_time_to_dispatch_overall - median_ttd_nout` |
| Phone Time | `r median_phone_time_overall` | `r median_phone_time_nout` | `r median_phone_time_overall - median_phone_time_nout` |
| Processing Time | `r median_processing_time_overall` | `r median_processing_time_nout` | `r median_processing_time_overall - median_processing_time_nout` |

There is little impact on the median times because the median value is less sensitive to the impact of outliers.

## Comparisons

This section is meant to compare this report week's data with the week prior. This will allow us to view some trend information and allow us to generate additional insights. For example there were `r nrow(current_week)` service calls this week and `r nrow(last_week)` service calls the week prior, this represents a change of `r nrow(current_week) - nrow(last_week)` calls from the week prior to last week. When the delta is positive, we had more calls last week, than the week prior. If the delta is negative, we had fewer service calls last week than the week prior.

### General Comparisons

This table will show the differences for some of the collected statistics.

| Statistic | Report Week | Previous Week | Delta | Pct. Change |
| :-------: | :---------: | :-----------: | :---: | :---------: |
| Call Count | `r nrow(current_week)` | `r nrow(last_week)` | `r nrow(current_week) - nrow(last_week)` | `r round((nrow(current_week) - nrow(last_week))/nrow(last_week) * 100, 2)`% |
| APD Calls | `r sum(current_week$Agency == "POLICE")` | `r sum(last_week$Agency == "POLICE")` | `r sum(current_week$Agency == "POLICE") - sum(last_week$Agency == "POLICE")` | `r round((sum(current_week$Agency == "POLICE") - sum(last_week$Agency == "POLICE"))/sum(last_week$Agency == "POLICE") * 100, 2)`% |
| AFD Fire Calls | `r sum(current_week$Agency == "FIRE")` | `r sum(last_week$Agency == "FIRE")` | `r sum(current_week$Agency == "FIRE") - sum(last_week$Agency == "FIRE")` | `r round((sum(current_week$Agency == "FIRE") - sum(last_week$Agency == "FIRE"))/sum(last_week$Agency == "FIRE") * 100, 2)`% |
| AFD EMS Calls | `r sum(current_week$Agency == "EMS")` | `r sum(last_week$Agency == "EMS")` | `r sum(current_week$Agency == "EMS") - sum(last_week$Agency == "EMS")` | `r round((sum(current_week$Agency == "EMS") - sum(last_week$Agency == "EMS"))/sum(last_week$Agency == "EMS") * 100, 2)`% |

As we can see, there were a few more calls last week than the week prior. APD calls were also up, by a higher percentage than the general call volume. Interesting to note, but there was no difference between the two weeks for AFD non-medical calls. Medical calls were down over 7% from the week prior.

### Outlier Comparisons

This table will show the differences in the numbers of likely outlier values from the last two weeks. 

| Statistic | Report Week | Previous Week | Delta | Pct. Change |
| :-------: | :---------: | :-----------: | :---: | :---------: |
| Time To Queue | `r nrow(ttq_outliers)` | `r nrow(lw_ttq_outliers)` | `r nrow(ttq_outliers) - nrow(lw_ttq_outliers)` | `r round((nrow(ttq_outliers) - nrow(lw_ttq_outliers))/nrow(lw_ttq_outliers) * 100, 2)`% |
| Time To Dispatch | `r nrow(ttd_outliers)` | `r nrow(lw_ttd_outliers)` | `r nrow(ttd_outliers) - nrow(lw_ttd_outliers)` | `r round((nrow(ttd_outliers) - nrow(lw_ttd_outliers))/nrow(lw_ttd_outliers) * 100, 2)`% |
| Phone Time | `r nrow(phone_outliers)` | `r nrow(lw_phone_outliers)` | `r nrow(phone_outliers) - nrow(lw_phone_outliers)` | `r round((nrow(phone_outliers) - nrow(lw_phone_outliers))/nrow(lw_phone_outliers) * 100, 2)`% |
| Processing Time | `r nrow(proc_outliers)` | `r nrow(lw_proc_outliers)` | `r nrow(proc_outliers) - nrow(lw_proc_outliers)` | `r round((nrow(proc_outliers) - nrow(lw_proc_outliers))/nrow(lw_proc_outliers) * 100, 2)`% |

From this table, we can see that the number of calls where we've identified a potential outlier value have come down significantly last week from the week prior. The largest decline was in the time taken from phone pickup to dispatch ready. There are, possibly, several reasons behind this and this could be a study in trend analysis in the future.

## Conclusion

This report has covered various aspects of the calls for service during week `r WEEK_NUMBER`. We have analyzed the data for completeness and accuracy, explored it for insights into call patterns and trends, and focused on specific areas of interest such as high-priority calls and cardiac arrest incidents. The findings will assist in making informed decisions to improve service delivery and operational efficiency.
