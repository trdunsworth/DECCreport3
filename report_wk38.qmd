---
title: "DECC Weekly Report"
author: "Tony Dunsworth, PhD"
date: "2025-09-23"
format: 
  docx:
    toc: false
    toc-depth: 3
    number-sections: true
    fig-width: 6
    fig-height: 4
    fig-align: center
    code-fold: true
    code-summary: "Click to show/hide code"
    code-line-numbers: true
    highlight-style: "tango"
    fontsize: 12pt
    margin-left: 1in
    margin-right: 1in
    margin-top: 1in
    margin-bottom: 1in
    documentclass: article
    linestretch: 1.5
    keep-md: true
    md_extensions: +autolink_bare_uris
    prefer-html: true
---

## Week 38 covering 14 September through 20 September 2025

## Table of Contents

### Main Sections

- **[Introduction](#introduction)**
- **[Data Cleaning](#data-cleaning)**
- **[Exploratory Analysis](#exploratory-analysis)**
  - [Call Distribution: Hour by Day of Week](#call-distribution-hour-by-day-of-week)
  - [Summary Statistics and Analyses](#summary-statsitcs-and-analyses)
- **[Discipline Analyses](#discipline-analyses)**
  - [APD Analyses](#apd-analyses)
  - [AFD FIRE Analyses](#afd-fire-analyses)
  - [AFD EMS Analyses](#afd-ems-analyses)
- **[Additional Analyses](#additional-analyses)**
  - [Possible Service Delays](#possible-service-delays)
- **[High-Priority and Critical Calls](#high-priority-and-critical-calls)**
  - [High-Priority Call Types](#high-priority-call-types)
  - [High-Priority Response Times](#high-priority-response-times)
- **[Cardiac Arrest Calls Analysis](#cardiac-arrest-calls-analysis)**
- **[Mental Health Analyses](#mental-health-analyses)**
- **[Conclusion](#conclusion)**

---

```{r libraries}
#| label: setup
#| echo: false
#| message: false
#| warning: false

# Suppress dbus warnings that can cause preview issues
suppressWarnings({
  suppressPackageStartupMessages({
    library(tidyverse)
    library(tidymodels)
    library(devtools)
    library(remotes)
    library(ggpubr)
    library(ggrepel)
    library(ggraph)
    library(gt)
    library(gtExtras)
    library(GGally)
    library(rstatix)
    library(car)
    library(janitor)
    library(Hmisc)
    library(psych)
    library(corrr)
    library(ggcorrplot)
    library(ggthemes)
    library(ggridges)
    library(multcomp)
    library(emmeans)
    library(RVAideMemoire)
    library(FactoMineR)
    library(DescTools)
    library(nlme)
    library(funModeling)
    library(inspectdf)
    library(dlookr)
    library(viridis)
    library(merTools)
    library(factoextra)
    library(nortest)
    library(MASS)
    library(randtests)
    library(summarytools)
    library(report)
    library(knitr)
    library(kableExtra)
    library(modelbased)
    library(parameters)
    library(performance)
    library(insight)
    library(lubridate)
    library(broom)
    library(GPfit)
    library(survival)
  })
})

# Define week number for dynamic title generation
WEEK_NUMBER <- 38

# Calculate week dates properly for 2025
# Week 38 should be Sept 14-20, 2025 (Sunday to Saturday) - CORRECTED
# Sunday-first week calculation

# Method 1: Direct date specification for week 38
if(WEEK_NUMBER == 38) {
  WEEK_START_DATE <- as.Date("2025-09-14")  # Sunday Sept 14 (CORRECTED)
  WEEK_END_DATE <- as.Date("2025-09-20")    # Saturday Sept 20 (CORRECTED)
} else {
  # For other weeks, we can use a more general calculation
  # Find the first Sunday of 2025 (which is actually Dec 29, 2024)
  jan_1_2025 <- as.Date("2025-01-01")
  # January 1, 2025 was a Wednesday
  # First Sunday is the Sunday before or on Jan 1
  first_sunday_2025 <- jan_1_2025 - days(wday(jan_1_2025) - 1)  # Dec 29, 2024
  
  # Calculate the start date for the specified week
  WEEK_START_DATE <- first_sunday_2025 + weeks(WEEK_NUMBER - 1)
  WEEK_END_DATE <- WEEK_START_DATE + days(6)
}

# Format dates for display
WEEK_START_FORMATTED <- format(WEEK_START_DATE, "%d %b")
WEEK_END_FORMATTED <- format(WEEK_END_DATE, "%d %b")

# Update title dynamically (since we can't use R in YAML)
# Note: You'll need to manually update the YAML title, or use this for reference
DYNAMIC_TITLE <- paste0("Weekly Report: Week ", WEEK_NUMBER, " (", WEEK_START_FORMATTED, " through ", WEEK_END_FORMATTED, ")")

# Print the calculated dates for verification
# cat("Week", WEEK_NUMBER, "dates:", WEEK_START_FORMATTED, "through", WEEK_END_FORMATTED, "\n")
```

```{r table-helpers}
#| label: table-helpers
#| echo: false
#| message: false
#| warning: false

suppressPackageStartupMessages({
  library(flextable)
  library(officer)
})

# Consistent, accessible Word tables
to_ft <- function(x, caption = NULL, header_map = NULL, digits = NULL) {
  ft <- flextable::flextable(x)
  if (!is.null(header_map)) {
    ft <- flextable::set_header_labels(ft, values = header_map)
  }
  ft <- flextable::theme_booktabs(ft)
  # zebra striping on body rows
  if (nrow(x) > 1) {
    ft <- flextable::bg(ft, i = seq(2, nrow(x), by = 2), bg = "#f7f7f7", part = "body")
  }
  # header emphasis
  ft <- flextable::bold(ft, part = "header")
  ft <- flextable::fontsize(ft, part = "header", size = 12)
  # numeric formatting
  num_cols <- names(Filter(is.numeric, x))
  if (length(num_cols) > 0) {
    if (is.null(digits)) {
      ft <- flextable::colformat_num(ft, col_keys = num_cols, big.mark = ",")
    } else {
      ft <- flextable::colformat_num(ft, col_keys = num_cols, digits = digits, big.mark = ",")
    }
    ft <- flextable::align(ft, j = num_cols, align = "right", part = "body")
  }
  if (!is.null(caption)) ft <- flextable::set_caption(ft, caption)
  ft <- flextable::autofit(ft)
  ft
}
```

## Introduction

This is the weekly report for week `r WEEK_NUMBER` covering the period from `r WEEK_START_FORMATTED` through `r WEEK_END_FORMATTED` 2025. The report will include analyses of the data to emphasize different information that is contained within the data and may be pertinent to both operations and management.

```{r data-load}
#| echo: false
#| output: false

# Dynamic data file loading with error handling
# Check for different naming conventions
possible_files <- c(
  paste0("data\\week", WEEK_NUMBER, ".csv"),
  paste0("data\\Week", WEEK_NUMBER, "_25.csv"),
  paste0("data\\Week", WEEK_NUMBER, ".csv")
)

# Find the first file that exists
data_file <- NULL
for(file in possible_files) {
  if(file.exists(file)) {
    data_file <- file
    break
  }
}

if(is.null(data_file)) {
  stop("No data file found for week ", WEEK_NUMBER, ". Checked: ", paste(possible_files, collapse = ", "))
}

cat("Loading data from:", data_file, "\n")

# Use read_csv with error handling and show parsing issues
df <- read_csv(data_file, show_col_types = FALSE)

# Check if data loaded correctly and show any parsing problems  
if(nrow(df) == 0) {
  stop("Data file is empty or failed to load: ", data_file)
}

# Report any parsing problems
parsing_problems <- problems(df)
if(nrow(parsing_problems) > 0) {
  cat("Warning: Found", nrow(parsing_problems), "parsing issues in data\n")
}

cat("Successfully loaded", nrow(df), "rows from", data_file, "\n")
cat("Call_Reception column check:", "Call_Reception" %in% names(df), "\n")
 
# Use lubridate and across() to efficiently parse all date-time columns
df <- df |>
  mutate(across(c(Response_Date,
                   Incident_Start_Time,
                   TimeCallViewed,
                   Incident_Queue_Time,
                   Incident_Dispatch_Time,
                   Incident_Phone_Stop,
                   TimeFirstUnitDispatchAcknowledged,
                   Incident_Enroute_Time,
                   Incident_Arrival_Time,
                   TimeFirstCallCleared,
                   Incident_First_Close_Time,
                   Final_Closed_Time,
                   First_Reopen_Time), ymd_hms))

df$WeekNo <- as.factor(df$WeekNo)
df$Day <- as.factor(df$Day)
df$Hour <- as.factor(df$Hour)

# Convert DOW to an ordered factor to respect the sequence of days
df$DOW <- factor(
    df$DOW,
    levels = c("SUN", "MON", "TUE", "WED", "THU", "FRI", "SAT"),
    ordered = TRUE
)

df$ShiftPart <- factor(
  df$ShiftPart,
  levels = c("EARLY", "MIDS", "LATE"),
  ordered = TRUE
)

# Convert Priority_Number to an ordered factor as well
df$Priority_Number <- ordered(df$Priority_Number)

# Convert numeric variables from 'doubles' to integers
df[c('Time_To_Queue', 'Time_To_Dispatch', 'Phone_Time', 'Processing_Time', 'Rollout_Time', 'Transit_Time', 'Total_Call_Time')] <- sapply(df[c('Time_To_Queue', 'Time_To_Dispatch', 'Phone_Time', 'Processing_Time', 'Rollout_Time', 'Transit_Time', 'Total_Call_Time')], as.numeric)
```

For this week, there were a total of `r nrow(df)` calls for service. The column list is below:

```{r example-data}
#| echo: false
#| tbl-cap: "A sample of the first 10 rows of incident data."

colnames(df)
```

## Data Cleaning

In order to have a good dataset for analysis, some data cleaning was performed. The first step is to check for missing values in the dataset.

```{r missing-values}
#| echo: false
#| fig-cap: "Prevalence of missing values. Only columns with missing data are shown."

# ...existing code...
# Count columns with any missing data
missing_cols_count <- sum(colSums(is.na(df)) > 0)

# Get named vector of missing counts per column
missing_counts <- colSums(is.na(df))

# Optionally, create a tibble for easy use
missing_summary <- tibble(
  column = names(df),
  missing = missing_counts
)

# Find the column with the largest number of missing values
max_missing_col <- names(missing_counts)[which.max(missing_counts)]
max_missing_count <- max(missing_counts)

# Calculate percentage of calls without Incident_Arrival_Time
incident_arrival_missing_pct <- round(missing_counts["Incident_Arrival_Time"] / nrow(df) * 100, 1)

# ...existing code...

inspect_na(df) |>
  dplyr::filter(cnt > 0) |> # Explicitly use dplyr's filter
  show_plot(
    col_palette = 3, # Inferno palette from viridis (supported)
    text_labels = TRUE,
    label_color = "white"
  ) +
  geom_text(
    aes(label = paste0(round(pcnt, 1), "%")),
    stat = "identity",
    vjust = -0.5,
    size = 3.5,
    color = "black",
    fontface = "bold"
  ) +
  ggthemes::theme_fivethirtyeight(base_size = 10) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8), # Rotate x-axis labels
    axis.text.y = element_text(size = 9) # Fine-tune y-axis label size
  )
```

From this plot, we can see that there are only `r missing_cols_count` columns with missing data. Of those, the column with the largest number of missing values is `r max_missing_col`. That is something that we would like to see because that means that most of our calls are closed once and left that way. Later, we will look deeper into those calls to see if there are any patterns to those calls. The number of missing values in Incident_Arrival_Time may be something we wish to focus on in future because it shows that we have calls to which we never arrived. We will want to correlate those with their disposition to see if they were cancelled. Where there are calls that were not cancelled but we did not arrive, we will want to look into those further to see what happened. Additionally, `r incident_arrival_missing_pct`% of calls did not have a recorded time that the call stopped. We will have to determine if they were cancelled or how many of those were mutual aid calls where we did not receive a phone call.

## Exploratory Analysis

One of the first analyses is to break down different factor elements to see what we have in the dataset. Starting with the day of the week, the barchart below shows the number of calls for service by day of the week.

```{r day-of-week}
#| echo: false
#| fig-cap: "Number of calls for service by day of the week."
# ggplot2
dow_counts <- df |>
  count(DOW, sort = TRUE)

max_dow_info <- dow_counts |> filter(n == max(n))
busiest_day_abbr <- max_dow_info |> slice(1) |> pull(DOW)
busiest_day_count <- max_dow_info |> slice(1) |> pull(n)

min_dow_info <- dow_counts |> filter(n == min(n))
slowest_day_abbr <- min_dow_info |> slice(1) |> pull(DOW)
slowest_day_count <- min_dow_info |> slice(1) |> pull(n)

# Create mapping from abbreviations to full day names
day_names <- c(
  "SUN" = "Sunday",
  "MON" = "Monday", 
  "TUE" = "Tuesday",
  "WED" = "Wednesday",
  "THU" = "Thursday",
  "FRI" = "Friday",
  "SAT" = "Saturday"
)

# Convert abbreviations to full day names
busiest_day <- day_names[busiest_day_abbr]
slowest_day <- day_names[slowest_day_abbr]

barDOW <- df |> ggplot(aes(x=DOW, fill=DOW)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="E") +
  labs(title="Number of Calls for Service by Day of the Week",
       x="Day of the Week",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) + 
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barDOW
```

From this chart, we can see that `r busiest_day` was the busiest day of the week with `r busiest_day_count` service calls, and the slowest day was `r slowest_day` with `r slowest_day_count` service calls.

Over the last three weeks of this report structure, we've seen different days of the week for the busiest and slowest days. A larger sample size will be needed to determine if there are any underlying patterns.

```{r hour-of-day}
#| echo: false
#| fig-cap: "Number of calls for service by hour of the day." 
# ggplot2
hour_counts <- df |>
  count(Hour, sort = TRUE)

max_hour_info <- hour_counts |> filter(n == max(n))
busiest_hour <- sprintf("%02d", max_hour_info |> slice(1) |> pull(Hour))
busiest_hour_count <- max_hour_info |> slice(1) |> pull(n)

min_hour_info <- hour_counts |> filter(n == min(n))
slowest_hour <- sprintf("%02d", min_hour_info |> slice(1) |> pull(Hour))
slowest_hour_count <- min_hour_info |> slice(1) |> pull(n)

barHour <- df |> ggplot(aes(x=Hour, fill=Hour)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="E") +
  labs(title="Number of Calls for Service by Hour of the Day",
       x="Hour of the Day",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) + 
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=8),
        axis.text.y = element_text(size=10))

barHour
```

This week, the busiest hour of the day was `r busiest_hour`00 hours, with `r busiest_hour_count` calls for service. `r slowest_hour`00 hours was the slowest hour of the day with `r slowest_hour_count` calls. Additionally, the pattern shows consistent traffic from late rush hour through the day into the early evening before seeing the volumes start to decline. This appears to confirm assumptions about the busiest parts of the day.

Next, we can examnine the differences within the shifts and the split between calls received during the day and during the night to see how that can play a role in addressing staffing needs.

```{r shifts}
#| echo: false
#| fig-cap: "Number of calls for service by shift."
shift_counts <- df |>
  count(Shift, sort = TRUE)

dn_counts <- df |>
  count(Day_Night, sort = TRUE)

sp_counts <- df |>
  count(ShiftPart, sort = TRUE) # Note: ShiftPart is not used in the plots below

barShift <- df |>
  ggplot(aes(x=Shift, fill=ShiftPart)) +
  geom_bar() +
  scale_fill_manual(
    values = c("#1f77b4", "#17becf", "#9467bd"),
    name = "Shift Part"
  ) +
  labs(title="Call Volume per Shift",
    x = "Shift",
    y = "Number of Calls") +
  # Add segment counts centered within each segment (white text)
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    position = position_stack(vjust = 0.5),
    size=3,
    color = "white",
    fontface = "bold"
  ) +
  # Add total counts at the top of each bar
  stat_count(
    aes(label = after_stat(count), fill = NULL),
    geom = "text",
    vjust = -0.5,
    size=3.5,
    fontface = "bold",
    color = "black"
  ) +
  theme_minimal() +  
  theme(legend.position="right",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10),
        legend.title = element_text(size=10),
        legend.text = element_text(size=9))

barShift

barDN <- df |>
  ggplot(aes(x=Day_Night, fill=ShiftPart)) +
  geom_bar() +
  scale_fill_manual(
    values = c("#1f77b4", "#17becf", "#9467bd"),
    name = "Shift Part"
  ) +
  labs(title="Call Volume by Day/Night",
       x="Day/Night",
       y="Number of Calls") +
  # Add segment counts centered within each segment (white text)
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    position = position_stack(vjust = 0.5),
    size=3,
    color = "white",
    fontface = "bold"
  ) +
  # Add total counts at the top of each bar
  stat_count(
    aes(label = after_stat(count), fill = NULL),
    geom = "text",
    vjust = -0.5,
    size=3.5,
    fontface = "bold",
    color = "black"
  ) + 
  theme_minimal() +
  theme(legend.position="right",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10),
        legend.title = element_text(size=10),
        legend.text = element_text(size=9))

barDN
```

These bar graphs compare the distribution of calls between shifts and by whether the calls were received on day or night shift.

```{r priority-level}
#| echo: false
#| fig-cap: "Number of calls for service by priority level."
# ggplot2
pn_counts <- df |>
  count(Priority_Number, sort = TRUE)

max_pn_info <- pn_counts |> filter(n == max(n))
busiest_pn <- max_pn_info |> slice(1) |> pull(Priority_Number)
busiest_pn_count <- max_pn_info |> slice(1) |> pull(n)

# Calculate percentage statistics for inline use
busiest_pn_percentage <- round(busiest_pn_count / nrow(df) * 100, 1)
priority1_count <- sum(df$Priority_Number == "1", na.rm = TRUE)
priority1_percentage <- round(priority1_count / nrow(df) * 100, 1)

barPriority <- df |> ggplot(aes(x=Priority_Number, fill=Priority_Number)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="E") +
  labs(title="Number of Calls for Service by Priority Level",
       x="Priority Level",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) + 
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barPriority
```

The majority of calls received were Priority `r busiest_pn` calls. Priority `r busiest_pn` calls are `r busiest_pn_percentage` percent of the total number of calls, while Priority 1 calls are `r priority1_percentage` percent of the total number of calls.

This appears to be consistent through the new reports.

```{r discipline}
#| echo: false
#| fig-cap: "Number of calls for service by discipline."
# ggplot2
agency_counts <- df |>
  count(Agency, sort = TRUE)

max_agency_info <- agency_counts |> filter(n == max(n))
busiest_agency <- max_agency_info |> slice(1) |> pull(Agency)
busiest_agency_count <- max_agency_info |> slice(1) |> pull(n)

# Calculate percentage for Police calls
police_percentage <- round((sum(df$Agency == "POLICE", na.rm = TRUE) / nrow(df)) * 100, 1)

barDiscipline <- df |> ggplot(aes(x=Agency, fill=Agency)) +
  geom_bar() + 
  scale_fill_manual(
    values = c(POLICE = "#1f77b4", FIRE = "#d62728", EMS = "#2ca02c"),
    name = "Agency"
  ) +
  labs(title="Number of Calls for Service by Discipline",
       x="Discipline",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    position = position_stack(vjust = 0.5),
    size=3,
    color = "white",
    fontface = "bold"
  ) + 
  theme_minimal() +
  theme(legend.position="right",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10),
        legend.title = element_text(size=10),
        legend.text = element_text(size=9))

barDiscipline
```

As expected, the majority of calls are for `r busiest_agency`. They represent `r police_percentage`
percent of the total number of calls. This is fairly consistent with previous analyses. We can also examine the way in which we are receiving the calls by looking at the Call_Reception column. That chart is below.

```{r call-reception}
#| echo: false
#| fig-cap: "Number of calls for service by call reception."

# ggplot2
cr_counts <- df |>
  count(Call_Reception, sort = TRUE)

max_cr_info <- cr_counts |> filter(n == max(n))
busiest_cr <- max_cr_info |> slice(1) |> pull(Call_Reception)
busiest_cr_count <- max_cr_info |> slice(1) |> pull(n)

# Calculate statistics for inline use
e911_count <- sum(df$Call_Reception == "E-911", na.rm = TRUE)
not_captured_count <- sum(df$Call_Reception == "NOT CAPTURED", na.rm = TRUE)
e911_percentage <- round((e911_count / nrow(df)) * 100, 2)
not_captured_percentage <- round((not_captured_count / nrow(df)) * 100, 1)

barReception <- df |> ggplot(aes(x=Call_Reception, fill=Call_Reception)) +
  geom_bar() + 
  scale_fill_viridis(discrete=TRUE, option="E") +
  labs(title="Number of Calls for Service by Call Reception",
       x="Call Reception",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) + 
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barReception
```

Most of the calls arrived by `r busiest_cr`. 911 trunk line calls were `r e911_percentage` percent of all calls. There were `r not_captured_count` calls where we did not indicate how the service call was received. This is `r not_captured_percentage` percent of the total number of calls. The number is staying consistent over the lifespan of the reports, but should be investigated to determine why those calls are not being tracked for origination.

The following is a chart of the top 10 call types. The data is limited to ensure visual clarity and legibility of the information.

```{r call-type}
#| echo: false
#| fig-cap: "Number of calls for service by call type."
# ggplot2
ct_counts <- df |>
  count(Problem, sort = TRUE)

max_ct_info <- ct_counts |> filter(n == max(n))
busiest_ct <- max_ct_info |> slice(1) |> pull(Problem)
busiest_ct_count <- max_ct_info |> slice(1) |> pull(n)

# Find the most common Problem for each Agency
agency_top_problems <- df |>
  count(Agency, Problem, sort = TRUE) |>
  group_by(Agency) |>
  slice_max(n, n = 1, with_ties = FALSE) |>
  ungroup() |>
  dplyr::select(Agency, Problem, n)

# Create named vectors for easy access in inline code
agency_top_problem_names <- setNames(agency_top_problems$Problem, agency_top_problems$Agency)
agency_top_problem_counts <- setNames(agency_top_problems$n, agency_top_problems$Agency)

problem_counts <- df |>
  count(Problem, sort = TRUE) |>
  slice_head(n = 10)

barProblem <- problem_counts |>
  ggplot(aes(x=reorder(Problem, -n), y=n, fill=Problem)) +
  geom_bar(stat="identity") +
  scale_fill_viridis(discrete=TRUE, option="E") +
  labs(title="Number of Calls for Service by Call Type",
       x="Call Type",
       y="Number of Calls") +
  geom_text(
    aes(label = n),
    vjust = -0.5, 
    size = 3) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barProblem
```

This week, the most common problem nature was `r busiest_ct`. For AFD, the most common was `r agency_top_problem_names["FIRE"]`. The overall trend is that Disorderly Conduct and Trouble Breathing appear in the top 5 weekly over the four weeks of this report. These trends should be brought to the attention of our partners.

We can also look at the number of calls taken by telecommunicators. Again, like the problem types, we will limit the chart to the top 10 telecommunicators to ensure visual clarity and legibility of the information.

```{r telecommunicator}
#| echo: false
#| fig-cap: "Number of calls for service by telecommunicator."
# ggplot2
tc_counts <- df |>
  count(Call_Taker, sort = TRUE)

max_tc_info <- tc_counts |> filter(n == max(n))
busiest_tc <- max_tc_info |> slice(1) |> pull(Call_Taker)
busiest_tc_count <- max_tc_info |> slice(1) |> pull(n)

tc_counts <- df |> 
  count(Call_Taker, sort = TRUE) |>
  slice_head(n = 10)

barCallTaker <- tc_counts |>
  ggplot(aes(x=reorder(Call_Taker, -n), y=n, fill=Call_Taker)) +
  geom_bar(stat="identity") +
  scale_fill_viridis(discrete=TRUE, option="E") +
  labs(title="Number of Calls for Service by Call Taker",
       x="Call Taker",
       y="Number of Calls") +
  geom_text(
    aes(label = n),
    vjust = -0.5,
    size = 3) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barCallTaker
```

It is interesting to note that the top "call taker" is `r busiest_tc` again this week with `r busiest_tc_count` calls, and with a large margin between that volume and the busiest calltaker.

### Call Distribution: Hour by Day of Week

The following visualization shows the distribution of calls throughout the day (by hour) for each day of the week. This helps identify patterns in call volume across different days and times.

```{r hour-dow-analysis}
#| label: hour-dow-analysis
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Call Volume by Hour and Day of Week"

# Create summary data
hourly_dow_summary <- df |>
  group_by(DOW, Hour) |>
  summarise(call_count = n(), .groups = 'drop') |>
  mutate(Hour_numeric = as.numeric(as.character(Hour)))

# Create a heatmap showing call patterns
hour_dow_plot <- ggplot(hourly_dow_summary, aes(x = Hour_numeric, y = DOW, fill = call_count)) +
  geom_tile(color = "white", linewidth = 0.1) +
  scale_x_continuous(name = "Hour of Day", 
                     breaks = seq(0, 23, 2),
                     labels = sprintf("%02d:00", seq(0, 23, 2))) +
  scale_y_discrete(name = "Day of Week", limits = rev) +
  scale_fill_gradient2(name = "Calls", 
                       low = "lightblue", 
                       mid = "yellow",
                       high = "red",
                       midpoint = median(hourly_dow_summary$call_count)) +
  labs(title = "Call Volume Heatmap by Hour and Day of Week",
       subtitle = "Darker colors indicate higher call volumes") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 12),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
    axis.text.y = element_text(size = 10),
    axis.title = element_text(size = 12),
    legend.position = "right",
    panel.grid = element_blank()
  )

hour_dow_plot
```

```{r alternative-ridge-plot}
#| label: alternative-ridge-plot  
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Ridge Plot Alternative - Calls per Hour by Day of Week"

# Try to create actual ridge plot if ggridges is available
tryCatch({
  if (requireNamespace("ggridges", quietly = TRUE)) {
    library(ggridges)
    
    # Create ridge plot using the raw data for a density estimate
    ridge_plot <- ggplot(df, aes(x = as.numeric(as.character(Hour)), y = DOW, fill = DOW)) +
      ggridges::geom_density_ridges(
        alpha = 0.7, 
        scale = 1.2, # Adjust scale for better separation
        rel_min_height = 0.01 # Removes trailing tails
      ) +
      scale_x_continuous(name = "Hour of Day",
                         breaks = seq(0, 23, 4),
                         labels = sprintf("%02d:00", seq(0, 23, 4))) +
      scale_y_discrete(name = "Day of Week", limits = rev) +
      scale_fill_brewer(name = "Day", palette = "Set3") +
      labs(title = "Ridge Plot: Call Volume Distribution by Hour and Day of Week",
           subtitle = "Each ridge shows the hourly distribution for one day") +
      theme_minimal() +
      theme(
        plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, size = 12),
        axis.text = element_text(size = 10),
        axis.title = element_text(size = 12),
        legend.position = "none"
      )
    
    print(ridge_plot)
  } else {
    cat("ggridges package not available. Using heatmap above instead.\n")
  }
}, error = function(e) {
  cat("Could not create ridge plot. Error:", e$message, "\n")
  cat("The heatmap above provides similar insights.\n")
})
```

```{r ridge-plot-summary-stats}
#| label: ridge-plot-summary-stats
#| echo: false
#| message: false
#| warning: false

# Summary statistics for calls by hour and DOW
hourly_summary <- df |>
  group_by(DOW) |>
  summarise(
    total_calls = n(),
    peak_hour = names(sort(table(Hour), decreasing = TRUE))[1],
    avg_calls_per_hour = round(n() / 24, 1),
    .groups = 'drop'
  ) |>
  arrange(desc(total_calls))

# Display summary table
to_ft(
  hourly_summary,
  caption = "Call Volume Summary by Day of Week",
  header_map = c(DOW = "Day of Week", total_calls = "Total Calls", peak_hour = "Peak Hour", avg_calls_per_hour = "Avg Calls/Hour"),
  digits = 0
)
```

Based on the heat map, the middle of the day is still the busiest time for calls to arrive in the center. For this week, the 1700 hour for Friday was the busiest day and hour combination for the week. We may need to go back to that date and time range to determine if there was a significant event in that hour that could account for the spike in calls.

### Summary statsitcs and analyses

In this section, we will analyse the continuous variables that represent the elapsed time for various segments of the call process. The variables of interest include: Time_To_Queue, Time_To_Dispatch, Phone_Time, Processing_Time, Rollout_Time, Transit_Time, and Total_Call_Time. They are defined as follows:

* Time_To_Queue
: The time from the start of the call to the time it is released to queue for dispatch.

* Time_To_Dispatch
: The time from the time the call is released for dispatch to the time the first unit is assigned.

* Phone_Time
: The time from the start of the call to the time the phone call ended.

* Processing_Time
: The time from the start of the call until the first unit is assigned.

* Rollout_Time
: The time from the assignment of the first unit to the first unit marking en route to the call.

* Transit_Time
: The time from the first unit marking en route to the call to the first unit arriving on scene.

* Total_Call_Time
: The total time from the start of the call to the time the call was closed. If the call is re-opened, then this clock stops with the first closure.

```{r custom-summary}
#| label: custom-summary
#| echo: false
#| message: false
#| warning: false

# Initialize variables
summary_table <- NULL
median_time_to_queue <- NA_real_
median_time_to_dispatch <- NA_real_

# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time", 
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")
  
  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df)]
  
  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {
    
    summary_table <- df %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]
                
                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)
                    
                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)
                    
                    if (var_val == 0) return(NA_real_)
                    
                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })
                
                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display
    
    # Extract key metrics for use in text
    median_time_to_queue <- summary_table |> 
      filter(Variable == "Time To Queue") |> 
      pull(Median)
      
    median_time_to_dispatch <- summary_table |> 
      filter(Variable == "Time To Dispatch") |> 
      pull(Median)

    median_processing_time <- summary_table |>
      filter(Variable == "Processing_Time") |>
      pull(Median)

    median_phone_time <- summary_table |>
      filter(Variable == "Phone_Time") |>
      pull(Median)

    to_ft(
      summary_table,
      caption = "Weekly Elapsed Time Summary Table",
      header_map = c(Variable = "Time Metric", Minimum = "Min", Mean = "Mean", Median = "Median", Std_Dev = "Std Dev", Skewness = "Skew", Kurtosis = "Kurt"),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})
```

The values from this table describe operations for the week being analyzed. In this case, the median time for a call to be placed in queue is `r median_time_to_queue` seconds. This is still in line with what has been seen in the last two weeks. The median time in queue was `r median_time_to_dispatch` seconds. These are comparable numbers with the prior weeks.  

```{r ttq-plots}
#| label: ttq-plots
#| echo: false
#| message: false
#| warning: false

# Helper to format seconds as mm:ss or h:mm:ss
sec_label <- function(x) {
  s <- round(x)
  h <- s %/% 3600
  m <- (s %% 3600) %/% 60
  sec <- s %% 60
  ifelse(h > 0, sprintf("%d:%02d:%02d", h, m, sec), sprintf("%d:%02d", m, sec))
}

# Prepare TTQ values in seconds and basic stats
ttq <- df |>
  transmute(ttq_sec = as.numeric(Time_To_Queue)) |>
  filter(!is.na(ttq_sec), ttq_sec >= 0)

n_ttq <- nrow(ttq)
ttq_med <- median(ttq$ttq_sec, na.rm = TRUE)
ttq_p90 <- as.numeric(quantile(ttq$ttq_sec, 0.90, na.rm = TRUE))
ttq_p99 <- as.numeric(quantile(ttq$ttq_sec, 0.99, na.rm = TRUE))
n_clipped <- sum(ttq$ttq_sec > ttq_p99, na.rm = TRUE)

# Adaptive bin width (Freedman–Diaconis) with nicening
fd <- 2 * IQR(ttq$ttq_sec, na.rm = TRUE) / (n_ttq^(1/3))
binw <- fd
if (!is.finite(binw) || binw <= 0) binw <- 5
binw <- dplyr::case_when(
  binw < 1 ~ 1,
  binw < 2 ~ 2,
  binw < 5 ~ 5,
  binw < 10 ~ 10,
  binw < 15 ~ 15,
  TRUE ~ round(binw, -1)
)

# Clip extreme tail for readability (ensure at least 20s to show reference lines)
x_max <- max(20, ttq_p99)

# Single plot: histogram (normalized to density) with density overlay and markers
ttq_hist_dens <- ggplot(ttq, aes(x = ttq_sec)) +
  # Histogram scaled to density for alignment with density curve
  geom_histogram(aes(y = after_stat(density)),
                 binwidth = binw, boundary = 0, closed = "left",
                 fill = "#1c5789", color = "white", alpha = 0.6) +
  # Density overlay
  geom_density(color = "#1c5789", fill = "#1c5789", alpha = 0.2, adjust = 1.1, linewidth = 1) +
  # Median and P90 reference lines
  geom_vline(xintercept = ttq_med, linetype = "dashed", color = "#d62728", linewidth = 0.8) +
  geom_vline(xintercept = ttq_p90, linetype = "dotted", color = "#ff7f0e", linewidth = 0.8) +
  # NENA/NFPA reference lines at 0:15 and 0:20
  geom_vline(xintercept = 15, linetype = "longdash", color = "#2ca02c", linewidth = 0.7) +
  geom_vline(xintercept = 20, linetype = "longdash", color = "#9467bd", linewidth = 0.7) +
  # Labels for markers (placed above the density peak area)
  annotate("label", x = ttq_med, y = Inf, vjust = 1.2,
           label = paste0("Median: ", round(ttq_med), "s"), size = 3, fill = "white") +
  annotate("label", x = ttq_p90, y = Inf, vjust = 1.2,
           label = paste0("P90: ", round(ttq_p90), "s"), size = 3, fill = "white") +
  annotate("label", x = 15, y = Inf, vjust = 1.2,
           label = "NENA 0:15", size = 3, fill = "white") +
  annotate("label", x = 20, y = Inf, vjust = 1.2,
           label = "NFPA 0:20", size = 3, fill = "white") +
  scale_x_continuous(name = "Time to Queue (mm:ss)",
                     limits = c(0, x_max),
                     breaks = scales::pretty_breaks(8),
                     labels = sec_label) +
  scale_y_continuous(name = "Density") +
  labs(title = "Time to Queue — Histogram with Density",
    subtitle = paste0("Histogram normalized to density; clipped at 99th percentile (", scales::comma(n_clipped), " removed).\n",
          "Median (dashed red), 90th percentile (dotted orange). Reference lines at 0:15 (NENA) and 0:20 (NFPA 1225)."),
       caption = "Data: CAD") +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold"),
    axis.title.x = element_text(size = 12),
    axis.title.y = element_text(size = 12)
  )

ttq_hist_dens

```

These combined histogram and density plots are designed to show the distribution of the elapsed time between events in the call's lifecycle. Using the 90^th^ percentile, we can state that 90% of all service calls are ready to be dispatched within `r ttq_p90` seconds. The same 90^th^ lines are reflected in the remaining plots below.

```{r elapsed-time-plots}
#| label: elapsed-time-plots
#| echo: false
#| message: false
#| warning: false

# Reusable plot helper: histogram normalized to density with density overlay and markers
plot_time_hist_dens <- function(data, var, title_text) {
  vals <- as.numeric(data[[var]])
  vals <- vals[is.finite(vals) & vals >= 0]
  if (length(vals) < 2) return(NULL)

  # Stats
  n <- length(vals)
  med <- median(vals)
  p90 <- as.numeric(quantile(vals, 0.90))
  p99 <- as.numeric(quantile(vals, 0.99))
  n_clipped <- sum(vals > p99)

  # Freedman–Diaconis bin width with nicening
  fd <- 2 * IQR(vals) / (n^(1/3))
  binw <- fd
  if (!is.finite(binw) || binw <= 0) binw <- 5
  binw <- dplyr::case_when(
    binw < 1 ~ 1,
    binw < 2 ~ 2,
    binw < 5 ~ 5,
    binw < 10 ~ 10,
    binw < 15 ~ 15,
    TRUE ~ round(binw, -1)
  )

  x_max <- max(20, p99)

  # mm:ss (or h:mm:ss) labels
  sec_label <- function(x) {
    s <- round(x)
    h <- s %/% 3600
    m <- (s %% 3600) %/% 60
    sec <- s %% 60
    ifelse(h > 0, sprintf("%d:%02d:%02d", h, m, sec), sprintf("%d:%02d", m, sec))
  }

  ggplot(data.frame(x = vals), aes(x = x)) +
    geom_histogram(aes(y = after_stat(density)),
                   binwidth = binw, boundary = 0, closed = "left",
                   fill = "#1c5789", color = "white", alpha = 0.6) +
    geom_density(color = "#1c5789", fill = "#1c5789", alpha = 0.2, adjust = 1.1, linewidth = 1) +
    geom_vline(xintercept = med, linetype = "dashed", color = "#d62728", linewidth = 0.8) +
    geom_vline(xintercept = p90, linetype = "dotted", color = "#ff7f0e", linewidth = 0.8) +
    geom_vline(xintercept = 15, linetype = "longdash", color = "#2ca02c", linewidth = 0.7) +
    geom_vline(xintercept = 20, linetype = "longdash", color = "#9467bd", linewidth = 0.7) +
  annotate("label", x = med, y = Inf, vjust = 1.2,
       label = "Median", size = 2.5, fill = "white") +
  annotate("label", x = p90, y = Inf, vjust = 1.2,
       label = "P90", size = 2.5, fill = "white") +
  annotate("label", x = 15, y = Inf, vjust = 1.2,
       label = "NENA", size = 2.5, fill = "white") +
  annotate("label", x = 20, y = Inf, vjust = 1.2,
       label = "NFPA", size = 2.5, fill = "white") +
    scale_x_continuous(name = "Time (mm:ss)",
                       limits = c(0, x_max),
                       breaks = scales::pretty_breaks(8),
                       labels = sec_label) +
    scale_y_continuous(name = "Density") +
    labs(title = title_text,
      subtitle = NULL,
      caption = "Data: CAD") +
    theme_minimal(base_size = 12) +
    theme(
      plot.title = element_text(face = "bold"),
      axis.title.x = element_text(size = 12),
      axis.title.y = element_text(size = 12)
    )
}

# Build and print plots for other elapsed-time metrics
p_dispatch   <- plot_time_hist_dens(df, "Time_To_Dispatch",  "Time to Dispatch — Histogram with Density")
p_phone      <- plot_time_hist_dens(df, "Phone_Time",       "Phone Time — Histogram with Density")
p_processing <- plot_time_hist_dens(df, "Processing_Time",  "Processing Time — Histogram with Density")
p_rollout    <- plot_time_hist_dens(df, "Rollout_Time",     "Rollout Time — Histogram with Density")
p_transit    <- plot_time_hist_dens(df, "Transit_Time",     "Transit Time — Histogram with Density")
p_total      <- plot_time_hist_dens(df, "Total_Call_Time",  "Total Call Time — Histogram with Density")

if (!is.null(p_dispatch))   print(p_dispatch)
if (!is.null(p_phone))      print(p_phone)
if (!is.null(p_processing)) print(p_processing)
if (!is.null(p_rollout))    print(p_rollout)
if (!is.null(p_transit))    print(p_transit)
if (!is.null(p_total))      print(p_total)
```

```{r elapsed-time-grid}
#| label: elapsed-time-grid
#| echo: false
#| message: false
#| warning: false

# Short-title versions for grid
p_dispatch_grid   <- plot_time_hist_dens(df, "Time_To_Dispatch",  "Time to Dispatch")
p_phone_grid      <- plot_time_hist_dens(df, "Phone_Time",       "Phone Time")
p_processing_grid <- plot_time_hist_dens(df, "Processing_Time",  "Processing Time")
p_rollout_grid    <- plot_time_hist_dens(df, "Rollout_Time",     "Rollout Time")
p_transit_grid    <- plot_time_hist_dens(df, "Transit_Time",     "Transit Time")
p_total_grid      <- plot_time_hist_dens(df, "Total_Call_Time",  "Total Call Time")

# Arrange elapsed-time plots in a 2x3 grid (skip NULL plots)
plots_list <- list(
  p_dispatch_grid,  # Time To Dispatch
  p_phone_grid,     # Phone Time
  p_processing_grid,# Processing Time
  p_rollout_grid,   # Rollout Time
  p_transit_grid,   # Transit Time
  p_total_grid      # Total Call Time
)
plots_list <- Filter(Negate(is.null), plots_list)

if (length(plots_list) > 0) {
  grid <- ggpubr::ggarrange(plotlist = plots_list, ncol = 3, nrow = 2, align = "hv")
  print(grid)
}
```

**Plot Key:**

| Line Type/Color      | Meaning                |
|---------------------|------------------------|
| **Dashed Red**    | Median                 |
| **Dotted Orange** | 90th Percentile (P90)  |
| **Longdash Green** | NENA 0:15 Standard     |
| **Longdash Purple** | NFPA 0:20 Standard     |

These show that the processing times for DECC are well within the NENA and NFPA guidelines. This is good operational data to show how well we are performing with respect to those guidelines. Over time, we can track these metrics to ensure that we continue to meet or exceed those standards.

## Discipline Analyses

As discussed earlier, we can create additional subsets from this data to look at specific areas of interest. We will create several new datasets from this weekly set for further analysis. The first will be a dataset that combines APD Priority 1 calls with AFD Priority 1 and 2 calls and evaluates those as emergency calls. We will also create specific datasets for law, fire, and EMS for specific analyses of the disciplines. We will also create datasets that identify calls that exceed certain parameters that have been defined from other reports. Finally, because we have been evaluating Cardiac Arrest calls for some time, we'll create and analyze that dataset.

```{r new-datasets}
#| label: new-datasets
#| echo: false
#| message: false
#| warning: false

df_hp <- df |> 
  filter((Agency == "POLICE" & Priority_Number < 2) | (Agency %in% c("FIRE", "EMS") & Priority_Number < 3))

df_law <- df |> filter(Agency == "POLICE")
df_fire <- df |> filter(Agency == "FIRE")
df_ems <- df |> filter(Agency == "EMS")
df_ttq_delay <- df_hp |> filter(Time_To_Queue > 60)
df_ttd_delay <- df_hp |> filter(Time_To_Dispatch > 60)
df_ca <- df |> filter(Problem == "CARDIAC ARREST ALS 2- SUPV")

mental_health <- c("MUTUAL PSYCHOLOGICAL EMERGENCY", "PSYCHIATRIC EMERGENCY ALS 1", "PSYCHIATRIC EMERGENCY VIOLENT", "WELFARE CHECK", "JUMPER FROM WWB", "MENTAL HEALTH CASE", "SUICIDE DELAY", "SUICIDE IN PROG NO INJ", "SUICIDE IN PROG INJ/PILLS", "SUICIDE IN PROG TRAUMA")

# Mental health related calls subset
# - Filters rows where `Problem` is one of the values in `mental_health`
# - Uses `%in%` and handles potential NA values safely with `is.na()` check
# - Creates a new dataset `df_mh` for downstream analysis
df_mh <- df |> dplyr::filter(!is.na(Problem) & Problem %in% mental_health)
```

By defining these datasets, we can now add to our analyses. For example, we can reuse the same information from above to drill down into APD and AFD calls. Starting with APD calls for service, we can examine everything as we did above.

### APD Analyses

```{r apd-day-of-week}
#| echo: false
#| fig-cap: "Number of calls for service by day of the week."
# ggplot2
dow_counts_law <- df_law |>
  count(DOW, sort = TRUE)

max_dow_info_law <- dow_counts |> filter(n == max(n))
busiest_day_abbr_law <- max_dow_info_law |> slice(1) |> pull(DOW)
busiest_day_count_law <- max_dow_info_law |> slice(1) |> pull(n)

min_dow_info_law <- dow_counts |> filter(n == min(n))
slowest_day_abbr_law <- min_dow_info_law |> slice(1) |> pull(DOW)
slowest_day_count_law <- min_dow_info_law |> slice(1) |> pull(n)

# Create mapping from abbreviations to full day names
day_names <- c(
  "SUN" = "Sunday",
  "MON" = "Monday", 
  "TUE" = "Tuesday",
  "WED" = "Wednesday",
  "THU" = "Thursday",
  "FRI" = "Friday",
  "SAT" = "Saturday"
)

# Convert abbreviations to full day names
busiest_day_law <- day_names[busiest_day_abbr_law]
slowest_day_law <- day_names[slowest_day_abbr_law]

barDOW_APD <- df_law |> ggplot(aes(x=DOW, fill=DOW)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Calls for Service for APD by Day of the Week",
       x="Day of the Week",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) + 
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barDOW_APD
```

This week, `r busiest_day_law` was the busiest day of the week for APD service calls. `r slowest_day_law`, being the lightest day of the week overall, was the lightest day for the APD as well. 

```{r apd-hour-of-day}
#| echo: false
#| fig-cap: "Number of calls for service by hour of the day."
# ggplot2
hour_counts_law <- df_law |>
  count(Hour, sort = TRUE)

max_hour_info_law <- hour_counts_law |> filter(n == max(n))
busiest_hour_law <- sprintf("%02d", max_hour_info_law |> slice(1) |> pull(Hour))
busiest_hour_count_law <- max_hour_info_law |> slice(1) |> pull(n)

min_hour_info_law <- hour_counts_law |> filter(n == min(n))
slowest_hour_law <- sprintf("%02d", min_hour_info_law |> slice(1) |> pull(Hour))
slowest_hour_count_law <- min_hour_info_law |> slice(1) |> pull(n)

barHour_APD <- df_law |> ggplot(aes(x=Hour, fill=Hour)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Calls for Service for APD by Hour of the Day",
       x="Hour of the Day",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=8),
        axis.text.y = element_text(size=10))

barHour_APD
```

The busiest time of the week for APD calls is from 0900 to 1200 hours. This is in contrast to the prior three weeks where the mid to late afternoon was the busiest time of the day.

```{r apd-call-reception}
#| echo: false
#| fig-cap: "Number of calls for service by call reception."
# ggplot2
cr_counts_law <- df_law |>
  count(Call_Reception, sort = TRUE)

max_cr_info_law <- cr_counts_law |> filter(n == max(n))
busiest_cr_law <- max_cr_info_law |> slice(1) |> pull(Call_Reception)
busiest_cr_count_law <- max_cr_info_law |> slice(1) |> pull(n)

barReception_APD <- df_law |> ggplot(aes(x=Call_Reception, fill=Call_Reception)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Calls for Service for by Call Reception",
       x="Call Reception",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barReception_APD
```

As can be seen, the majority of calls came through `r busiest_cr_law`. This comports to the call reception results for the week overall.

```{r apd-call-type}
#| echo: false
#| fig-cap: "Number of calls for service by call type."
# ggplot2
prob_counts_law <- df_law |>
  count(Problem, sort = TRUE)

max_prob_info_law <- prob_counts_law |> filter(n == max(n))
busiest_prob_law <- max_prob_info_law |> slice(1) |> pull(Problem)
busiest_prob_count_law <- max_prob_info_law |> slice(1) |> pull(n)

problem_counts_APD <- df_law |>
  count(Problem, sort = TRUE) |>
  slice_head(n = 10)

barProblem_APD <- problem_counts_APD |>
  ggplot(aes(x=reorder(Problem, -n), y=n, fill=Problem)) +
  geom_bar(stat="identity") +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Calls for Service by Call Type",
       x="Call Type",
       y="Number of Calls") +
  geom_text(
    aes(label = n),
    vjust = -0.5,
    size=3) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barProblem_APD
```

The largest call type was for `r busiest_prob_law`, which was also the largest call type for the week overall. This could be something to monitor over time to see how the trend changes over time.

```{r apd-priority-level}
#| echo: false
#| fig-cap: "Number of calls for service by priority level."
# ggplot2
pri_counts_law <- df_law |>
  count(Priority_Number, sort = TRUE)

max_pri_info_law <- pri_counts_law |> filter(n == max(n))
busiest_pri_law <- max_pri_info_law |> slice(1) |> pull(Priority_Number)
busiest_pri_count_law <- max_pri_info_law |> slice(1) |> pull(n)

# Calculate percentage for APD priority calls
busiest_pri_law_percentage <- round((sum(df_law$Priority_Number == busiest_pri_law, na.rm = TRUE) / nrow(df_law)) * 100, 1)

# Calculate percentage for P1 calls
p1_law_percentage <- round((sum(df_law$Priority_Number == 1, na.rm = TRUE) / nrow(df_law)) * 100, 1)

barPriority_APD <- df_law |> ggplot(aes(x=Priority_Number, fill=Priority_Number)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Calls for Service by Priority Level",
       x="Priority Level",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barPriority_APD
```

As expected, the largest number of calls were Priority `r busiest_pri_law` calls which represent `r busiest_pri_law_percentage` percent of all APD calls. Again, this comports with the overall weekly trends.

```{r apd-custom-summary}
#| label: apd-custom-summary
#| echo: false
#| message: false
#| warning: false

# Initialize variables
summary_table <- NULL
median_phone_time <- NA_real_
median_processing_time <- NA_real_

# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time", 
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")
  
  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_law)]
  
  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {
    
    summary_table <- df_law %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]
                
                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)
                    
                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)
                    
                    if (var_val == 0) return(NA_real_)
                    
                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })
                
                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display
    
    # Extract key metrics for use in text
    median_time_to_queue <- summary_table |> 
      filter(Variable == "Time To Queue") |> 
      pull(Median)
      
    median_time_to_dispatch <- summary_table |> 
      filter(Variable == "Time To Dispatch") |> 
      pull(Median)

    median_processing_time <- summary_table |>
      filter(Variable == "Processing_Time") |>
      pull(Median)

    median_phone_time <- summary_table |>
      filter(Variable == "Phone_Time") |>
      pull(Median)

    to_ft(
      summary_table,
      caption = "Weekly Elapsed Time Summary Table",
      header_map = c(Variable = "Time Metric", Minimum = "Min", Mean = "Mean", Median = "Median", Std_Dev = "Std Dev", Skewness = "Skew", Kurtosis = "Kurt"),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})

# Extract key metrics for inline use (outside tryCatch to ensure they're available)
median_phone_time <- tryCatch({
  if (exists("summary_table")) {
    summary_table |> filter(Variable == "Phone Time") |> pull(Median)
  } else {
    NA_real_
  }
}, error = function(e) NA_real_)

median_processing_time <- tryCatch({
  if (exists("summary_table")) {
    summary_table |> filter(Variable == "Processing Time") |> pull(Median)  
  } else {
    NA_real_
  }
}, error = function(e) NA_real_)

# Calculate P4 percentage for inline use
p4_percentage_apd <- round((sum(df_law$Priority_Number == "4", na.rm = TRUE) / nrow(df_law)) * 100, 1)
```

This table shows that overall, we have a median time on the phones of about `r median_phone_time` seconds and it takes about double that for a call to start and be dispatched, `r median_processing_time` seconds. Some of that difference is going to be due to having to hold Priority 4 and above calls until there is a unit available. Since the P4 calls are `r p4_percentage_apd` percent of APD calls, this could have a measureable impact on service times for DECC staff.

### AFD FIRE Analyses

Because AFD calls for service can be split into two distinct disciplines, fire-related calls and medical-related calls, DECC has, historically, separated the two disciplines for analytical purposes. So this section will look at fire-related calls for service for the week.

```{r afd-day-of-week}
#| echo: false
#| fig-cap: "Number of calls for service by day of the week."
# ggplot2
dow_counts_fire <- df_fire |>
  count(DOW, sort = TRUE)

max_dow_info_fire <- dow_counts_fire |> filter(n == max(n))
busiest_day_abbr_fire <- max_dow_info_fire |> slice(1) |> pull(DOW)
busiest_day_count_fire <- max_dow_info_fire |> slice(1) |> pull(n)

min_dow_info_fire <- dow_counts_fire |> filter(n == min(n))
slowest_day_abbr_fire <- min_dow_info_fire |> slice(1) |> pull(DOW)
slowest_day_count_fire <- min_dow_info_fire |> slice(1) |> pull(n)

# Create mapping from abbreviations to full day names
day_names <- c(
  "SUN" = "Sunday",
  "MON" = "Monday", 
  "TUE" = "Tuesday",
  "WED" = "Wednesday",
  "THU" = "Thursday",
  "FRI" = "Friday",
  "SAT" = "Saturday"
)

# Convert abbreviations to full day names
busiest_day_fire <- day_names[busiest_day_abbr_fire]
slowest_day_fire <- day_names[slowest_day_abbr_fire]

barDOW_AFD <- df_fire |> ggplot(aes(x=DOW, fill=DOW)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="F") +
  labs(title="Number of Calls for Service for AFD by Day of the Week",
       x="Day of the Week",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barDOW_AFD
```

This week, the busiest day for fire-related calls was `r busiest_day_fire` with `r busiest_day_count_fire` calls for service. `r slowest_day_fire` was the lightest day for fire-related calls with `r slowest_day_count_fire` calls for service. The busiest day this week for fire-related calls comports with the overall weekly information.

```{r afd-hour-of-day}
#| echo: false
#| fig-cap: "Number of calls for service by hour of the day."
# ggplot2
hour_counts_fire <- df_fire |>
  count(Hour, sort = TRUE)

max_hour_info_fire <- hour_counts_fire |> filter(n == max(n))
busiest_hour_fire <- sprintf("%02d", max_hour_info_fire |> slice(1) |> pull(Hour))
busiest_hour_count_fire <- max_hour_info_fire |> slice(1) |> pull(n)

min_hour_info_fire <- hour_counts_fire |> filter(n == min(n))
slowest_hour_fire <- sprintf("%02d", min_hour_info_fire |> slice(1) |> pull(Hour))
slowest_hour_count_fire <- min_hour_info_fire |> slice(1) |> pull(n)

barHour_AFD <- df_fire |> ggplot(aes(x=Hour, fill=Hour)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="F") +
  labs(title="Number of Calls for Service for AFD by Hour of the Day",
       x="Hour of the Day",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=8),
        axis.text.y = element_text(size=10))

barHour_AFD
```

Fire-related calls are much more spread out through the day as can be seen in the graph above. However, `r busiest_hour_fire` hours was the busiest hour for the week There hasn't been an overall trend identified in the four weeks of this report. However, we will continue to observe the patterns to see if any trends emerge in fire related calls.

```{r afd-call-reception}
#| echo: false
#| fig-cap: "Number of calls for service by call reception."
# ggplot2
cr_counts_fire <- df_fire |>
  count(Call_Reception, sort = TRUE)

max_cr_info_fire <- cr_counts_fire |> filter(n == max(n))
busiest_cr_fire <- max_cr_info_fire |> slice(1) |> pull(Call_Reception)
busiest_cr_count_fire <- max_cr_info_fire |> slice(1) |> pull(n)

busiest_cr_pct_fire <- round((sum(df_fire$Call_Reception == busiest_cr_fire, na.rm = TRUE) / nrow(df_fire)) * 100, 1)
cr_phone_pct_fire <- round((sum(df_fire$Call_Reception == "Phone", na.rm = TRUE) / nrow(df_fire)) * 100, 1)

barReception_AFD <- df_fire |> ggplot(aes(x=Call_Reception, fill=Call_Reception)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="F") +
  labs(title="Number of Calls for Service for AFD by Call Reception",
       x="Call Reception",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barReception_AFD
```

Like APD calls, most fire-related calls came in via `r busiest_cr_fire`. That accounts for `r busiest_cr_pct_fire` percent of all fire-related calls. However the numbers for Mutual Aid and E-911 were larger percentages of the overall volume. In this case, Phone, not necessarily E-911 represented `r cr_phone_pct_fire` percent of all fire-related service calls received.

```{r afd-call-type}
#| echo: false
#| fig-cap: "Number of calls for service by call type."
# ggplot2
prob_counts_fire <- df_fire |>
  count(Problem, sort = TRUE)

max_prob_info_fire <- prob_counts_fire |> filter(n == max(n))
busiest_prob_fire <- max_prob_info_fire |> slice(1) |> pull(Problem)
busiest_prob_count_fire <- max_prob_info_fire |> slice(1) |> pull(n)

problem_counts_AFD <- df_fire |>
  count(Problem, sort = TRUE) |>
  slice_head(n = 10)

barProblem_AFD <- problem_counts_AFD |>
  ggplot(aes(x=reorder(Problem, -n), y=n, fill=Problem)) +
  geom_bar(stat="identity") +
  scale_fill_viridis(discrete=TRUE, option="F") +
  labs(title="Number of Calls for Service for AFD by Call Type",
       x="Call Type",
       y="Number of Calls") +
  geom_text(
    aes(label = n),
    vjust = -0.5,
    size=3) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barProblem_AFD
```

The greatest number of fire-related service calls were for `r busiest_prob_fire`. That is an interesting observation and should be watched through the future.

```{r afd-priority-level}
#| echo: false
#| fig-cap: "Number of calls for service by priority level."
# ggplot2
pri_counts_fire <- df_fire |>
  count(Priority_Number, sort = TRUE)

max_pri_info_fire <- pri_counts_fire |> filter(n == max(n))
busiest_pri_fire <- max_pri_info_fire |> slice(1) |> pull(Priority_Number)
busiest_pri_count_fire <- max_pri_info_fire |> slice(1) |> pull(n)

# Calculate percentage for APD priority calls
busiest_pri_fire_percentage <- round((sum(df_fire$Priority_Number == busiest_pri_fire, na.rm = TRUE) / nrow(df_fire)) * 100, 1)

# Calculate percentage for P1 calls
p1_fire_percentage <- round((sum(df_fire$Priority_Number == 1, na.rm = TRUE) / nrow(df_fire)) * 100, 1)

barPriority_AFD <- df_fire |> ggplot(aes(x=Priority_Number, fill=Priority_Number)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="F") +
  labs(title="Number of Calls for Service by Priority Level",
       x="Priority Level",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barPriority_AFD
```

The most-used priority for fire-related calls was P`r busiest_pri_fire`. P1 calls account for `r p1_fire_percentage` percent of all fire-related calls this week.

```{r afd-custom-summary}
#| label: afd-custom-summary
#| echo: false
#| message: false
#| warning: false

# Initialize variables
summary_table <- NULL
median_processing_time <- NA_real_
median_phone_time <- NA_real_
mean_phone_time <- NA_real_

# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time", 
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")
  
  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_fire)]
  
  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {
    
    summary_table <- df_fire %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]
                
                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)
                    
                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)
                    
                    if (var_val == 0) return(NA_real_)
                    
                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })
                
                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display
    
    # Extract key metrics for use in text
    median_time_to_queue <- summary_table |> 
      filter(Variable == "Time To Queue") |> 
      pull(Median)
      
    median_time_to_dispatch <- summary_table |> 
      filter(Variable == "Time To Dispatch") |> 
      pull(Median)

    median_processing_time <- summary_table |>
      filter(Variable == "Processing_Time") |>
      pull(Median)

    median_phone_time <- summary_table |>
      filter(Variable == "Phone_Time") |>
      pull(Median)

    mean_phone_time <- summary_table |>
      filter(Variable == "Phone_Time") |>
      pull(Mean)

    to_ft(
      summary_table,
      caption = "Weekly Elapsed Time Summary Table",
      header_map = c(Variable = "Time Metric", Minimum = "Min", Mean = "Mean", Median = "Median", Std_Dev = "Std Dev", Skewness = "Skew", Kurtosis = "Kurt"),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})
```

Overall, DECC operations appear to be very efficient at getting fire-related service calls out to the field. The median processing time was only `r median_processing_time` seconds. This shows that we can easily be in compliance with all necessary NENA and NFPA guidelines. The median time on the phone was `r median_phone_time` seconds. The mean time was `r mean_phone_time` seconds, which is still amazing.

### AFD EMS Analyses

Because AFD calls for service can be split into two distinct disciplines, fire-related calls and medical-related calls, DECC has, historically, separated the two disciplines for analytical purposes. So this section will look at medical-related calls for service for the week.

```{r ems-day-of-week}
#| echo: false
#| fig-cap: "Number of calls for service by day of the week."
# ggplot2
dow_counts_ems <- df_ems |>
  count(DOW, sort = TRUE)

max_dow_info_ems <- dow_counts_ems |> filter(n == max(n))
busiest_day_abbr_ems <- max_dow_info_ems |> slice(1) |> pull(DOW)
busiest_day_count_ems <- max_dow_info_ems |> slice(1) |> pull(n)

min_dow_info_ems <- dow_counts_ems |> filter(n == min(n))
slowest_day_abbr_ems <- min_dow_info_ems |> slice(1) |> pull(DOW)
slowest_day_count_ems <- min_dow_info_ems |> slice(1) |> pull(n)

# Create mapping from abbreviations to full day names
day_names <- c(
  "SUN" = "Sunday",
  "MON" = "Monday", 
  "TUE" = "Tuesday",
  "WED" = "Wednesday",
  "THU" = "Thursday",
  "FRI" = "Friday",
  "SAT" = "Saturday"
)

# Convert abbreviations to full day names
busiest_day_ems <- day_names[busiest_day_abbr_ems]
slowest_day_ems <- day_names[slowest_day_abbr_ems]

barDOW_EMS <- df_ems |> ggplot(aes(x=DOW, fill=DOW)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="D") +
  labs(title="Number of Calls for Service for EMS by Day of the Week",
       x="Day of the Week",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barDOW_EMS
```

 This week, there is a spike in medical calls on `r busiest_day_ems`. This appears to correlate to the information that we saw earlier in the report. Outside of `r busiest_day_ems`, the remainder of the week appears to be consistent for the number of medical calls handled.

```{r ems-hour-of-day}
#| echo: false
#| fig-cap: "Number of calls for service by hour of the day." 
# ggplot2
hour_counts_ems <- df_ems |>
  count(Hour, sort = TRUE)

max_hour_info_ems <- hour_counts_ems |> filter(n == max(n))
busiest_hour_ems <- sprintf("%02d", max_hour_info_ems |> slice(1) |> pull(Hour))
busiest_hour_count_ems <- max_hour_info_ems |> slice(1) |> pull(n)

min_hour_info_ems <- hour_counts_ems |> filter(n == min(n))
slowest_hour_ems <- sprintf("%02d", min_hour_info_ems |> slice(1) |> pull(Hour))
slowest_hour_count_ems <- min_hour_info_ems |> slice(1) |> pull(n)

barHour_EMS <- df_ems |> ggplot(aes(x=Hour, fill=Hour)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="D") +
  labs(title="Number of Calls for Service for EMS by Hour of the Day",
       x="Hour of the Day",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=8),
        axis.text.y = element_text(size=10))

barHour_EMS
```

This week, the busiest hour was `r busiest_hour_ems` hours. The afternoon to evening hours, this week, stayed consistently busy which appears to continue the trends previously observed.

```{r ems-call-reception}
#| echo: false
#| fig-cap: "Number of calls for service by call reception."
# ggplot2
cr_counts_ems <- df_ems |>
  count(Call_Reception, sort = TRUE)

max_cr_info_ems <- cr_counts_ems |> filter(n == max(n))
busiest_cr_ems <- max_cr_info_ems |> slice(1) |> pull(Call_Reception)
busiest_cr_count_ems <- max_cr_info_ems |> slice(1) |> pull(n)

cr_nr_pct_ems <- round((sum(df_ems$Call_Reception == "NOT CAPTURED") / nrow(df_ems)) * 100, 1)

barReception_EMS <- df_ems |> ggplot(aes(x=Call_Reception, fill=Call_Reception)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="D") +
  labs(title="Number of Calls for Service for EMS by Call Reception",
       x="Call Reception",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barReception_EMS
```

As expected, the vast majority of medical calls arrived via `r busiest_cr_ems`. However, `r cr_nr_pct_ems` percent of medical calls arrived without a method by which we recevied the call. We should continue to monitor and investigate why these are occurring.

```{r ems-call-type}
#| echo: false
#| fig-cap: "Number of calls for service by call type."
# ggplot2
prob_counts_med <- df_ems |>
  count(Problem, sort = TRUE)

max_prob_info_med <- prob_counts_med |> filter(n == max(n))
busiest_prob_med <- max_prob_info_med |> slice(1) |> pull(Problem)
busiest_prob_count_med <- max_prob_info_med |> slice(1) |> pull(n)

ems_ma_call <- sum(startsWith(df_ems$Problem, "MUTUAL"), na.rm = TRUE)

problem_counts_EMS <- df_ems |>
  count(Problem, sort = TRUE) |>
  slice_head(n = 10)

barProblem_EMS <- problem_counts_EMS |>
  ggplot(aes(x=reorder(Problem, -n), y=n, fill=Problem)) +
  geom_bar(stat="identity") +
  scale_fill_viridis(discrete=TRUE, option="E") +
  labs(title="Number of Calls for Service for EMS by Call Type",
       x="Call Type",
       y="Number of Calls") +
  geom_text(
    aes(label = n),
    vjust = -0.5, 
    size = 3) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barProblem_EMS
```

The most prevalent medical service type this week was 'r busiest_prob_med'. Further we had `r ems_ma_call` mutual aid medical calls this week.

```{r ems-priority-level}
#| echo: false
#| fig-cap: "Number of calls for service by priority level."
# ggplot2
pri_counts_ems <- df_ems |>
  count(Priority_Number, sort = TRUE)

max_pri_info_ems <- pri_counts_ems |> filter(n == max(n))
busiest_pri_ems <- max_pri_info_ems |> slice(1) |> pull(Priority_Number)
busiest_pri_count_ems <- max_pri_info_ems |> slice(1) |> pull(n)

# Calculate percentage for APD priority calls
busiest_pri_ems_percentage <- round((sum(df_ems$Priority_Number == busiest_pri_ems, na.rm = TRUE) / nrow(df_ems)) * 100, 1)

# Calculate percentage for P1 calls
p1_ems_percentage <- round((sum(df_ems$Priority_Number == 1, na.rm = TRUE) / nrow(df_ems)) * 100, 1)

barPriority_EMS <- df_ems |> ggplot(aes(x=Priority_Number, fill=Priority_Number)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="E") +
  labs(title="Number of Calls for Service by Priority Level",
       x="Priority Level",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barPriority_EMS
```

 The majority of medical service calls are P`r busiest_pri_ems`, which is to be expected.

```{r ems-custom-summary}
#| label: ems-custom-summary
#| echo: false
#| message: false
#| warning: false

# Initialize variables
summary_table <- NULL
median_processing_time <- NA_real_
median_phone_time <- NA_real_

# Create a summary table of elapsed time variables
# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time", 
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")
  
  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_ems)]
  
  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {
    
    summary_table <- df_ems %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]
                
                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)
                    
                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)
                    
                    if (var_val == 0) return(NA_real_)
                    
                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })
                
                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display
    
    # Extract key metrics for use in text
    median_time_to_queue <- summary_table |> 
      filter(Variable == "Time To Queue") |> 
      pull(Median)
      
    median_time_to_dispatch <- summary_table |> 
      filter(Variable == "Time To Dispatch") |> 
      pull(Median)

    median_processing_time <- summary_table |>
      filter(Variable == "Processing_Time") |>
      pull(Median)

    median_phone_time <- summary_table |>
      filter(Variable == "Phone_Time") |>
      pull(Median)

    to_ft(
      summary_table,
      caption = "Weekly Elapsed Time Summary Table",
      header_map = c(Variable = "Time Metric", Minimum = "Min", Mean = "Mean", Median = "Median", Std_Dev = "Std Dev", Skewness = "Skew", Kurtosis = "Kurt"),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})
```

The median time to process medical calls was `r median_processing_time` seconds. Again, this puts us in good form when examening our operational efficiency. The median time on phones, `r median_phone_time` seconds, is longer than the overall median. That is to be expected with these calls taking longer to triage.

## Additional Analyses

Earlier, for this analysis, we created some additional datasets that we can investigate in the course of our analysis. The first two are lists of calls where the elapsed time prior to release to queue or the time spent in dispatch is greater than 60 seconds for *emergency* calls. For the first, there are `r nrow(df_ttq_delay)` emergency service calls where the elapsed time from call start to the call entering the queue for dispatch was over 60 seconds. There are also `r nrow(df_ttd_delay)` emergency service calls where the elapsed time from entering queue to the first unit assigned was over 60 seconds.

### Possible Service Delays

We can look at the datasets and see if there are telecommunicators who may experience more challenging calls during the week. First will be a table of telecommunicators who worked emergency calls that took longer than 60 seconds to go from start to queue. The second will be a table of dispatchers who assigned an emergency call that waited in queue longer than 60 seconds.

```{r queue-too-long}
#| echo: false
#| message: false
#| warning: false

# Table: Call_Taker frequency in df_ttq_delay (descending)
library(dplyr)

call_taker_counts <- df_ttq_delay %>%
  count(Call_Taker, sort = TRUE)

to_ft(
  call_taker_counts,
  caption = "Frequency of Call Taker in Delayed TTQ Calls (Descending)",
  header_map = c(Call_Taker = "Call Taker", n = "Count"),
  digits = 0
)
```

From this, since there are a small number of telecommunicators who have more than one call in the table above, there may not be any need for amerlioration. This, however, could be something that is included in the report template in order to monitor. Should a telecommunicator appear multiple times in this table over a period of time, additional training or mentoring may be called for.

```{r ttd-delay-table}
#| echo: false
#| message: false
#| warning: false

# Table: Dispatcher frequency in df_ttd_delay (descending)
dispatcher_counts <- df_ttd_delay %>%
  count(Dispatcher, sort = TRUE)

to_ft(
  dispatcher_counts,
  caption = "Frequency of Dispatcher in Delayed TTD Calls (Descending)",
  header_map = c(Dispatcher = "Dispatcher", n = "Count"),
  digits = 0
)
```

This list is fairly short and could simply be monitored in future should the need arise. 

## High-Priority and Critical Calls

In this section, we will focus on the calls that are deemed high-priority or critical. This includes APD Priority 1 calls and AFD Priority 1 and 2 calls. We have identified these calls in the `df_hp` dataset created earlier.

### High-Priority Call Types

```{r hp-call-types}
#| echo: false
#| fig-cap: "Top High-Priority Call Types"
# ggplot2
hp_call_types <- df_hp |>
  count(Problem, sort = TRUE) |>
  slice_head(n = 10)

ggplot(hp_call_types, aes(x = reorder(Problem, n), y = n, fill = Problem)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  scale_fill_viridis(discrete = TRUE, option = "E") +
  labs(title = "Top High-Priority Call Types",
       x = "Call Type",
       y = "Number of Calls") +
  theme_minimal()
```

Almost all of the problem types in this graph belong to AFD and are medical calls. Based on the information above, this is to be expected.

### High-Priority Response Times

```{r hp-response-times}
#| echo: false
#| fig-cap: "High-Priority Call Response Times"
#| warning: false
#| message: false
# ggplot2
hp_response_times <- df_hp |>
  transmute(
    Time_To_Queue = as.numeric(Time_To_Queue),
    Time_To_Dispatch = as.numeric(Time_To_Dispatch),
    Phone_Time = as.numeric(Phone_Time),
    Processing_Time = as.numeric(Processing_Time),
    Rollout_Time = as.numeric(Rollout_Time),
    Transit_Time = as.numeric(Transit_Time),
    Total_Call_Time = as.numeric(Total_Call_Time)
  ) |>
  pivot_longer(everything(), names_to = "Metric", values_to = "Time")
  
ggplot(hp_response_times |> filter(!is.na(Time) & Time >= 0), aes(x = Time)) +
  geom_histogram(binwidth = 5, fill = "#1c5789", color = "white", alpha = 0.7) +
  facet_wrap(~ Metric, scales = "free") +
  scale_x_continuous(labels = scales::comma) + # Improve readability of x-axis
  labs(title = "Distribution of Response Times for High-Priority Calls",
       x = "Time (seconds)",
       y = "Frequency") +
  theme_minimal()
```

These histograms show that, overall, our record for handling these calls is excellent.

## Cardiac Arrest Calls Analysis

Finally, we will look into the specific subset of calls that are related to cardiac arrests. These calls have been identified in the `df_ca` dataset.

```{r ca-call-volume}
#| echo: false
#| fig-cap: "Cardiac Arrest Call Volume by Day and Hour"
# ggplot2
barDOW_CA <- df_ca |> ggplot(aes(x=DOW, fill=DOW)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="D") +
  labs(title="Number of Calls for Service for Cardiac Arrest by Day of the Week",
       x="Day of the Week",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barDOW_CA
```

As we can see, with a very limited number of cardiac arrest calls for the week.

```{r ca-response-times}
#| echo: false
#| fig-cap: "Cardiac Arrest Call Response Times"
#| warning: false
#| message: false
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time", 
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_ca)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- df_ca %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

     #Extract key metrics for use in text
    median_time_to_queue <- summary_table |> 
      filter(Variable == "Time To Queue") |> 
      pull(Median)
      
    median_time_to_dispatch <- summary_table |> 
      filter(Variable == "Time To Dispatch") |> 
      pull(Median)

    median_processing_time <- summary_table |>
      filter(Variable == "Processing_Time") |>
      pull(Median)

    median_phone_time <- summary_table |>
      filter(Variable == "Phone_Time") |>
      pull(Median)

    to_ft(
      summary_table,
      caption = "Weekly Elapsed Time Summary Table",
      header_map = c(Variable = "Time Metric", Minimum = "Min", Mean = "Mean", Median = "Median", Std_Dev = "Std Dev", Skewness = "Skew", Kurtosis = "Kurtosis"),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})
```

However, we can see that the median time to process a cardiac arrest and get the units rolling is about `r median_processing_time` seconds. The median time that we are on the phone is significantly longer, `r median_phone_time` seconds. That is to be expected since the calltaker is likely giving T-CPR instructions while the units are en route.

### Mental Health Analyses

With the advent of Marcus' Law in Virginia, there has been an emphasis on how mental health calls are processed and serviced. The following analyses will focus on the mental health calls that have been defined as such after consultation with DCHS.

```{r mh-day-of-week}
#| echo: false
#| fig-cap: "Number of calls for service by day of the week."
# ggplot2
dow_counts_mh <- df_mh |>
  count(DOW, sort = TRUE)

max_dow_info_mh <- dow_counts_mh |> filter(n == max(n))
busiest_day_abbr_mh <- max_dow_info_mh |> slice(1) |> pull(DOW)
busiest_day_count_mh <- max_dow_info_mh |> slice(1) |> pull(n)

min_dow_info_mh <- dow_counts_mh |> filter(n == min(n))
slowest_day_abbr_mh <- min_dow_info_mh |> slice(1) |> pull(DOW)
slowest_day_count_mh <- min_dow_info_mh |> slice(1) |> pull(n)

# Create mapping from abbreviations to full day names
day_names <- c(
  "SUN" = "Sunday",
  "MON" = "Monday", 
  "TUE" = "Tuesday",
  "WED" = "Wednesday",
  "THU" = "Thursday",
  "FRI" = "Friday",
  "SAT" = "Saturday"
)

# Convert abbreviations to full day names
busiest_day_mh <- day_names[busiest_day_abbr_mh]
slowest_day_mh <- day_names[slowest_day_abbr_mh]

barDOW_MH <- df_mh |> ggplot(aes(x=DOW, fill=DOW)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Mental Health related calls by Day of the Week",
       x="Day of the Week",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) + 
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barDOW_MH
```

The busiest day of the week for mental health calls was `r busiest_day_mh` with `r busiest_day_count_mh` service calls. 

```{r mh-hour-of-day}
#| echo: false
#| fig-cap: "Number of calls for service by hour of the day."
# ggplot2
hour_counts_mh <- df_mh |>
  count(Hour, sort = TRUE)

max_hour_info_mh <- hour_counts_mh |> filter(n == max(n))
busiest_day_mh <- max_hour_info_mh |> slice(1) |> pull(Hour)
busiest_day_count_mh <- max_hour_info_mh |> slice(1) |> pull(n)

min_hour_info_mh <- hour_counts_mh |> filter(n == min(n))
slowest_day_mh <- min_hour_info_mh |> slice(1) |> pull(Hour)
slowest_day_count_mh <- min_hour_info_mh |> slice(1) |> pull(n)

barHour_MH <- df_mh |> ggplot(aes(x=Hour, fill=Hour)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Calls for Service for APD by Hour of the Day",
       x="Hour of the Day",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=8),
        axis.text.y = element_text(size=10))

barHour_MH
```

Most of these calls arrived, for this past week, in the late mornings through evenings. Again, should this data prove to be part of a trend, then we should adjust the availability of repsonders to address the community's needs.

```{r mh-call-reception}
#| echo: false
#| fig-cap: "Number of calls for service by call reception."
# ggplot2
cr_counts_mh <- df_mh |>
  count(Call_Reception, sort = TRUE)

max_cr_info_mh <- cr_counts_mh |> filter(n == max(n))
busiest_cr_mh <- max_cr_info_mh |> slice(1) |> pull(Call_Reception)
busiest_cr_count_mh <- max_cr_info_mh |> slice(1) |> pull(n)

min_cr_info_mh <- cr_counts_mh |> filter(n == min(n))
slowest_cr_mh <- min_cr_info_mh |> slice(1) |> pull(Call_Reception)
slowest_cr_count_mh <- min_cr_info_mh |> slice(1) |> pull(n)

barReception_MH <- df_mh |> ggplot(aes(x=Call_Reception, fill=Call_Reception)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Calls for Service for by Call Reception",
       x="Call Reception",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barReception_MH
```

This week, most of our mental health calls `r busiest_cr_mh` Further analysis could be understaken to determine if any of these are transfer calls from our local 988 provider partner.

```{r mh-call-type}
#| echo: false
#| fig-cap: "Number of calls for service by call type."
# ggplot2
ct_counts_mh <- df_mh |>
  count(Problem, sort = TRUE)

max_ct_info_mh <- ct_counts_mh |> filter(n == max(n))
busiest_ct_mh <- max_ct_info_mh |> slice(1) |> pull(Problem)
busiest_ct_count_mh <- max_ct_info_mh |> slice(1) |> pull(n)

min_ct_info_mh <- ct_counts_mh |> filter(n == min(n))
slowest_ct_mh <- min_ct_info_mh |> slice(1) |> pull(Problem)
slowest_ct_count_mh <- min_ct_info_mh |> slice(1) |> pull(n)

problem_counts_MH <- df_mh |>
  count(Problem, sort = TRUE) |>
  slice_head(n = 10)

barProblem_MH <- problem_counts_MH |>
  ggplot(aes(x=reorder(Problem, -n), y=n, fill=Problem)) +
  geom_bar(stat="identity") +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Calls for Service by Call Type",
       x="Call Type",
       y="Number of Calls") +
  geom_text(
    aes(label = n),
    vjust = -0.5,
    size=3) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barProblem_MH
```

The most used call type was `r busiest_ct_mh` which is expected.

```{r mh-priority-level}
#| echo: false
#| fig-cap: "Number of calls for service by priority level."
# ggplot2
pri_counts_mh <- df_mh |>
  count(Priority_Number, sort = TRUE)

max_pri_info_mh <- pri_counts_mh |> filter(n == max(n))
busiest_pri_mh <- max_pri_info_mh |> slice(1) |> pull(Priority_Number)
busiest_pri_count_mh <- max_pri_info_mh |> slice(1) |> pull(n)

min_pri_info_mh <- pri_counts_mh |> filter(n == min(n))
slowest_pri_mh <- min_pri_info_mh |> slice(1) |> pull(Priority_Number)
slowest_pri_count_mh <- min_pri_info_mh |> slice(1) |> pull(n)

barPriority_MH <- df_mh |> ggplot(aes(x=Priority_Number, fill=Priority_Number)) +
  geom_bar() +
  scale_fill_viridis(discrete=TRUE, option="G") +
  labs(title="Number of Calls for Service by Priority Level",
       x="Priority Level",
       y="Number of Calls") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    vjust = -0.5,
    size=3
  ) +
  theme_minimal() +
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(angle=45, hjust=1, size=10),
        axis.text.y = element_text(size=10))

barPriority_MH
```

Since `r busiest_ct_mh` was the most used call type and is a P2 call, Priority `r busiest_pri_mh` is the most used priority. The question, in the future, will be does these calls need to changed to a higher priority?

```{r mh-custom-summary}
#| label: mh-custom-summary
#| echo: false
#| message: false
#| warning: false

# Initialize variables
summary_table <- NULL
median_processing_time <- NA_real_

# Create a summary table of elapsed time variables
tryCatch({
  # Define the columns we want to analyze
  time_columns <- c("Time_To_Queue", "Time_To_Dispatch", "Phone_Time",
                    "Processing_Time", "Rollout_Time", "Transit_Time", "Total_Call_Time")

  # Check which columns actually exist in the data
  existing_columns <- time_columns[time_columns %in% names(df_mh)]

  if (length(existing_columns) == 0) {
    cat("No time columns found in the data.\n")
  } else {

    summary_table <- df_mh %>%
      # 1. Select only the columns that exist
      dplyr::select(all_of(existing_columns)) %>%
      # 2. Summarize across all selected columns, converting difftime to numeric safely
      summarise(across(everything(),
        list(
          Minimum  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(min(vals, na.rm = TRUE), 2)
          },
          Mean     = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(mean(vals, na.rm = TRUE), 2)
          },
          Median   = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(median(vals, na.rm = TRUE), 2)
          },
          Std_Dev  = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else round(sd(vals, na.rm = TRUE), 2)
          },
          Skewness = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals))) NA_real_ else {
              tryCatch(round(psych::skew(vals, na.rm = TRUE), 2), error = function(e) NA_real_)
            }
          },
          Kurtosis = ~ {
            vals <- as.numeric(.)
            if (all(is.na(vals)) || length(vals[!is.na(vals)]) < 4) {
              NA_real_
            } else {
              tryCatch({
                # Remove NA values first
                clean_vals <- vals[!is.na(vals)]

                # Try multiple approaches for kurtosis calculation
                result <- tryCatch({
                  # Method 1: Use psych::kurtosis with explicit namespace
                  psych::kurtosis(clean_vals)
                }, error = function(e1) {
                  tryCatch({
                    # Method 2: Use moments package if available
                    if (requireNamespace("moments", quietly = TRUE)) {
                      moments::kurtosis(clean_vals) - 3  # Convert to excess kurtosis
                    } else {
                      stop("moments not available")
                    }
                  }, error = function(e2) {
                    # Method 3: Manual calculation
                    n <- length(clean_vals)
                    if (n < 4) return(NA_real_)

                    mean_val <- mean(clean_vals)
                    var_val <- var(clean_vals)

                    if (var_val == 0) return(NA_real_)

                    # Calculate fourth moment
                    fourth_moment <- mean((clean_vals - mean_val)^4)
                    # Calculate kurtosis (excess kurtosis = kurtosis - 3)
                    kurt_val <- (fourth_moment / (var_val^2)) - 3
                    kurt_val
                  })
                })

                round(result, 2)
              }, error = function(e) {
                # Final fallback
                NA_real_
              })
            }
          }
        ),
        .names = "{.col}---{.fn}" # Use a unique separator
      )) %>%
      # 3. Reshape the data to a long format, then back to a clean wide format
      pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
      separate(Variable, into = c("Variable", "Statistic"), sep = "---") %>%
      pivot_wider(names_from = Statistic, values_from = Value) %>%
      mutate(Variable = str_replace_all(Variable, "_", " ")) # Clean up names for display

    #Extract key metrics for use in text
    median_time_to_queue <- summary_table |> 
      filter(Variable == "Time To Queue") |> 
      pull(Median)
      
    median_time_to_dispatch <- summary_table |> 
      filter(Variable == "Time To Dispatch") |> 
      pull(Median)

    median_processing_time <- summary_table |>
      filter(Variable == "Processing_Time") |>
      pull(Median)

    median_phone_time <- summary_table |>
      filter(Variable == "Phone_Time") |>
      pull(Median)

    to_ft(
      summary_table,
      caption = "Weekly Elapsed Time Summary Table",
      header_map = c(Variable = "Time Metric", Minimum = "Min", Mean = "Mean", Median = "Median", Std_Dev = "Std Dev", Skewness = "Skew", Kurtosis = "Kurt"),
      digits = 2
    )
  }
}, error = function(e) {
  cat("Error creating summary table:", e$message, "\n")
  cat("Available columns in df:", paste(names(df), collapse = ", "), "\n")
})
```

Processing times for these calls are longer, somewhere around `r median_processing_time` seconds. There are several factors that can impact this. The time to make it dispatchable was longer, implying that with these types of calls, it take calltakers longer to get the information necessary in the initial triage to accurately locate and classify the call. Another possible issue, in reviewing the dispatch times is that these calls require specialized training and skill sets on the part of the field responders. If those responders are already assigned to other calls, this could create the delay as seen here. As these values change over time, we should be able to build better pictures and determine the delay points and create strategies to ameliorate them.

## Conclusion

This report has covered various aspects of the calls for service during week 35, 2025. We have analyzed the data for completeness and accuracy, explored it for insights into call patterns and trends, and focused on specific areas of interest such as high-priority calls and cardiac arrest incidents. The findings will assist in making informed decisions to improve service delivery and operational efficiency.
